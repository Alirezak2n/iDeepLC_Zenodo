{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-28T17:13:21.711266Z",
     "end_time": "2023-08-28T17:13:26.058943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chemical_features': (0, 13), 'diamino_chemical_features': (13, 26), 'atoms': (26, 32), 'sequence_metadata': (32, 36), 'one_hot': (36, 57), 'is_modified': (57, 57)}\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.cuda\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import utilities_diamino13_new_meta as utilities\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-28T17:14:31.182368Z",
     "end_time": "2023-08-28T17:14:31.198499Z"
    }
   },
   "outputs": [],
   "source": [
    "# Changing the sequence to desired form\n",
    "# Pay attention to the index, +1 can be needed\n",
    "def reform_seq(seq, mod):\n",
    "    mod_list= [m for m in mod.split('|')]\n",
    "    mod_list_tuple=[]\n",
    "    if mod =='':\n",
    "        return (seq)\n",
    "    else:\n",
    "        while mod_list:\n",
    "            mod_list_tuple.append((int(mod_list.pop(0)),mod_list.pop(0)))\n",
    "        mod_list_tuple.sort()\n",
    "        while mod_list_tuple != []:\n",
    "            tuple_mod = mod_list_tuple.pop()\n",
    "            modification = tuple_mod[1]\n",
    "            index = tuple_mod[0]\n",
    "            seq = seq[:int(index)]+'['+modification+']'+seq[int(index):]\n",
    "        return (seq)\n",
    "\n",
    "# extract a dataset from whole data based on types\n",
    "def data_separator( file, dataset, type):\n",
    "\n",
    "    index=[]\n",
    "    sequence=[]\n",
    "    mod=[]\n",
    "    retention=[]\n",
    "    prediction=[]\n",
    "    df = pd.read_csv(file ,keep_default_na=False)\n",
    "\n",
    "    for idx, seq in enumerate(df['seq']):\n",
    "        if df['data set'][idx] == dataset  and df['set_type'][idx] == type:\n",
    "            index.append(idx)\n",
    "            sequence.append(seq)\n",
    "            mod.append(df['modifications'][idx])\n",
    "            retention.append(df['tr'][idx])\n",
    "            prediction.append((df['predictions'][idx]))\n",
    "\n",
    "    data_frame=pd.DataFrame()\n",
    "    data_frame['index']=index\n",
    "    data_frame['seq'] = sequence\n",
    "    data_frame['modifications'] = mod\n",
    "    data_frame['tr'] = retention\n",
    "    data_frame['predictions'] = prediction\n",
    "\n",
    "    return data_frame\n",
    "\n",
    "def seq_to_matrix(data, df):\n",
    "    results =[]\n",
    "    tr=[]\n",
    "    prediction=[]\n",
    "    errors=[]\n",
    "    for idx,seq in tqdm.tqdm(enumerate(data)):\n",
    "        try:\n",
    "            result = utilities.peptide_to_matrix(seq)\n",
    "            results.append(result)\n",
    "            tr.append(df['tr'][idx])\n",
    "            prediction.append(df['predictions'][idx])\n",
    "\n",
    "        except KeyError as e:\n",
    "            errors.append([seq,idx,e])\n",
    "        except Exception as e:\n",
    "            errors.append([seq,idx,e])\n",
    "    results_stack = np.stack(results)\n",
    "    print(len(tr),len(data),'Usable data=',len(tr) / len(data) * 100, '%')\n",
    "    return results_stack, tr, prediction, errors\n",
    "\n",
    "\n",
    "def dataframe_to_matrix(data, df, column_mask=None):\n",
    "\n",
    "    results, tr, prediction, errors = utilities.df_to_matrix(data, df, column_mask)\n",
    "\n",
    "    print(len(tr), len(data), 'Usable data=', len(tr) / len(data) * 100, '%')\n",
    "    return results, tr, prediction, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-28T17:14:34.674180Z",
     "end_time": "2023-08-28T17:14:48.233673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod= Acetyl min: 7.453849999999999 max: 61.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5182it [00:01, 4196.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5182 5182 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "239it [00:00, 5069.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239 239 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "272it [00:00, 8713.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272 272 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "239it [00:00, 14891.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239 239 Usable data= 100.0 %\n",
      "mod= Carbamidomethyl min: 7.453849999999999 max: 61.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4968it [00:00, 7949.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4968 4968 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "464it [00:00, 5601.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464 464 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "261it [00:00, 7123.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261 261 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "464it [00:00, 7445.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464 464 Usable data= 100.0 %\n",
      "mod= Crotonyl min: 7.453849999999999 max: 61.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5111it [00:00, 7446.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5111 5111 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:00, 6567.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313 313 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "269it [00:00, 5691.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269 269 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:00, 28412.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313 313 Usable data= 100.0 %\n",
      "mod= Deamidated min: 7.453849999999999 max: 61.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5199it [00:00, 7293.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5199 5199 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "221it [00:00, 6985.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221 221 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273it [00:00, 8661.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273 273 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "221it [00:00, 7099.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221 221 Usable data= 100.0 %\n",
      "mod= Dimethyl min: 7 max: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4770it [00:00, 7916.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4770 4770 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "672it [00:00, 7079.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672 672 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "251it [00:00, 7808.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251 251 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "672it [00:00, 15053.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672 672 Usable data= 100.0 %\n",
      "mod= Formyl min: 7.453849999999999 max: 61.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5050it [00:00, 7259.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5050 5050 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "378it [00:00, 6897.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378 378 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "265it [00:00, 8376.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265 265 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "378it [00:00, 11948.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378 378 Usable data= 100.0 %\n",
      "mod= Malonyl min: 7.453849999999999 max: 61.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5111it [00:00, 7162.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5111 5111 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:00, 6620.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313 313 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "269it [00:00, 7052.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269 269 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:00, 12707.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313 313 Usable data= 100.0 %\n",
      "mod= Methyl min: 7.453849999999999 max: 61.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4695it [00:00, 7577.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4695 4695 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "751it [00:00, 6926.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751 751 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "247it [00:00, 7807.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247 247 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "751it [00:00, 11867.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751 751 Usable data= 100.0 %\n",
      "mod= Nitro min: 7.453849999999999 max: 61.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5208it [00:00, 7647.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5208 5208 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [00:00, 6588.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211 211 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "274it [00:00, 7470.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274 274 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [00:00, 13502.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211 211 Usable data= 100.0 %\n",
      "mod= Oxidation min: 7.453849999999999 max: 61.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4276it [00:00, 8204.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4276 4276 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1192it [00:00, 5456.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192 1192 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "225it [00:00, 10166.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225 225 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1192it [00:00, 9362.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192 1192 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod= Phospho min: 7.453849999999999 max: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5209it [00:00, 7224.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5209 5209 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "210it [00:00, 6744.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 210 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "274it [00:00, 8801.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274 274 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "210it [00:00, 9488.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 210 Usable data= 100.0 %\n",
      "mod= Propionyl min: 7.453849999999999 max: 61.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5084it [00:00, 7144.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5084 5084 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "342it [00:00, 6435.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342 342 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "267it [00:00, 7188.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267 267 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "342it [00:00, 21360.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342 342 Usable data= 100.0 %\n",
      "mod= Succinyl min: 7.453849999999999 max: 61.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5090it [00:00, 6935.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5090 5090 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "336it [00:00, 6037.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336 336 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "267it [00:00, 5649.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267 267 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "336it [00:00, 13990.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336 336 Usable data= 100.0 %\n",
      "mod= Trimethyl min: 7.453849999999999 max: 61.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5082it [00:00, 6863.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5082 5082 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "344it [00:00, 6071.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344 344 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "267it [00:00, 8439.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267 267 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "344it [00:00, 22014.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344 344 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "standardized = True\n",
    "diff_results = False\n",
    "\n",
    "path = './Data_files/14ptm'\n",
    "mods = ['Acetyl','Carbamidomethyl','Crotonyl','Deamidated','Dimethyl','Formyl','Malonyl','Methyl','Nitro','Oxidation',\n",
    "       'Phospho','Propionyl','Succinyl','Trimethyl']\n",
    "sets = ['train','test','valid']\n",
    "columns = ['index_name','seq','modifications','tr','predictions']\n",
    "for mod in mods:\n",
    "    df_train = pd.read_csv(path + '/prosit_ptm_2020_' +mod+ '_modanalysis_'+sets[0]+'.csv',keep_default_na=False)\n",
    "    df_test = pd.read_csv(path + '/prosit_ptm_2020_' +mod+ '_modanalysis_'+sets[1]+'.csv',keep_default_na=False)\n",
    "    df_valid = pd.read_csv(path + '/prosit_ptm_2020_' +mod+ '_modanalysis_'+sets[2]+'.csv',keep_default_na=False)\n",
    "\n",
    "    df_train = df_train[columns]\n",
    "    df_test = df_test[columns]\n",
    "    df_valid = df_valid[columns]\n",
    "\n",
    "\n",
    "    maxx,minn=df_train['tr'].max(),df_train['tr'].min()\n",
    "    for i in [ df_test, df_valid]:\n",
    "        if i['tr'].min() < minn:\n",
    "            minn = int(i['tr'].min())\n",
    "        if i['tr'].max() > maxx:\n",
    "            maxx = int(i['tr'].max())\n",
    "    print('mod=',mod,'min:',minn,'max:',maxx)\n",
    "\n",
    "    for i in [df_train, df_test, df_valid]:\n",
    "        i['tr_norm'] = (i['tr'] - minn)/(maxx - minn)\n",
    "\n",
    "    df_test_no_mod=df_test.copy()\n",
    "    df_test_no_mod['modifications'].replace(f\"[|][*0-9]+[|]{mod}\", '',inplace=True,regex=True)\n",
    "    df_test_no_mod['modifications'].replace(f\"[*0-9]+[|]{mod}\", '',inplace=True,regex=True)\n",
    "    df_test_no_mod['modifications'].replace(\"^[|]|[|]$\", '',inplace=True,regex=True)\n",
    "\n",
    "    my_train = [reform_seq(df_train['seq'][i],m) for i, m in enumerate (df_train['modifications'])]\n",
    "    my_test = [reform_seq(df_test['seq'][i],m) for i, m in enumerate (df_test['modifications'])]\n",
    "    my_eval = [reform_seq(df_valid['seq'][i],m) for i, m in enumerate (df_valid['modifications'])]\n",
    "    my_test_no_mod = [reform_seq(df_test_no_mod['seq'][i],m) for i, m in enumerate (df_test_no_mod['modifications'])]\n",
    "\n",
    "    if standardized:\n",
    "        mod = \"diamino_atom_stan/\"+mod\n",
    "    path_save = './data_matrix/14ptm/'+mod\n",
    "\n",
    "    if diff_results:\n",
    "        temp=pd.concat([pd.DataFrame(my_test),pd.read_csv('./saved_models/14ptms/'+mod+'_results.csv')],axis=1)\n",
    "        temp=temp.drop(temp.columns[[1]],axis=1)\n",
    "        # temp=pd.read_csv('./saved_models/14ptms/'+mod+'_results.csv',index_col=0)\n",
    "        temp['diff'] = temp['1'] - temp['0']\n",
    "        temp.sort_values('diff',inplace=True)\n",
    "        temp.rename(columns={0:'seq','0':'tr','1':'predict'},inplace=True)\n",
    "\n",
    "        temp.to_csv('./saved_models/14ptms/'+mod+'_results.csv')\n",
    "\n",
    "    if not diff_results:\n",
    "        train_x, train_y, train_prediction, train_error = dataframe_to_matrix(my_train, df_train)\n",
    "        test_x, test_y, test_prediction, test_error = dataframe_to_matrix(my_test, df_test)\n",
    "        val_x, val_y, val_prediction, val_error = dataframe_to_matrix(my_eval, df_valid)\n",
    "        test_no_mod_x, test_no_mod_y, test_no_mod_prediction, test_no_mod_error = dataframe_to_matrix(my_test_no_mod, df_test_no_mod)\n",
    "\n",
    "\n",
    "        # save the matrix\n",
    "        Path(path_save).mkdir(parents=True, exist_ok=True)\n",
    "        np.save(path_save+'/val_prediction',val_prediction)\n",
    "        np.save(path_save+'/val_x',val_x)\n",
    "        np.save(path_save+'/val_y',val_y)\n",
    "        np.save(path_save+'/test_x',test_x)\n",
    "        np.save(path_save+'/test_y',test_y)\n",
    "        np.save(path_save+'/test_prediction',test_prediction)\n",
    "        np.save(path_save+'/test_no_mod_x',test_no_mod_x)\n",
    "        np.save(path_save+'/test_no_mod_y',test_no_mod_y)\n",
    "        np.save(path_save+'/test_no_mod_prediction',test_no_mod_prediction)\n",
    "        np.save(path_save+'/train_prediction',train_prediction)\n",
    "        np.save(path_save+'/train_y',train_y)\n",
    "        np.save(path_save+'/train_x',train_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Separate R Dimethyl"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./Data_files/14ptm' + '/prosit_ptm_2020_' +'Dimethyl'+ '_modanalysis_'+'test'+'.csv',keep_default_na=False)\n",
    "columns = ['index_name','seq','modifications','tr','predictions']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-23T16:23:31.475552Z",
     "end_time": "2023-08-23T16:23:31.475552Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "['GGNFGGR[Dimethyl]GGYGGGGGGSR',\n 'GAR[Dimethyl]LPGPVSSAR',\n 'ANYNFRGM[Oxidation]YNQR[Dimethyl]',\n 'LLALSGPGGGR[Dimethyl]GR',\n 'FGDM[Oxidation]SYSIR[Dimethyl]HSISM[Oxidation]PAM[Oxidation]R',\n 'PGTPAGYGRGR[Dimethyl]',\n 'GPPR[Dimethyl]GGPGGELPR',\n 'MTEDALR[Dimethyl]LNLLK',\n 'AR[Dimethyl]GWAGVER',\n 'AR[Dimethyl]GGC[Carbamidomethyl]PGGEATLSQPPPR',\n 'VEPGSLESSPGLGR[Dimethyl]GR',\n 'STNVTVAAAR[Dimethyl]GIK',\n 'VTR[Dimethyl]YPILGIPQAHR',\n 'FGR[Dimethyl]GTGLTQPSQR',\n 'GGGDR[Dimethyl]GGFGPGK',\n 'GLATFPSR[Dimethyl]GAK',\n 'R[Dimethyl]GATPPAPPR',\n 'VLATQPGPGRGR[Dimethyl]',\n 'GSYLGGYSAGRGIYSR[Dimethyl]',\n 'SGSNWGR[Dimethyl]GSNM[Oxidation]NSGPPR',\n 'FSHSVQNSER[Dimethyl]GK',\n 'R[Dimethyl]GTVGAGC[Carbamidomethyl]GFNQR',\n 'GR[Dimethyl]GLPGPAESLR',\n 'GR[Dimethyl]GAPPPPPSR',\n 'LGTTPSVYGGAGGR[Dimethyl]GIR',\n 'PLPVALQTR[Dimethyl]LAK',\n 'FGDSR[Dimethyl]GGGGNFGPGPGSNFR',\n 'SGGGGYSQNR[Dimethyl]WGNNNR',\n 'GGNFGGRGGYGGGGGGSR[Dimethyl]',\n 'VTVAR[Dimethyl]GSALEM[Oxidation]EFK',\n 'YGR[Dimethyl]GQLNYPLPDFSK',\n 'FGPGVAFR[Dimethyl]APSIHGGSGGR',\n 'SGATAGAAGGR[Dimethyl]GK',\n 'DRGVGAEPLLPWNR[Dimethyl]',\n 'ANYNFR[Dimethyl]GM[Oxidation]YNQR',\n 'GEGGFGGR[Dimethyl]GGGR',\n 'AAGGAPLSC[Carbamidomethyl]FIC[Carbamidomethyl]GGGIGR[Dimethyl]GK',\n 'GNDTFGPRAQGWAR[Dimethyl]',\n 'TQAEGQGQGR[Dimethyl]GLGLVSAVR',\n 'GGPR[Dimethyl]GGLGGGM[Oxidation]R',\n 'M[Oxidation]TEDALR[Dimethyl]LNLLK',\n 'R[Dimethyl]GGPGGPPGPLM[Oxidation]EQM[Oxidation]GGR',\n 'PM[Oxidation]GAR[Dimethyl]GQC[Carbamidomethyl]M[Oxidation]SGASR',\n 'PGTPAGYGR[Dimethyl]GR',\n 'SR[Dimethyl]GSGGLGGAC[Carbamidomethyl]GGAGFGSR',\n 'GGSAGGR[Dimethyl]GFFR',\n 'YSR[Dimethyl]GAGLGFSTAPNK',\n 'SR[Dimethyl]GGGGNFGPGPGSNFR',\n 'GVPR[Dimethyl]GVHLGSQGLQR',\n 'GAPGGR[Dimethyl]GAQLSVPSPAR',\n 'R[Dimethyl]GM[Oxidation]APASTDDLFAR',\n 'R[Dimethyl]GVGAGAIAK',\n 'VC[Carbamidomethyl]TFR[Dimethyl]FPSTNIK',\n 'R[Dimethyl]VYYLSLEFYM[Oxidation]GR',\n 'C[Carbamidomethyl]VEGQLLR[Dimethyl]GLSK',\n 'C[Carbamidomethyl]GLVIGR[Dimethyl]GGENVK',\n 'PR[Dimethyl]GVISTPVIR',\n 'DNNNSNNR[Dimethyl]GSYNR',\n 'GGPM[Oxidation]R[Dimethyl]GGPGPGPGPYHR',\n 'SFDEVEGVFGR[Dimethyl]GGGR',\n 'VPLLGSR[Dimethyl]LPYQPPALC[Carbamidomethyl]SLR',\n 'M[Oxidation]SGR[Dimethyl]GVGSVSQFK',\n 'PR[Dimethyl]TPTAPAVNLAGAR',\n 'TPSSFR[Dimethyl]QPFTPTSR',\n 'SGGGYGGDR[Dimethyl]SSGGGYSGDR',\n 'GSGLGAR[Dimethyl]GSSYGVTSTESYK',\n 'GR[Dimethyl]GTSQLAEGATAK',\n 'TGSGYSR[Dimethyl]SSQGQPWR',\n 'GEGGFGGRGGGR[Dimethyl]',\n 'AFSSR[Dimethyl]SYTSGPGSR',\n 'R[Dimethyl]GPPFPPPPPGAM[Oxidation]FGASR',\n 'GIGR[Dimethyl]GTVQALHATGAR',\n 'AM[Oxidation]LPGSPLFTR[Dimethyl]APPPK',\n 'YR[Dimethyl]GDYDR',\n 'GGSGGR[Dimethyl]GVSVSSAR',\n 'APSIHGGSGGR[Dimethyl]GVSVSSAR',\n 'HFGPVPPSQGR[Dimethyl]GR',\n 'SGR[Dimethyl]GGNFGFGDSR',\n 'GATVTR[Dimethyl]GVPPPPTVR',\n 'GGSAGGRGFFR[Dimethyl]',\n 'SYR[Dimethyl]SFSNLWK',\n 'SQTGWPR[Dimethyl]GVTQFGNK',\n 'SGGGGYSQNRWGNNNR[Dimethyl]',\n 'SYR[Dimethyl]GGGGGGGGWR',\n 'PSIHGGSGGR[Dimethyl]GVSVSSAR',\n 'AAHGR[Dimethyl]GALAEAAR',\n 'M[Oxidation]GAGM[Oxidation]GFGLER[Dimethyl]M[Oxidation]AAPIDR',\n 'SR[Dimethyl]FYSGFNSR',\n 'GR[Dimethyl]GAANDSTQFTVAGR',\n 'TNAAAM[Oxidation]NM[Oxidation]GR[Dimethyl]PFQK',\n 'VTPHPSPR[Dimethyl]GSPLPTPK',\n 'GNDTFGPR[Dimethyl]AQGWAR',\n 'LQLGTPPPLLAAR[Dimethyl]LVPPR',\n 'DSR[Dimethyl]GGGGNFGPGPGSNFR',\n 'SM[Oxidation]R[Dimethyl]GGYIGSTYFER',\n 'SR[Dimethyl]GLPC[Carbamidomethyl]TELFVAPVGVASK',\n 'AQPQDSR[Dimethyl]GGSK',\n 'GGPGGELPR[Dimethyl]GPQAGLGPR',\n 'GGGYGGDR[Dimethyl]GGYGGDR',\n 'AAYQAAIR[Dimethyl]GVGSAQSR',\n 'AVPSSPAGSAAR[Dimethyl]GR',\n 'PGGLGSSSLYGLGASRPR[Dimethyl]',\n 'GIGR[Dimethyl]GIALQLC[Carbamidomethyl]K',\n 'SR[Dimethyl]GAIISQYYNR',\n 'AGR[Dimethyl]GAGWSLR',\n 'GGGGGR[Dimethyl]GGLHDFR',\n 'FGSR[Dimethyl]GGPGGGFR',\n 'IEVIEIM[Oxidation]TDR[Dimethyl]GSGK',\n 'M[Oxidation]TGSSGGDR[Dimethyl]GGFK',\n 'LR[Dimethyl]QQLQAK',\n 'GM[Oxidation]DTR[Dimethyl]GPVPGPR',\n 'AVSTGDC[Carbamidomethyl]GQVLR[Dimethyl]GGVIQSTR',\n 'GR[Dimethyl]GTGEAEEEYVGPR',\n 'GNDTFGPR[Dimethyl]',\n 'GNLEGPWGGGR[Dimethyl]GLR',\n 'APPIDR[Dimethyl]SILPTAPR',\n 'PR[Dimethyl]LPATGTFK',\n 'SFDEVEGVFGRGGGR[Dimethyl]',\n 'SGGYGGSR[Dimethyl]DYYSSR',\n 'YSSSSIPEPFGSR[Dimethyl]GSPR',\n 'SGGGYGGDR[Dimethyl]GGGYGGDR',\n 'GFLQPPPLR[Dimethyl]QPR',\n 'GSYLGGYSAGR[Dimethyl]GIYSR',\n 'AGR[Dimethyl]GFSLEELR',\n 'GFGR[Dimethyl]GGAESHTFK',\n 'HFGPVPPSQGRGR[Dimethyl]',\n 'M[Oxidation]R[Dimethyl]GNDTFGPR',\n 'GGPR[Dimethyl]GIVGGGM[Oxidation]M[Oxidation]R',\n 'ATEGSGSM[Oxidation]RGGGGGNAR[Dimethyl]',\n 'VELPTC[Carbamidomethyl]M[Oxidation]YR[Dimethyl]LPNVHGR',\n 'R[Dimethyl]GFGFGGFAISAGK',\n 'R[Dimethyl]GWGAAAVAAGLR',\n 'IPSTIC[Carbamidomethyl]FGQGAAR[Dimethyl]GFK',\n 'GNLEGPWGGGRGLR[Dimethyl]',\n 'M[Oxidation]SEGR[Dimethyl]GLPPPPR',\n 'VLATQPGPGR[Dimethyl]GR',\n 'SGGR[Dimethyl]GVSVSSAR',\n 'PGGLGSSSLYGLGASR[Dimethyl]PR',\n 'R[Dimethyl]GGSGSHNWGTVK',\n 'HGGSGGR[Dimethyl]GVSVSSAR',\n 'R[Dimethyl]GGFAVQAK',\n 'LVLPSQAQAR[Dimethyl]LPSGEVVK',\n 'M[Oxidation]GAGM[Oxidation]GFGLERM[Oxidation]AAPIDR[Dimethyl]',\n 'DC[Carbamidomethyl]GTLPNR[Dimethyl]FVITPR',\n 'GGFQNR[Dimethyl]GGGSGGGGNYR',\n 'LGSGR[Dimethyl]GTGQM[Oxidation]EGR',\n 'LGTTPSVYGGAGGRGIR[Dimethyl]',\n 'GR[Dimethyl]GVGSPEPGPTAPYLGR',\n 'FR[Dimethyl]VTQPNTFGLFLYK',\n 'M[Oxidation]EGSDFESSGGR[Dimethyl]GLK',\n 'AGLR[Dimethyl]LVAPDVEGM[Oxidation]R',\n 'R[Dimethyl]GSFSSENYWR',\n 'APSIHGGSGGRGVSVSSAR[Dimethyl]',\n 'FR[Dimethyl]GFM[Oxidation]QK',\n 'ATEGSGSM[Oxidation]R[Dimethyl]GGGGGNAR',\n 'DFAFYNPR[Dimethyl]LQGK',\n 'LGILPVGR[Dimethyl]GK',\n 'DC[Carbamidomethyl]GTLPNRFVITPR[Dimethyl]',\n 'YSSNLGNFNYEQR[Dimethyl]GAFR',\n 'HHTLR[Dimethyl]QAPQPQSSLQR',\n 'R[Dimethyl]LPTTGYLVYR',\n 'SGGGYGGDRGGGYGGDR[Dimethyl]',\n 'VR[Dimethyl]LQTTLR',\n 'AQVLAGGR[Dimethyl]GK',\n 'SQDVR[Dimethyl]M[Oxidation]NGPM[Oxidation]GAGNSVR',\n 'GGPGGELPRGPQAGLGPR[Dimethyl]',\n 'LVIFTNQM[Oxidation]SIGR[Dimethyl]GK',\n 'VEPIHAVVLPR[Dimethyl]GK',\n 'GAPLSSAPILGGWFGR[Dimethyl]GC[Carbamidomethyl]TK',\n 'C[Carbamidomethyl]R[Dimethyl]QPAPPTSQPPR',\n 'DR[Dimethyl]GVGAEPLLPWNR',\n 'IEVIEIMTDR[Dimethyl]GSGK',\n 'YC[Carbamidomethyl]LTAPNYR[Dimethyl]LK',\n 'AM[Oxidation]EAASSLKSQAATK[Dimethyl]',\n 'ITGK[Dimethyl]NQVTATK',\n 'SLEEEGAAHSGK[Dimethyl]R',\n 'GEVQVSDK[Dimethyl]ER',\n 'IISSIEQK[Dimethyl]EENK',\n 'TEVLSPNSKVESK[Dimethyl]',\n 'SPSDSGYSYETIGKTTK[Dimethyl]',\n 'LGSPSGK[Dimethyl]TGTC[Carbamidomethyl]R',\n 'FSM[Oxidation]PGFK[Dimethyl]GEGPDVDVNLPK',\n 'IDLPAENSNSETIIITGK[Dimethyl]R',\n 'GEVGAGAGPGAQAGPSAK[Dimethyl]R',\n 'SGDWK[Dimethyl]GYTGK',\n 'LAEEENK[Dimethyl]AK',\n 'K[Dimethyl]FGVLSDNFK',\n 'SLK[Dimethyl]LNTAEDAK',\n 'TTGFYSGFSEVAEK[Dimethyl]R',\n 'DNSTM[Oxidation]GYM[Oxidation]AAKK[Dimethyl]',\n 'IHVQSSSDSSDEPAEK[Dimethyl]R',\n 'IQQELQTAKK[Dimethyl]',\n 'SLFFGGK[Dimethyl]GAPLQR',\n 'SQNTDM[Oxidation]VQK[Dimethyl]SVSK',\n 'IINEPTAAAIAYGLDK[Dimethyl]K',\n 'VFSIISSEK[Dimethyl]ELK',\n 'DEPSVAAM[Oxidation]VYPFTGDHK[Dimethyl]QK',\n 'DNSTM[Oxidation]GYM[Oxidation]AAK[Dimethyl]K',\n 'FELGK[Dimethyl]LM[Oxidation]ELHGEGSSSGK',\n 'IPQEK[Dimethyl]C[Carbamidomethyl]ILQTDVK',\n 'VSPASSVDSNIPSSQGYKK[Dimethyl]',\n 'VTAAAM[Oxidation]AGNK[Dimethyl]STPR',\n 'VFIGNLNTLVVKK[Dimethyl]',\n 'DYSAPVNFISAGLKK[Dimethyl]',\n 'TGGTQTDLFTC[Carbamidomethyl]GK[Dimethyl]C[Carbamidomethyl]K',\n 'TLQK[Dimethyl]QSVVYGGK',\n 'DTPTSAGPNSFNK[Dimethyl]GK',\n 'VNALK[Dimethyl]NLQVK',\n 'VNVEAPDVNLEGLGGK[Dimethyl]LK',\n 'ASDTSSETVFGK[Dimethyl]R',\n 'SQPATK[Dimethyl]TEDYGM[Oxidation]GPGR',\n 'TPGQNAQK[Dimethyl]WIPAR',\n 'TPNLYIYSK[Dimethyl]K',\n 'LQLEIDQK[Dimethyl]K',\n 'M[Oxidation]AQELK[Dimethyl]YGDIVER',\n 'PGGEGPQC[Carbamidomethyl]EK[Dimethyl]TTDVK',\n 'K[Dimethyl]YEEIDNAPEER',\n 'LLFSNTAAQK[Dimethyl]LR',\n 'IVFSGNLFQHQEDSK[Dimethyl]K',\n 'GETASK[Dimethyl]LQSEISR',\n 'ITESVAETAQTIKK[Dimethyl]',\n 'TQDQISNIKYHEEFEK[Dimethyl]',\n 'YEEIVKEVSTYIK[Dimethyl]',\n 'STGLELETPSLVPVKK[Dimethyl]',\n 'FSAYIK[Dimethyl]NSNPALNDNLEK',\n 'HAVSEGTK[Dimethyl]AVTK',\n 'SGK[Dimethyl]FTQQDIDEAK',\n 'SLEEEGAAHSGK[Dimethyl]',\n 'TFGK[Dimethyl]AAGPSLSHTSGGTQSK',\n 'LTSLNVK[Dimethyl]YNNDK',\n 'LEQVEK[Dimethyl]ELLR',\n 'GWLK[Dimethyl]SNVSDAVAQSTR',\n 'ASEALLKQLK[Dimethyl]',\n 'ANEK[Dimethyl]TESSSAQQVAVSR',\n 'DTPTSAGPNSFNKGK[Dimethyl]',\n 'SSFAYK[Dimethyl]DQNENR',\n 'TGGTQTDLFTC[Carbamidomethyl]GKC[Carbamidomethyl]K[Dimethyl]',\n 'VGTASVLQPVKK[Dimethyl]',\n 'LQLEIDQKK[Dimethyl]',\n 'VYIASSSGSTAIK[Dimethyl]K',\n 'YEQEHAAIQDK[Dimethyl]LFQVAK',\n 'VGEFSGANKEK[Dimethyl]',\n 'VNVEAPDVNLEGLGGKLK[Dimethyl]',\n 'INNFSADIK[Dimethyl]DSK',\n 'VQGGALEDSQLVAGVAFK[Dimethyl]K',\n 'LLTWDVK[Dimethyl]DTLLR',\n 'PSK[Dimethyl]GPLQSVQVFGR',\n 'VFIGNLNTLVVK[Dimethyl]K',\n 'TLDEILQEKK[Dimethyl]',\n 'VDIDVPDVNIEGPDAK[Dimethyl]LK',\n 'GGPGSTLSFVGK[Dimethyl]R',\n 'SM[Oxidation]STEGLM[Oxidation]K[Dimethyl]FVDSK',\n 'LLEQK[Dimethyl]TQESQK',\n 'VDIDAPDVSIEGPDAKLK[Dimethyl]',\n 'VGVEVPDVNIEGPEGK[Dimethyl]LK',\n 'SPDEAYAIAKK[Dimethyl]',\n 'ALLQAK[Dimethyl]ASGK',\n 'IDEM[Oxidation]PEAAVKSTANK[Dimethyl]',\n 'VGVEVPDVNIEGPEGKLK[Dimethyl]',\n 'LGNDFHTNK[Dimethyl]R',\n 'DESFLGK[Dimethyl]LGGTLAR',\n 'LDHLAEK[Dimethyl]FR',\n 'IGNSYFK[Dimethyl]EEK',\n 'VGEFSGANK[Dimethyl]EK',\n 'STGLELETPSLVPVK[Dimethyl]K',\n 'SLGEPAAGEGSAK[Dimethyl]R',\n 'VDIDVPDVNIEGPDAKLK[Dimethyl]',\n 'LFQSDTNAM[Oxidation]LGK[Dimethyl]K',\n 'LLEEEDSK[Dimethyl]LK',\n 'VDVEVPDVSLEGPEGKLK[Dimethyl]',\n 'FLQEFYQDDELGKK[Dimethyl]',\n 'LTEK[Dimethyl]ELAEAASK',\n 'LQDVSGQLSSSKK[Dimethyl]',\n 'GGNIGAK[Dimethyl]WTIDLK',\n 'VETTVTSLKTK[Dimethyl]',\n 'K[Dimethyl]NISQELNM[Oxidation]EQK',\n 'AQQHWGSGVGVK[Dimethyl]K',\n 'DIVAAIQHNYK[Dimethyl]M[Oxidation]SAFK',\n 'DVNAAIAAIK[Dimethyl]TK',\n 'VLTANSNPSSPSAAK[Dimethyl]R',\n 'ANLEK[Dimethyl]AQAELVGTADEATR',\n 'GTITVSAQELK[Dimethyl]DNR',\n 'ITESVAETAQTIK[Dimethyl]K',\n 'PSVGSQSNQAGQGK[Dimethyl]R',\n 'SGFGK[Dimethyl]FER',\n 'ASAISIKLGSSK[Dimethyl]',\n 'LEK[Dimethyl]TIDDLEEK',\n 'AAQASDLEKIHLDEK[Dimethyl]',\n 'SEASSEFAK[Dimethyl]K',\n 'M[Oxidation]M[Oxidation]SK[Dimethyl]PQTSGAYVLNK',\n 'M[Oxidation]QQLK[Dimethyl]EQYR',\n 'IDEM[Oxidation]PEAAVK[Dimethyl]STANK',\n 'DYSAPVNFISAGLK[Dimethyl]K',\n 'ASEALLK[Dimethyl]QLK',\n 'AAINQK[Dimethyl]LIETGER',\n 'TQDQISNIK[Dimethyl]YHEEFEK',\n 'FSM[Oxidation]PGFK[Dimethyl]GEGPDVDVSLPK',\n 'IQQELQTAK[Dimethyl]K',\n 'DEPSVAAM[Oxidation]VYPFTGDHKQK[Dimethyl]',\n 'LQEK[Dimethyl]VESAQSEQK',\n 'PSVSTK[Dimethyl]LISK',\n 'LGVATVAK[Dimethyl]GVC[Carbamidomethyl]GAC[Carbamidomethyl]K',\n 'M[Oxidation]GSK[Dimethyl]SPGNTSQPPAFFSK',\n 'VGTASVLQPVK[Dimethyl]K',\n 'DFDTALK[Dimethyl]HYDK',\n 'DYSSGFGGK[Dimethyl]YGVQADR',\n 'GSLGAQK[Dimethyl]LANTC[Carbamidomethyl]FNEIEK',\n 'TLDEILQEK[Dimethyl]K',\n 'ISALQGK[Dimethyl]LSK',\n 'SEDTISK[Dimethyl]M[Oxidation]NDFM[Oxidation]R',\n 'VGLQVVAVK[Dimethyl]APGFGDNR',\n 'AGPNASIISLK[Dimethyl]SDK',\n 'LLIVSTTPYSEKDTK[Dimethyl]',\n 'AGPNASIISLKSDK[Dimethyl]',\n 'GSLEK[Dimethyl]GSPEDK',\n 'SEASSEFAKK[Dimethyl]',\n 'SPDEAYAIAK[Dimethyl]K',\n 'LGVATVAKGVC[Carbamidomethyl]GAC[Carbamidomethyl]K[Dimethyl]',\n 'VQGGALEDSQLVAGVAFKK[Dimethyl]',\n 'VTLEVGK[Dimethyl]VIQQGR',\n 'DLSTVEALQNLKNLK[Dimethyl]',\n 'AANM[Oxidation]LQQSGSK[Dimethyl]NTGAK',\n 'GTISAPGK[Dimethyl]VVTAAAQAK',\n 'IQTK[Dimethyl]AGEATVK',\n 'SK[Dimethyl]HLQEQLNELK',\n 'TVSK[Dimethyl]VDDFLANEAK',\n 'TPNLYIYSKK[Dimethyl]',\n 'ASAISIK[Dimethyl]LGSSK',\n 'LM[Oxidation]DQNLK[Dimethyl]C[Carbamidomethyl]LSAAEEK',\n 'DLGK[Dimethyl]FQVATDALK',\n 'ATVASSTQKFQDLGVK[Dimethyl]',\n 'IHNAENIQPGEQK[Dimethyl]',\n 'SGNFSAAM[Oxidation]KDLSGK[Dimethyl]',\n 'GFC[Carbamidomethyl]FITYTDEEPVK[Dimethyl]K',\n 'ITQDLM[Oxidation]AK[Dimethyl]VR',\n 'AM[Oxidation]EAASSLK[Dimethyl]SQAATK',\n 'ITLSK[Dimethyl]HQNVQLPR',\n 'VETTVTSLK[Dimethyl]TK',\n 'SITHSTVGSKGEK[Dimethyl]',\n 'LQDVSGQLSSSK[Dimethyl]K',\n 'SPSDSGYSYETIGK[Dimethyl]TTK',\n 'VSPASSVDSNIPSSQGYK[Dimethyl]K',\n 'AVVK[Dimethyl]TDIQIALPSGC[Carbamidomethyl]YGR',\n 'IISSIEQKEENK[Dimethyl]',\n 'ATVASSTQK[Dimethyl]FQDLGVK',\n 'DQEALM[Oxidation]K[Dimethyl]SVK',\n 'SDDESSQK[Dimethyl]DIK',\n 'SITHSTVGSK[Dimethyl]GEK',\n 'ALASIDSKLNQAK[Dimethyl]',\n 'DASTLQSQK[Dimethyl]AEGTGDAK',\n 'DK[Dimethyl]ALPLEAEVAPVK',\n 'GFALVGVGSEASSK[Dimethyl]K',\n 'GAEK[Dimethyl]TEVAEPR',\n 'VGDDIAK[Dimethyl]ATGDWK',\n 'IINEPTAAAIAYGLDKK[Dimethyl]',\n 'DTM[Oxidation]GIADK[Dimethyl]TENTLER',\n 'LTM[Oxidation]LNTVSK[Dimethyl]IR',\n 'VDIDAPDVSIEGPDAK[Dimethyl]LK',\n 'AANDAGYFNDEM[Oxidation]APIEVK[Dimethyl]TK',\n 'GTVPDDAVEALADSLGKK[Dimethyl]',\n 'LFQSDTNAM[Oxidation]LGKK[Dimethyl]',\n 'SGNFSAAM[Oxidation]K[Dimethyl]DLSGK',\n 'LQAAYAGDK[Dimethyl]ADDIQK',\n 'DVNAAIAAIKTK[Dimethyl]',\n 'LLIVSTTPYSEK[Dimethyl]DTK',\n 'HELQANC[Carbamidomethyl]YEEVK[Dimethyl]DR',\n 'ITASLC[Carbamidomethyl]DLK[Dimethyl]SR',\n 'GTVPDDAVEALADSLGK[Dimethyl]K',\n 'FLQEFYQDDELGK[Dimethyl]K',\n 'LSSAM[Oxidation]SAAK[Dimethyl]AIC[Carbamidomethyl]DHVR',\n 'VDVEVPDVSLEGPEGK[Dimethyl]LK',\n 'GDFTFFIDTFK[Dimethyl]',\n 'SNEEGSEEK[Dimethyl]GPEVR',\n 'YIEIYVQK[Dimethyl]VNPSR',\n 'LLEEEIQAPTSSK[Dimethyl]R',\n 'LTEKELAEAASK[Dimethyl]',\n 'LDEK[Dimethyl]ENLSAK',\n 'SQNTDM[Oxidation]VQKSVSK[Dimethyl]',\n 'DLSTVEALQNLK[Dimethyl]NLK',\n 'ILDLK[Dimethyl]TGTVK',\n 'AANDAGYFNDEM[Oxidation]APIEVKTK[Dimethyl]',\n 'LLEEEDSKLK[Dimethyl]',\n 'AGQK[Dimethyl]LIDVNHYAK',\n 'LQDEIQNM[Oxidation]K[Dimethyl]EEM[Oxidation]AR',\n 'SSGPGGQNVNKVNSK[Dimethyl]',\n 'GFC[Carbamidomethyl]FITYTDEEPVKK[Dimethyl]',\n 'ISSNFSSIIAEK[Dimethyl]LR',\n 'IVSGK[Dimethyl]DYNVTANSK',\n 'TQAYQDQK[Dimethyl]PGTSGLR',\n 'GFALVGVGSEASSKK[Dimethyl]',\n 'AAQASDLEK[Dimethyl]IHLDEK',\n 'IHNAENIQPGEQK[Dimethyl]YEYK',\n 'YEEIVK[Dimethyl]EVSTYIK',\n 'ASNAAVK[Dimethyl]LAESK',\n 'DDSFLGK[Dimethyl]LGGTLAR',\n 'AANM[Oxidation]LQQSGSKNTGAK[Dimethyl]',\n 'SDSGK[Dimethyl]PYYYNSQTK',\n 'IVFSGNLFQHQEDSKK[Dimethyl]',\n 'DIVAAIQHNYKM[Oxidation]SAFK[Dimethyl]',\n 'ALASIDSK[Dimethyl]LNQAK',\n 'VPTTEK[Dimethyl]PTVTVNFR',\n 'LEEVTGK[Dimethyl]LQVAR',\n 'DSSPTTNSK[Dimethyl]LSALQR',\n 'TEVLSPNSK[Dimethyl]VESK',\n 'SSGPGGQNVNK[Dimethyl]VNSK',\n 'TLHLSHLNR[Dimethyl]FPAR',\n 'IR[Dimethyl]M[Oxidation]THNLLLNYGLYR',\n 'HSPTLPEPGGLRLELPLSR[Dimethyl]',\n 'GR[Dimethyl]GGAESHTFK',\n 'R[Dimethyl]FGGPVHHQAQR',\n 'LLALSGPGGGRGR[Dimethyl]',\n 'YSSNLGNFNYEQRGAFR[Dimethyl]',\n 'AVPSSPAGSAARGR[Dimethyl]',\n 'LLR[Dimethyl]GVGGSHGR',\n 'GR[Dimethyl]GAAAAGGPQR',\n 'HTPTPGIYM[Oxidation]GR[Dimethyl]PTYGSSR',\n 'HTLLR[Dimethyl]APAPELLPANVAGR',\n 'IQNGAQGIR[Dimethyl]FIYTR',\n 'ATSAGSSPSC[Carbamidomethyl]SLAGR[Dimethyl]GVSSR',\n 'HSPTLPEPGGLR[Dimethyl]LELPLSR',\n 'LPPGALAHR[Dimethyl]LPTTGYLVYR',\n 'ATSAGSSPSC[Carbamidomethyl]SLAGRGVSSR[Dimethyl]',\n 'LFHLPEYTWR[Dimethyl]LPC[Carbamidomethyl]GGVR',\n 'HTPTPGIYM[Oxidation]GRPTYGSSR[Dimethyl]',\n 'SR[Dimethyl]GFGFITFTNPEHASVAM[Oxidation]R',\n 'LVIFTNQMSIGR[Dimethyl]GK',\n 'M[Oxidation]R[Dimethyl]GGGFGDR',\n 'YGLIFHSTFIGR[Dimethyl]AAAK',\n 'AVSTGDC[Carbamidomethyl]GQVLRGGVIQSTR[Dimethyl]',\n 'VEPGSLESSPGLGRGR[Dimethyl]',\n 'SHLR[Dimethyl]HAIPSAER',\n 'SGGGYGGDRSSGGGYSGDR[Dimethyl]',\n 'FR[Dimethyl]SGNYR',\n 'PAR[Dimethyl]GLYSR',\n 'GMDTR[Dimethyl]GPVPGPR',\n 'M[Oxidation]EGSDFESSGGR[Dimethyl]',\n 'VELPTC[Carbamidomethyl]M[Oxidation]YRLPNVHGR[Dimethyl]',\n 'IR[Dimethyl]MTHNLLLNYGLYR',\n 'VEPIHAVVLPR[Dimethyl]',\n 'GAGR[Dimethyl]GSLWR',\n 'MEGSDFESSGGR[Dimethyl]GLK',\n 'AAYQAAIR[Dimethyl]',\n 'YSSNLGNFNYEQR[Dimethyl]',\n 'FGDM[Oxidation]SYSIR[Dimethyl]',\n 'ANYNFR[Dimethyl]GMYNQR',\n 'LGTTPSVYGGAGGR[Dimethyl]',\n 'LFHLPEYTWR[Dimethyl]',\n 'SGQGAFGNM[Oxidation]C[Carbamidomethyl]RGGR[Dimethyl]',\n 'MGAGMGFGLER[Dimethyl]MAAPIDR',\n 'AVSILPLLGHGVPR[Dimethyl]',\n 'AQVLAGGR[Dimethyl]',\n 'FGDMSYSIR[Dimethyl]HSISM[Oxidation]PAM[Oxidation]R',\n 'IEVIEIMTDR[Dimethyl]',\n 'TNAAAM[Oxidation]NMGR[Dimethyl]PFQK',\n 'AVSILPLLGHGVPR[Dimethyl]LPPGR',\n 'MTGSSGGDR[Dimethyl]GGFK',\n 'SGQGAFGNM[Oxidation]C[Carbamidomethyl]R[Dimethyl]',\n 'SQDVR[Dimethyl]MNGPM[Oxidation]GAGNSVR',\n 'TLHLSHLNR[Dimethyl]',\n 'M[Oxidation]GAGM[Oxidation]GFGLER[Dimethyl]MAAPIDR',\n 'GLATFPSR[Dimethyl]',\n 'GM[Oxidation]GPGTPAGYGR[Dimethyl]',\n 'GFLQPPPLRQPR[Dimethyl]',\n 'MR[Dimethyl]GNDTFGPR',\n 'LLALSGPGGGR[Dimethyl]',\n 'R[Dimethyl]GGPGGPPGPLMEQM[Oxidation]GGR',\n 'LVIFTNQM[Oxidation]SIGR[Dimethyl]',\n 'LVLPSQAQAR[Dimethyl]',\n 'WGWPSDNR[Dimethyl]SNQWHGR',\n 'M[Oxidation]GAGMGFGLER[Dimethyl]MAAPIDR',\n 'GMGPGTPAGYGRGR[Dimethyl]',\n 'GGPMR[Dimethyl]GGPGPGPGPYHR',\n 'DFAFYNPR[Dimethyl]',\n 'GGPR[Dimethyl]GGLGGGMR',\n 'FSHSVQNSER[Dimethyl]',\n 'HLR[Dimethyl]GHGPQPPLR',\n 'GR[Dimethyl]GVGSPEPGPTAPYLGRR',\n 'TNAAAMNMGR[Dimethyl]PFQK',\n 'GFLQPPPLR[Dimethyl]',\n 'FGPGVAFR[Dimethyl]',\n 'MSGR[Dimethyl]GVGSVSQFK',\n 'GGPGGELPR[Dimethyl]',\n 'PGTPAGYGR[Dimethyl]',\n 'GSYLGGYSAGR[Dimethyl]',\n 'GNLEGPWGGGR[Dimethyl]',\n 'M[Oxidation]GAGMGFGLER[Dimethyl]M[Oxidation]AAPIDR',\n 'GM[Oxidation]GPGTPAGYGRGR[Dimethyl]',\n 'SNQWHGR[Dimethyl]SWGNNYPQHR',\n 'FR[Dimethyl]GFMQK',\n 'STNVTVAAAR[Dimethyl]',\n 'ATEGSGSMR[Dimethyl]GGGGGNAR',\n 'GGPM[Oxidation]RGGPGPGPGPYHR[Dimethyl]',\n 'LQLGTPPPLLAAR[Dimethyl]',\n 'GAPLSSAPILGGWFGR[Dimethyl]',\n 'R[Dimethyl]GGPGGPPGPLM[Oxidation]EQMGGR',\n 'YSSSSIPEPFGSR[Dimethyl]',\n 'FGDM[Oxidation]SYSIR[Dimethyl]HSISMPAM[Oxidation]R',\n 'MGAGMGFGLER[Dimethyl]M[Oxidation]AAPIDR',\n 'AVSTGDC[Carbamidomethyl]GQVLR[Dimethyl]',\n 'VELPTC[Carbamidomethyl]M[Oxidation]YR[Dimethyl]',\n 'RQELEQVLGIR[Dimethyl]',\n 'ILILR[Dimethyl]QTTADWWWGER',\n 'AMLPGSPLFTR[Dimethyl]',\n 'LGILPVGR[Dimethyl]',\n 'PMGAR[Dimethyl]GQC[Carbamidomethyl]M[Oxidation]SGASR',\n 'SGQGAFGNM[Oxidation]C[Carbamidomethyl]R[Dimethyl]GGR',\n 'VTVAR[Dimethyl]GSALEMEFK',\n 'C[Carbamidomethyl]VEGQLLR[Dimethyl]',\n 'SFDEVEGVFGR[Dimethyl]',\n 'VPLLGSR[Dimethyl]',\n 'AVSILPLLGHGVPRLPPGR[Dimethyl]',\n 'VEPGSLESSPGLGR[Dimethyl]',\n 'M[Oxidation]GAGMGFGLER[Dimethyl]',\n 'TSAGWLPSFGR[Dimethyl]',\n 'YSSSSIPEPFGSRGSPR[Dimethyl]',\n 'MGAGMGFGLER[Dimethyl]',\n 'DC[Carbamidomethyl]GTLPNR[Dimethyl]',\n 'C[Carbamidomethyl]VHPR[Dimethyl]GGVIQSVSSWK',\n 'SGGYGGSRDYYSSR[Dimethyl]',\n 'AM[Oxidation]LPGSPLFTR[Dimethyl]',\n 'MGAGM[Oxidation]GFGLER[Dimethyl]M[Oxidation]AAPIDR',\n 'IEVIEIM[Oxidation]TDR[Dimethyl]',\n 'MGAGM[Oxidation]GFGLER[Dimethyl]',\n 'R[Dimethyl]GMAPASTDDLFAR',\n 'SR[Dimethyl]GFGFITFTNPEHASVAMR',\n 'SMR[Dimethyl]GGYIGSTYFER',\n 'QELEQVLGIR[Dimethyl]',\n 'HLQLSFQEPHFYLR[Dimethyl]C[Carbamidomethyl]LGK',\n 'AVPSSPAGSAAR[Dimethyl]',\n 'M[Oxidation]GAGM[Oxidation]GFGLER[Dimethyl]',\n 'AMLPGSPLFTR[Dimethyl]APPPK',\n 'MEGSDFESSGGR[Dimethyl]',\n 'M[Oxidation]TEDALR[Dimethyl]',\n 'TLHLSHLNRFPAR[Dimethyl]',\n 'PGGLGSSSLYGLGASR[Dimethyl]',\n 'MSEGR[Dimethyl]GLPPPPR',\n 'YGLIFHSTFIGR[Dimethyl]',\n 'LIPPLLSLPPPPWGR[Dimethyl]',\n 'QELKQELELR[Dimethyl]',\n 'MGAGM[Oxidation]GFGLER[Dimethyl]MAAPIDR',\n 'M[Oxidation]SFVK[Dimethyl]GWGAEYR',\n 'PLR[Dimethyl]AASPFR',\n 'IVEK[Dimethyl]YGYTHLSAGELLR',\n 'PANEK[Dimethyl]ATDDYHYEK',\n 'DAEAAEATAEGALK[Dimethyl]AEK',\n 'INNFSADIKDSK[Dimethyl]',\n 'LLLPGELAK[Dimethyl]HAVSEGTK',\n 'DTGEK[Dimethyl]LTVAENEAETK',\n 'IGNSYFKEEK[Dimethyl]',\n 'DAIHFYNK[Dimethyl]SLAEHR',\n 'FDDTNPEK[Dimethyl]EEAK',\n 'TEDEVLTSK[Dimethyl]GDAWAK',\n 'VYIASSSGSTAIKK[Dimethyl]',\n 'HAEM[Oxidation]VHTGLK[Dimethyl]LER',\n 'DAEAAEATAEGALKAEK[Dimethyl]',\n 'VFSIISSEKELK[Dimethyl]',\n 'SQDVR[Dimethyl]M[Oxidation]NGPMGAGNSVR',\n 'NEIM[Oxidation]ELNRMTQR[Dimethyl]',\n 'GM[Oxidation]GPGTPAGYGR[Dimethyl]GR',\n 'IQNGAQGIRFIYTR[Dimethyl]',\n 'TSAGWLPSFGR[Dimethyl]VWNNGR',\n 'GPPPR[Dimethyl]GGMAQK',\n 'LGSGR[Dimethyl]GTGQMEGR',\n 'GGGGGGGGGAPRGR[Dimethyl]',\n 'GGPR[Dimethyl]GIVGGGM[Oxidation]MR',\n 'TSAGWLPSFGRVWNNGR[Dimethyl]',\n 'M[Oxidation]GAGM[Oxidation]GFGLERMAAPIDR[Dimethyl]',\n 'MSFVK[Dimethyl]GWGAEYR',\n 'VLTANSNPSSPSAAK[Dimethyl]',\n 'VNVEAPDVNLEGLGGK[Dimethyl]',\n 'AANM[Oxidation]LQQSGSK[Dimethyl]',\n 'DFDTALK[Dimethyl]',\n 'GADFLVTEVENGGSLGSK[Dimethyl]',\n 'TLDEILQEK[Dimethyl]',\n 'VNIGQGSHPQK[Dimethyl]',\n 'IQQELQTAK[Dimethyl]',\n 'YEQEHAAIQDK[Dimethyl]',\n 'SM[Oxidation]STEGLM[Oxidation]K[Dimethyl]',\n 'VDIDVPDVNIEGPDAK[Dimethyl]',\n 'TGASWTDNIM[Oxidation]AQK[Dimethyl]C[Carbamidomethyl]SK',\n 'ASDTSSETVFGK[Dimethyl]',\n 'TGASWTDNIM[Oxidation]AQKC[Carbamidomethyl]SK[Dimethyl]',\n 'LLIVSTTPYSEK[Dimethyl]',\n 'EEEDLSK[Dimethyl]K',\n 'LLEEEIQAPTSSK[Dimethyl]',\n 'IHVQSSSDSSDEPAEK[Dimethyl]',\n 'DEPSVAAM[Oxidation]VYPFTGDHK[Dimethyl]',\n 'GTVPDDAVEALADSLGK[Dimethyl]',\n 'TTGFYSGFSEVAEK[Dimethyl]',\n 'LQDVSGQLSSSK[Dimethyl]',\n 'AANDAGYFNDEM[Oxidation]APIEVK[Dimethyl]',\n 'LQDEIQNM[Oxidation]K[Dimethyl]',\n 'DAEAAEATAEGALK[Dimethyl]',\n 'AM[Oxidation]EAASSLK[Dimethyl]',\n 'AGPNASIISLK[Dimethyl]',\n 'VGLQVVAVK[Dimethyl]',\n 'DLSTVEALQNLK[Dimethyl]',\n 'LQAAYAGDK[Dimethyl]',\n 'LTM[Oxidation]LNTVSK[Dimethyl]',\n 'ITASLC[Carbamidomethyl]DLK[Dimethyl]',\n 'VGVEVPDVNIEGPEGK[Dimethyl]',\n 'FGTINIVHPKLGSYTK[Dimethyl]',\n 'STGLELETPSLVPVK[Dimethyl]',\n 'DLSK[Dimethyl]TTLVK',\n 'VGDDIAK[Dimethyl]',\n 'MAQELK[Dimethyl]YGDIVER',\n 'IINEPTAAAIAYGLDK[Dimethyl]',\n 'DIVAAIQHNYK[Dimethyl]',\n 'ITESVAETAQTIK[Dimethyl]',\n 'TGASWTDNIM[Oxidation]AQK[Dimethyl]',\n 'ISSNFSSIIAEK[Dimethyl]',\n 'SK[Dimethyl]LLFSNTAAQK',\n 'FLQEFYQDDELGK[Dimethyl]',\n 'GFC[Carbamidomethyl]FITYTDEEPVK[Dimethyl]',\n 'VGIDTPDIDIHGPEGK[Dimethyl]LK',\n 'SPSDSGYSYETIGK[Dimethyl]',\n 'IPIANTEK[Dimethyl]YM[Oxidation]ADK',\n 'SLFFGGK[Dimethyl]',\n 'ISALQGK[Dimethyl]',\n 'TEDEVLTSK[Dimethyl]',\n 'DNSTM[Oxidation]GYMAAK[Dimethyl]K',\n 'VDVEVPDVSLEGPEGK[Dimethyl]',\n 'VSPASSVDSNIPSSQGYK[Dimethyl]',\n 'GTITVSAQELK[Dimethyl]',\n 'DTPTSAGPNSFNK[Dimethyl]',\n 'VGIDTPDIDIHGPEGKLK[Dimethyl]',\n 'VYIASSSGSTAIK[Dimethyl]',\n 'GEVGAGAGPGAQAGPSAK[Dimethyl]',\n 'IVFSGNLFQHQEDSK[Dimethyl]',\n 'GFALVGVGSEASSK[Dimethyl]',\n 'AMEAASSLK[Dimethyl]SQAATK',\n 'KIQQELQTAK[Dimethyl]K',\n 'KDVDEAYM[Oxidation]NK[Dimethyl]',\n 'VTLEVGK[Dimethyl]',\n 'IGNSYFK[Dimethyl]',\n 'SLGEPAAGEGSAK[Dimethyl]',\n 'AM[Oxidation]EAASSLK[Dimethyl]SQAATKK',\n 'PSVGSQSNQAGQGK[Dimethyl]',\n 'DNSTM[Oxidation]GYM[Oxidation]AAK[Dimethyl]',\n 'VTAAAMAGNK[Dimethyl]STPR',\n 'IDEM[Oxidation]PEAAVK[Dimethyl]',\n 'VLDSGAPIK[Dimethyl]IPVGPETLGR',\n 'SEASSEFAK[Dimethyl]',\n 'TQDQISNIK[Dimethyl]',\n 'GGPGSTLSFVGK[Dimethyl]',\n 'SSVAC[Carbamidomethyl]K[Dimethyl]WNLAEAQQK',\n 'VGTASVLQPVK[Dimethyl]',\n 'VGEFSGANK[Dimethyl]',\n 'IISSIEQK[Dimethyl]',\n 'DTMGIADK[Dimethyl]TENTLER',\n 'VFIGNLNTLVVK[Dimethyl]',\n 'AAQASDLEK[Dimethyl]',\n 'LQLEIDQK[Dimethyl]',\n 'AQQHWGSGVGVK[Dimethyl]',\n 'ITQDLM[Oxidation]AK[Dimethyl]',\n 'DESFLGK[Dimethyl]',\n 'TEDEVLTSKGDAWAK[Dimethyl]',\n 'VQGGALEDSQLVAGVAFK[Dimethyl]',\n 'LTSLNVKYNNDK[Dimethyl]',\n 'LFQSDTNAM[Oxidation]LGK[Dimethyl]',\n 'VDIDAPDVSIEGPDAK[Dimethyl]',\n 'LLFSNTAAQK[Dimethyl]',\n 'IDLPAENSNSETIIITGK[Dimethyl]',\n 'ASEALLK[Dimethyl]',\n 'SLINDYVK[Dimethyl]NK',\n 'DYSAPVNFISAGLK[Dimethyl]',\n 'FGTINIVHPK[Dimethyl]',\n 'DVNAAIAAIK[Dimethyl]',\n 'FSM[Oxidation]PGFK[Dimethyl]',\n 'VGIDTPDIDIHGPEGK[Dimethyl]',\n 'HAEMVHTGLK[Dimethyl]LER',\n 'DGSR[Dimethyl]HPR[Dimethyl]SHDEDR[Dimethyl]']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_test[columns]\n",
    "my_test = [reform_seq(df_test['seq'][i],m) for i, m in enumerate (df_test['modifications'])]\n",
    "my_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-23T16:24:36.529997Z",
     "end_time": "2023-08-23T16:24:36.529997Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                      0  Unnamed: 0          0          1\n0                ITESVAETAQTIK[Acetyl]K           0  31.570625  31.919634\n1                    ILDLK[Acetyl]TGTVK           1  36.621750  36.864071\n2                   GAEK[Acetyl]TEVAEPR           2  20.129126  20.192793\n3             IHNAENIQPGEQK[Acetyl]YEYK           3  25.592251  26.082335\n4    DTM[Oxidation]GIADK[Acetyl]TENTLER           4  33.042500  33.772171\n..                                  ...         ...        ...        ...\n234                VGTASVLQPVK[Acetyl]K         234  31.625500  32.233959\n235                MSFVK[Acetyl]GWGAEYR         235  42.429501  41.366127\n236             SSGPGGQNVNK[Acetyl]VNSK         236  16.240625  16.090347\n237           SPSDSGYSYETIGK[Acetyl]TTK         237  30.199375  30.594339\n238               VGDDIAK[Acetyl]ATGDWK         238  34.246250  34.422810\n\n[239 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>Unnamed: 0</th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ITESVAETAQTIK[Acetyl]K</td>\n      <td>0</td>\n      <td>31.570625</td>\n      <td>31.919634</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ILDLK[Acetyl]TGTVK</td>\n      <td>1</td>\n      <td>36.621750</td>\n      <td>36.864071</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GAEK[Acetyl]TEVAEPR</td>\n      <td>2</td>\n      <td>20.129126</td>\n      <td>20.192793</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>IHNAENIQPGEQK[Acetyl]YEYK</td>\n      <td>3</td>\n      <td>25.592251</td>\n      <td>26.082335</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DTM[Oxidation]GIADK[Acetyl]TENTLER</td>\n      <td>4</td>\n      <td>33.042500</td>\n      <td>33.772171</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>234</th>\n      <td>VGTASVLQPVK[Acetyl]K</td>\n      <td>234</td>\n      <td>31.625500</td>\n      <td>32.233959</td>\n    </tr>\n    <tr>\n      <th>235</th>\n      <td>MSFVK[Acetyl]GWGAEYR</td>\n      <td>235</td>\n      <td>42.429501</td>\n      <td>41.366127</td>\n    </tr>\n    <tr>\n      <th>236</th>\n      <td>SSGPGGQNVNK[Acetyl]VNSK</td>\n      <td>236</td>\n      <td>16.240625</td>\n      <td>16.090347</td>\n    </tr>\n    <tr>\n      <th>237</th>\n      <td>SPSDSGYSYETIGK[Acetyl]TTK</td>\n      <td>237</td>\n      <td>30.199375</td>\n      <td>30.594339</td>\n    </tr>\n    <tr>\n      <th>238</th>\n      <td>VGDDIAK[Acetyl]ATGDWK</td>\n      <td>238</td>\n      <td>34.246250</td>\n      <td>34.422810</td>\n    </tr>\n  </tbody>\n</table>\n<p>239 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=pd.concat([pd.DataFrame(my_test),pd.read_csv('./saved_models/14ptms/'+mod+'_results.csv')],axis=1)\n",
    "\n",
    "temp"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-09T10:27:04.530810Z",
     "end_time": "2023-08-09T10:27:04.550743Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ True, False, False, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       ...,\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('./data_matrix/14ptm/diamino13_stan/Acetyl/test_x.npy')[6]==np.load('./data_matrix/14ptm/diamino13_p1_stan/Acetyl/test_x.npy')[6]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-07T14:18:05.722850Z",
     "end_time": "2023-07-07T14:18:05.737371Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.    , -0.9487, -0.9487, ...,  0.    ,  0.    ,  0.    ],\n       [ 0.    , -1.967 , -1.967 , ...,  0.    ,  0.    ,  0.    ],\n       [ 0.    , -1.997 , -1.997 , ...,  0.    ,  0.    ,  0.    ],\n       ...,\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n      dtype=float16)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('./data_matrix/14ptm/diamino13_stan/Trimethyl/train_x.npy')[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-07T12:31:27.980365Z",
     "end_time": "2023-07-07T12:31:28.011995Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.    , -0.9497, -0.9497, ...,  0.    ,  0.    ,  0.    ],\n       [ 0.    , -1.967 , -1.967 , ...,  0.    ,  0.    ,  0.    ],\n       [ 0.    , -1.997 , -1.997 , ...,  0.    ,  0.    ,  0.    ],\n       ...,\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n      dtype=float16)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('./data_matrix/14ptm/diamino13_p1_stan/Trimethyl/train_x.npy')[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T17:12:28.966977Z",
     "end_time": "2023-07-06T17:12:28.983610Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "name                    Propionyl#K  Oxidation#M  Carbamidomethyl#C  \\\nBalabanJ_rdkit             0.000000     0.000000           0.000000   \nChi0n_rdkit                8.453945     5.740873           5.818223   \nLabuteASA_rdkit           83.665819    61.868920          68.119189   \nMinPartialCharge_rdkit    -0.480086    -0.480086          -0.480064   \nMolLogP_rdkit              0.094800    -0.833100          -1.383200   \nTPSA_rdkit                92.420000    80.390000         106.410000   \nSlogP_VSA1_rdkit          11.050456     5.733667          11.467335   \nSlogP_VSA2_rdkit          29.569610    33.335195          34.530560   \nSMR_VSA1_rdkit            14.695602    14.109963          14.695602   \nSMR_VSA2_rdkit             0.000000     0.000000           0.000000   \nPEOE_VSA1_rdkit           16.156983    10.840195          16.573862   \nPEOE_VSA2_rdkit            9.589074     9.003436           9.589074   \nMolWt_rdkit              202.254000   165.214000         178.213000   \n\nname                      Methyl#C    Methyl#E    Methyl#N    Methyl#K  \\\nBalabanJ_rdkit            0.000000    0.000000    0.000000    0.000000   \nChi0n_rdkit               4.625517    6.240873    5.625517    6.838590   \nLabuteASA_rdkit          52.252834   64.134675   58.206050   66.774412   \nMinPartialCharge_rdkit   -0.480064   -0.480086   -0.480075   -0.480086   \nMolLogP_rdkit            -0.238700   -0.648500   -1.465600   -0.212000   \nTPSA_rdkit               63.320000   89.620000   92.420000   75.350000   \nSlogP_VSA1_rdkit          5.733667    5.733667   11.050456   11.050456   \nSlogP_VSA2_rdkit         29.126296   30.196776   30.072525   30.710102   \nSMR_VSA1_rdkit            9.901065   19.432465   14.695602    9.901065   \nSMR_VSA2_rdkit            0.000000    0.000000    0.000000    0.000000   \nPEOE_VSA1_rdkit          10.840195   15.577058   16.156983   16.156983   \nPEOE_VSA2_rdkit           4.794537    9.589074    9.589074    4.794537   \nMolWt_rdkit             135.188000  161.157000  146.146000  160.217000   \n\nname                    Dimethyl#N  Dimethyl#K  Dimethyl#R  ...    Formyl#K  \\\nBalabanJ_rdkit            0.000000    0.000000    0.000000  ...    0.000000   \nChi0n_rdkit               6.572731    7.785803    8.578696  ...    6.824188   \nLabuteASA_rdkit          64.780964   73.349326   83.584539  ...   70.935935   \nMinPartialCharge_rdkit   -0.480075   -0.480086   -0.480086  ...   -0.480086   \nMolLogP_rdkit            -1.123400    0.130200   -1.026700  ...   -0.685400   \nTPSA_rdkit               83.630000   66.560000   99.740000  ...   92.420000   \nSlogP_VSA1_rdkit          5.733667    5.733667   16.367245  ...   11.050456   \nSlogP_VSA2_rdkit         42.020107   42.657684   43.717328  ...   30.072525   \nSMR_VSA1_rdkit           14.695602    9.901065    9.901065  ...   14.695602   \nSMR_VSA2_rdkit            0.000000    0.000000    0.000000  ...    0.000000   \nPEOE_VSA1_rdkit          15.740105   15.740105   21.473772  ...   16.156983   \nPEOE_VSA2_rdkit           9.589074    4.794537    9.786942  ...    9.589074   \nMolWt_rdkit             160.173000  174.244000  202.258000  ...  174.200000   \n\nname                    Deamidated#R  Deamidated#F  Deamidated#N  \\\nBalabanJ_rdkit              0.000000      0.000000      0.000000   \nChi0n_rdkit                 6.578696      6.473884      4.572731   \nLabuteASA_rdkit            69.889055     70.276219     51.085480   \nMinPartialCharge_rdkit     -0.481013     -0.479319     -0.481175   \nMolLogP_rdkit              -0.739130      0.674600     -1.127000   \nTPSA_rdkit                119.430000     57.530000    100.620000   \nSlogP_VSA1_rdkit           11.050456      0.000000      5.733667   \nSlogP_VSA2_rdkit           34.790637     22.286326     28.193506   \nSMR_VSA1_rdkit             15.007592     15.007592     19.802129   \nSMR_VSA2_rdkit              5.409284      0.000000      0.000000   \nPEOE_VSA1_rdkit            21.263511     10.213055     15.946722   \nPEOE_VSA2_rdkit            10.203821      0.000000      9.589074   \nMolWt_rdkit               175.188000    166.176000    133.103000   \n\nname                    Deamidated#Q   Malonyl#K     Nitro#Y    Biotin#K  \\\nBalabanJ_rdkit              0.000000    0.000000    0.000000    0.000000   \nChi0n_rdkit                 5.279838    8.809407    8.160244   14.330919   \nLabuteASA_rdkit            57.450422   92.621575   90.269085  151.593108   \nMinPartialCharge_rdkit     -0.481229   -0.480854   -0.502092   -0.480086   \nMolLogP_rdkit              -0.736900   -0.840500    0.254800    0.410600   \nTPSA_rdkit                100.620000  129.720000  126.690000  133.550000   \nSlogP_VSA1_rdkit            5.733667   11.050456    5.733667   21.684033   \nSlogP_VSA2_rdkit           28.193506   40.645442   27.147512   58.687197   \nSMR_VSA1_rdkit             19.802129   24.596666   19.930903   19.490139   \nSMR_VSA2_rdkit              0.000000    0.000000    0.000000    0.000000   \nPEOE_VSA1_rdkit            15.946722   21.263511   15.946722   26.790561   \nPEOE_VSA2_rdkit             9.589074   14.383612   14.908855    9.589074   \nMolWt_rdkit               147.130000  232.236000  226.188000  372.491000   \n\nname                    Oxidation#P    Methyl#R  \nBalabanJ_rdkit             0.000000    0.000000  \nChi0n_rdkit                4.871590    7.631483  \nLabuteASA_rdkit           52.493040   77.009626  \nMinPartialCharge_rdkit    -0.480080   -0.480086  \nMolLogP_rdkit             -1.206200   -1.077730  \nTPSA_rdkit                69.560000  111.230000  \nSlogP_VSA1_rdkit           5.316789   16.367245  \nSlogP_VSA2_rdkit          34.872924   36.669656  \nSMR_VSA1_rdkit            15.007592    9.901065  \nSMR_VSA2_rdkit             0.000000    5.409284  \nPEOE_VSA1_rdkit           15.529843   21.473772  \nPEOE_VSA2_rdkit            4.794537   10.203821  \nMolWt_rdkit              131.131000  188.231000  \n\n[13 rows x 36 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>name</th>\n      <th>Propionyl#K</th>\n      <th>Oxidation#M</th>\n      <th>Carbamidomethyl#C</th>\n      <th>Methyl#C</th>\n      <th>Methyl#E</th>\n      <th>Methyl#N</th>\n      <th>Methyl#K</th>\n      <th>Dimethyl#N</th>\n      <th>Dimethyl#K</th>\n      <th>Dimethyl#R</th>\n      <th>...</th>\n      <th>Formyl#K</th>\n      <th>Deamidated#R</th>\n      <th>Deamidated#F</th>\n      <th>Deamidated#N</th>\n      <th>Deamidated#Q</th>\n      <th>Malonyl#K</th>\n      <th>Nitro#Y</th>\n      <th>Biotin#K</th>\n      <th>Oxidation#P</th>\n      <th>Methyl#R</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>BalabanJ_rdkit</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Chi0n_rdkit</th>\n      <td>8.453945</td>\n      <td>5.740873</td>\n      <td>5.818223</td>\n      <td>4.625517</td>\n      <td>6.240873</td>\n      <td>5.625517</td>\n      <td>6.838590</td>\n      <td>6.572731</td>\n      <td>7.785803</td>\n      <td>8.578696</td>\n      <td>...</td>\n      <td>6.824188</td>\n      <td>6.578696</td>\n      <td>6.473884</td>\n      <td>4.572731</td>\n      <td>5.279838</td>\n      <td>8.809407</td>\n      <td>8.160244</td>\n      <td>14.330919</td>\n      <td>4.871590</td>\n      <td>7.631483</td>\n    </tr>\n    <tr>\n      <th>LabuteASA_rdkit</th>\n      <td>83.665819</td>\n      <td>61.868920</td>\n      <td>68.119189</td>\n      <td>52.252834</td>\n      <td>64.134675</td>\n      <td>58.206050</td>\n      <td>66.774412</td>\n      <td>64.780964</td>\n      <td>73.349326</td>\n      <td>83.584539</td>\n      <td>...</td>\n      <td>70.935935</td>\n      <td>69.889055</td>\n      <td>70.276219</td>\n      <td>51.085480</td>\n      <td>57.450422</td>\n      <td>92.621575</td>\n      <td>90.269085</td>\n      <td>151.593108</td>\n      <td>52.493040</td>\n      <td>77.009626</td>\n    </tr>\n    <tr>\n      <th>MinPartialCharge_rdkit</th>\n      <td>-0.480086</td>\n      <td>-0.480086</td>\n      <td>-0.480064</td>\n      <td>-0.480064</td>\n      <td>-0.480086</td>\n      <td>-0.480075</td>\n      <td>-0.480086</td>\n      <td>-0.480075</td>\n      <td>-0.480086</td>\n      <td>-0.480086</td>\n      <td>...</td>\n      <td>-0.480086</td>\n      <td>-0.481013</td>\n      <td>-0.479319</td>\n      <td>-0.481175</td>\n      <td>-0.481229</td>\n      <td>-0.480854</td>\n      <td>-0.502092</td>\n      <td>-0.480086</td>\n      <td>-0.480080</td>\n      <td>-0.480086</td>\n    </tr>\n    <tr>\n      <th>MolLogP_rdkit</th>\n      <td>0.094800</td>\n      <td>-0.833100</td>\n      <td>-1.383200</td>\n      <td>-0.238700</td>\n      <td>-0.648500</td>\n      <td>-1.465600</td>\n      <td>-0.212000</td>\n      <td>-1.123400</td>\n      <td>0.130200</td>\n      <td>-1.026700</td>\n      <td>...</td>\n      <td>-0.685400</td>\n      <td>-0.739130</td>\n      <td>0.674600</td>\n      <td>-1.127000</td>\n      <td>-0.736900</td>\n      <td>-0.840500</td>\n      <td>0.254800</td>\n      <td>0.410600</td>\n      <td>-1.206200</td>\n      <td>-1.077730</td>\n    </tr>\n    <tr>\n      <th>TPSA_rdkit</th>\n      <td>92.420000</td>\n      <td>80.390000</td>\n      <td>106.410000</td>\n      <td>63.320000</td>\n      <td>89.620000</td>\n      <td>92.420000</td>\n      <td>75.350000</td>\n      <td>83.630000</td>\n      <td>66.560000</td>\n      <td>99.740000</td>\n      <td>...</td>\n      <td>92.420000</td>\n      <td>119.430000</td>\n      <td>57.530000</td>\n      <td>100.620000</td>\n      <td>100.620000</td>\n      <td>129.720000</td>\n      <td>126.690000</td>\n      <td>133.550000</td>\n      <td>69.560000</td>\n      <td>111.230000</td>\n    </tr>\n    <tr>\n      <th>SlogP_VSA1_rdkit</th>\n      <td>11.050456</td>\n      <td>5.733667</td>\n      <td>11.467335</td>\n      <td>5.733667</td>\n      <td>5.733667</td>\n      <td>11.050456</td>\n      <td>11.050456</td>\n      <td>5.733667</td>\n      <td>5.733667</td>\n      <td>16.367245</td>\n      <td>...</td>\n      <td>11.050456</td>\n      <td>11.050456</td>\n      <td>0.000000</td>\n      <td>5.733667</td>\n      <td>5.733667</td>\n      <td>11.050456</td>\n      <td>5.733667</td>\n      <td>21.684033</td>\n      <td>5.316789</td>\n      <td>16.367245</td>\n    </tr>\n    <tr>\n      <th>SlogP_VSA2_rdkit</th>\n      <td>29.569610</td>\n      <td>33.335195</td>\n      <td>34.530560</td>\n      <td>29.126296</td>\n      <td>30.196776</td>\n      <td>30.072525</td>\n      <td>30.710102</td>\n      <td>42.020107</td>\n      <td>42.657684</td>\n      <td>43.717328</td>\n      <td>...</td>\n      <td>30.072525</td>\n      <td>34.790637</td>\n      <td>22.286326</td>\n      <td>28.193506</td>\n      <td>28.193506</td>\n      <td>40.645442</td>\n      <td>27.147512</td>\n      <td>58.687197</td>\n      <td>34.872924</td>\n      <td>36.669656</td>\n    </tr>\n    <tr>\n      <th>SMR_VSA1_rdkit</th>\n      <td>14.695602</td>\n      <td>14.109963</td>\n      <td>14.695602</td>\n      <td>9.901065</td>\n      <td>19.432465</td>\n      <td>14.695602</td>\n      <td>9.901065</td>\n      <td>14.695602</td>\n      <td>9.901065</td>\n      <td>9.901065</td>\n      <td>...</td>\n      <td>14.695602</td>\n      <td>15.007592</td>\n      <td>15.007592</td>\n      <td>19.802129</td>\n      <td>19.802129</td>\n      <td>24.596666</td>\n      <td>19.930903</td>\n      <td>19.490139</td>\n      <td>15.007592</td>\n      <td>9.901065</td>\n    </tr>\n    <tr>\n      <th>SMR_VSA2_rdkit</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>5.409284</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.409284</td>\n    </tr>\n    <tr>\n      <th>PEOE_VSA1_rdkit</th>\n      <td>16.156983</td>\n      <td>10.840195</td>\n      <td>16.573862</td>\n      <td>10.840195</td>\n      <td>15.577058</td>\n      <td>16.156983</td>\n      <td>16.156983</td>\n      <td>15.740105</td>\n      <td>15.740105</td>\n      <td>21.473772</td>\n      <td>...</td>\n      <td>16.156983</td>\n      <td>21.263511</td>\n      <td>10.213055</td>\n      <td>15.946722</td>\n      <td>15.946722</td>\n      <td>21.263511</td>\n      <td>15.946722</td>\n      <td>26.790561</td>\n      <td>15.529843</td>\n      <td>21.473772</td>\n    </tr>\n    <tr>\n      <th>PEOE_VSA2_rdkit</th>\n      <td>9.589074</td>\n      <td>9.003436</td>\n      <td>9.589074</td>\n      <td>4.794537</td>\n      <td>9.589074</td>\n      <td>9.589074</td>\n      <td>4.794537</td>\n      <td>9.589074</td>\n      <td>4.794537</td>\n      <td>9.786942</td>\n      <td>...</td>\n      <td>9.589074</td>\n      <td>10.203821</td>\n      <td>0.000000</td>\n      <td>9.589074</td>\n      <td>9.589074</td>\n      <td>14.383612</td>\n      <td>14.908855</td>\n      <td>9.589074</td>\n      <td>4.794537</td>\n      <td>10.203821</td>\n    </tr>\n    <tr>\n      <th>MolWt_rdkit</th>\n      <td>202.254000</td>\n      <td>165.214000</td>\n      <td>178.213000</td>\n      <td>135.188000</td>\n      <td>161.157000</td>\n      <td>146.146000</td>\n      <td>160.217000</td>\n      <td>160.173000</td>\n      <td>174.244000</td>\n      <td>202.258000</td>\n      <td>...</td>\n      <td>174.200000</td>\n      <td>175.188000</td>\n      <td>166.176000</td>\n      <td>133.103000</td>\n      <td>147.130000</td>\n      <td>232.236000</td>\n      <td>226.188000</td>\n      <td>372.491000</td>\n      <td>131.131000</td>\n      <td>188.231000</td>\n    </tr>\n  </tbody>\n</table>\n<p>13 rows × 36 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('./struct_to_features_20230704.csv')\n",
    "df[df.columns[1]]=0\n",
    "df=df.set_index('name').T\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T16:21:38.393026Z",
     "end_time": "2023-07-06T16:21:38.424656Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod= Nitro min: 7.453849999999999 max: 61.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5208it [00:00, 8408.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5208 5208 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [00:00, 13103.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211 211 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "274it [00:00, 8968.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274 274 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [00:00, 13502.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211 211 Usable data= 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "standardized = True\n",
    "\n",
    "\n",
    "path = './Data_files/14ptm'\n",
    "mods = ['Nitro']\n",
    "sets = ['train','test','valid']\n",
    "columns = ['index_name','seq','modifications','tr','predictions']\n",
    "for mod in mods:\n",
    "    df_train = pd.read_csv(path + '/prosit_ptm_2020_' +mod+ '_modanalysis_'+sets[0]+'.csv',keep_default_na=False)\n",
    "    df_test = pd.read_csv(path + '/prosit_ptm_2020_' +mod+ '_modanalysis_'+sets[1]+'.csv',keep_default_na=False)\n",
    "    df_valid = pd.read_csv(path + '/prosit_ptm_2020_' +mod+ '_modanalysis_'+sets[2]+'.csv',keep_default_na=False)\n",
    "\n",
    "    df_train = df_train[columns]\n",
    "    df_test = df_test[columns]\n",
    "    df_valid = df_valid[columns]\n",
    "\n",
    "\n",
    "    maxx,minn=df_train['tr'].max(),df_train['tr'].min()\n",
    "    for i in [ df_test, df_valid]:\n",
    "        if i['tr'].min() < minn:\n",
    "            minn = int(i['tr'].min())\n",
    "        if i['tr'].max() > maxx:\n",
    "            maxx = int(i['tr'].max())\n",
    "    print('mod=',mod,'min:',minn,'max:',maxx)\n",
    "\n",
    "    for i in [df_train, df_test, df_valid]:\n",
    "        i['tr_norm'] = (i['tr'] - minn)/(maxx - minn)\n",
    "\n",
    "    df_test_no_mod=df_test.copy()\n",
    "    df_test_no_mod['modifications'].replace(f\"[|][*0-9]+[|]{mod}\", '',inplace=True,regex=True)\n",
    "    df_test_no_mod['modifications'].replace(f\"[*0-9]+[|]{mod}\", '',inplace=True,regex=True)\n",
    "    df_test_no_mod['modifications'].replace(\"^[|]|[|]$\", '',inplace=True,regex=True)\n",
    "\n",
    "    my_train = [reform_seq(df_train['seq'][i],m) for i, m in enumerate (df_train['modifications'])]\n",
    "    my_test = [reform_seq(df_test['seq'][i],m) for i, m in enumerate (df_test['modifications'])]\n",
    "    my_eval = [reform_seq(df_valid['seq'][i],m) for i, m in enumerate (df_valid['modifications'])]\n",
    "    my_test_no_mod = [reform_seq(df_test_no_mod['seq'][i],m) for i, m in enumerate (df_test_no_mod['modifications'])]\n",
    "\n",
    "\n",
    "    train_x, train_y, train_prediction, train_error = dataframe_to_matrix(my_train, df_train)\n",
    "    test_x, test_y, test_prediction, test_error = dataframe_to_matrix(my_test, df_test)\n",
    "    val_x, val_y, val_prediction, val_error = dataframe_to_matrix(my_eval, df_valid)\n",
    "    test_no_mod_x, test_no_mod_y, test_no_mod_prediction, test_no_mod_error = dataframe_to_matrix(my_test_no_mod, df_test_no_mod)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-02T11:57:57.885113Z",
     "end_time": "2023-08-02T11:57:58.646994Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['LGDVYVNDAFGTAHR',\n 'TEVNSGFFYK',\n 'TAIQAAGYPDK',\n 'GSELQNYFTK',\n 'LAEQEAHYAVR',\n 'YVHADAPTNK',\n 'YHEDIFGLTLR',\n 'LDLSSLAYSGK',\n 'SYVDTGGVSR',\n 'AYILNLVK',\n 'IFDSEEILAGYK',\n 'AYGELPEHAK',\n 'YLHDESGLNR',\n 'ALYETELADAR',\n 'DYIM[Oxidation]EPSIFNTLK',\n 'LQEVEAEVAATGTYQLR',\n 'FLQEFYQDDELGK',\n 'LSSETYSQAK',\n 'YLEC[Carbamidomethyl]ISC[Carbamidomethyl]GSSDM[Oxidation]SC[Carbamidomethyl]ER',\n 'YQAEFPM[Oxidation]GPVTSAHAGTYR',\n 'PATSYVR',\n 'LEENHYNTYISK',\n 'TPENYPNAGLTM[Oxidation]NYC[Carbamidomethyl]R',\n 'GVEAVGSYAENQR',\n 'YNILEDVAVC[Carbamidomethyl]M[Oxidation]DLDTR',\n 'TYVQDILR',\n 'AQLSQYFDAVK',\n 'GHYTIGK',\n 'DLIDTYLLHM[Oxidation]EK',\n 'C[Carbamidomethyl]YYGSDTAGR',\n 'LELAQYR',\n 'YSTSGSSGLTTGK',\n 'DYSVTANSK',\n 'LAPEYEAAATR',\n 'VFLDC[Carbamidomethyl]C[Carbamidomethyl]NYITELR',\n 'GYLGPEQLPDC[Carbamidomethyl]LK',\n 'VHATSTYR',\n 'GVQYLNEIK',\n 'SYSVGASGSSSR',\n 'DM[Oxidation]AQSIYR',\n 'DLTTAGAVTQC[Carbamidomethyl]YR',\n 'YADLTEDQLPSC[Carbamidomethyl]ESLK',\n 'M[Oxidation]AGLELLSDQGYR',\n 'SAQEM[Oxidation]FTYIC[Carbamidomethyl]NHIK',\n 'VIVGGSSEYK',\n 'FNALFAQGNYSEAAK',\n 'GEPGEFYFDLR',\n 'PDTQYTGR',\n 'IGAEVYHHLK',\n 'M[Oxidation]YQM[Oxidation]DIQQELQR',\n 'HHAAYVNNLNVTEEK',\n 'FGPVFTLYVGSQR',\n 'SDQHIQLQLSAESVGEVYIK',\n 'IM[Oxidation]GPNYTPGK',\n 'SPYLYPLYGLGELPQGFAR',\n 'APEYLHR',\n 'YNILGTNTIM[Oxidation]DK',\n 'LVDQNIFSFYLSR',\n 'DDSNLYINVK',\n 'VVDLLAPYAK',\n 'GYGVIFANGNR',\n 'YIGTGHADTTK',\n 'GYAFIEFASFEDAK',\n 'LYHSEAFTVNFGDTEEAK',\n 'LAEYHAK',\n 'FHYQDQEFLK',\n 'AAVEINVAVLHSYQLAK',\n 'C[Carbamidomethyl]QGGQETQEYR',\n 'LGPNDQYK',\n 'TSAYGPPTR',\n 'M[Oxidation]QNDAGEFVDLYVPR',\n 'YNTYAYVGLTEGPSPGDFR',\n 'LNAIYQNNLTK',\n 'LYAVHQEGNK',\n 'TFSYAGFEM[Oxidation]QPK',\n 'LYGYQAQHYVC[Carbamidomethyl]M[Oxidation]K',\n 'LGGSLADSYLDEGFLLDK',\n 'LYLPVQNK',\n 'FTEYETQVK',\n 'MELSAEYLR',\n 'DLIDTYLLHMEK',\n 'AAVPSGASTGIYEALELR',\n 'AIQLNPSYIR',\n 'DYNVTANSK',\n 'IPPTYQIR',\n 'FEGAIYR',\n 'DYILEGEPGK',\n 'LM[Oxidation]AGQEDSNGC[Carbamidomethyl]INYEAFVK',\n 'SFEDIHQYR',\n 'TSYAQHQQVR',\n 'VYVGNLGTGAGK',\n 'YEITEQR',\n 'YTTNNSSEQR',\n 'GNDISSGTVLSDYVGSGPPK',\n 'AYDATHLVK',\n 'VDATADYIC[Carbamidomethyl]K',\n 'GEYFGEK',\n 'YNTYAYVGLTEGPSPGDFR',\n 'LDYSDFQIVK',\n 'SYVTMTATK',\n 'MYQM[Oxidation]DIQQELQR',\n 'TVTNAVVTVPAYFNDSQR',\n 'YVGESEANIR',\n 'GPQNATDSYVHK',\n 'DLATVYVDVLK',\n 'FM[Oxidation]SAYEQR',\n 'DYGNSPLHR',\n 'GYDVIAQAQSGTGK',\n 'WFYIASAFR',\n 'YTAQVDAEEK',\n 'LSITGTYDLK',\n 'YGMVAQVTQTLK',\n 'YEAEAAFFANLK',\n 'TLM[Oxidation]FGSYLDDEK',\n 'SEDFSLPAYM[Oxidation]DR',\n 'LQIASDENYK',\n 'LEENHYNTYISK',\n 'LM[Oxidation]C[Carbamidomethyl]ELGNVIINQIYEAR',\n 'VYSLFLDESR',\n 'VYSGHQGR',\n 'AAGQPYEQHAK',\n 'LPEYPQVDDLLLR',\n 'SEGFDTYR',\n 'YNTYAYVGLTEGPSPGDFR',\n 'SYHLQIVTK',\n 'YM[Oxidation]QNEYR',\n 'LQNSYQPTNK',\n 'VELC[Carbamidomethyl]SFSGYK',\n 'IYHPNVDK',\n 'VETFSGVYK',\n 'AYPLADAHLTK',\n 'SYC[Carbamidomethyl]AEIAHNVSSK',\n 'DLYEPSHGAK',\n 'SYGIPYIETSAK',\n 'DYVSQFEGSALGK',\n 'GDYDLNAVR',\n 'AEGYEVAHGGR',\n 'M[Oxidation]ELSAEYLR',\n 'HIGFVYHPTK',\n 'APVPTGEVYFADSFDR',\n 'AIEINPDSAQPYK',\n 'HGVYNPNK',\n 'YTC[Carbamidomethyl]SFC[Carbamidomethyl]GK',\n 'FYTEDSPGLK',\n 'AQSYAQQLGR',\n 'VNSM[Oxidation]VAYK',\n 'ASC[Carbamidomethyl]LYGQLPK',\n 'AEVSPM[Oxidation]YGVQK',\n 'HEALM[Oxidation]SDLSAYGSSIQALR',\n 'SYVTM[Oxidation]TATK',\n 'MYQMDIQQELQR',\n 'YILIPASQQPK',\n 'GC[Carbamidomethyl]ELYVQLHGIQQVLK',\n 'LGGDLGTYVINK',\n 'HEALMSDLSAYGSSIQALR',\n 'LVFDEYLK',\n 'LYDIDVAK',\n 'THLAPYSDELR',\n 'TSLM[Oxidation]NQYVNK',\n 'YTQVGPDHNR',\n 'MQNDAGEFVDLYVPR',\n 'VSFLSALEEYTK',\n 'SPASDTYIVFGEAK',\n 'LASEYLTPEEM[Oxidation]VTFK',\n 'LYGYQAQHYVC[Carbamidomethyl]M[Oxidation]K',\n 'C[Carbamidomethyl]SVLAAANPVYGR',\n 'VYPSSLSK',\n 'STC[Carbamidomethyl]TYVGAAK',\n 'M[Oxidation]YQMDIQQELQR',\n 'VVPGYGHAVLR',\n 'IYELAAGGTAVGTGLNTR',\n 'SVSSSSYR',\n 'SVQEIQATFFYFTPNK',\n 'MAGLELLSDQGYR',\n 'YEATSVQQK',\n 'GPPPSYGGSSR',\n 'DYIMEPSIFNTLK',\n 'LASEYLTPEEMVTFK',\n 'YGM[Oxidation]VAQVTQTLK',\n 'VVIGM[Oxidation]DVAASEFYR',\n 'TFSYAGFEMQPK',\n 'DVM[Oxidation]LEIYSNLLSM[Oxidation]GYQVSK',\n 'FMSAYEQR',\n 'LELEIETYR',\n 'YNILGTNTIMDK',\n 'TPENYPNAGLTMNYC[Carbamidomethyl]R',\n 'LYGYQAQHYVC[Carbamidomethyl]MK',\n 'FGYVDFESAEDLEK',\n 'FLQEFYQDDELGKK',\n 'VNSMVAYK',\n 'DLEEAEEYK',\n 'TLMFGSYLDDEK',\n 'SAQEMFTYIC[Carbamidomethyl]NHIK',\n 'TSLMNQYVNK',\n 'VPSYTLILPSLELPVLHVPR',\n 'IMGPNYTPGK',\n 'YQAEFPMGPVTSAHAGTYR',\n 'AEVSPMYGVQK',\n 'DMAQSIYR',\n 'YMQNEYR',\n 'LYGYQAQHYVC[Carbamidomethyl]MK',\n 'LSLESLTSYFSIESSTK',\n 'YNILEDVAVC[Carbamidomethyl]MDLDTR',\n 'VVIGMDVAASEFYR',\n 'KINDYVEKGSHGK',\n 'SEDFSLPAYMDR',\n 'IAALQAFADQLIAAGHYAK',\n 'KDVDAAYLNK',\n 'DTVFALVNYIFFK',\n 'LMC[Carbamidomethyl]ELGNVIINQIYEAR',\n 'LMAGQEDSNGC[Carbamidomethyl]INYEAFVK']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_test_no_mod"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[ 0.    , -0.534 , -0.534 , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    , -1.938 , -1.938 , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    , -1.903 , -1.903 , ...,  0.    ,  0.    ,  0.    ],\n        ...,\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n\n       [[ 0.    , -1.064 ,  1.062 , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.0905, -0.6855, ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.2418, -0.914 , ...,  0.    ,  0.    ,  0.    ],\n        ...,\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n\n       [[ 0.    , -0.534 ,  1.051 , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    , -1.938 , -0.949 , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    , -1.903 , -0.7803, ...,  0.    ,  0.    ,  0.    ],\n        ...,\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n\n       ...,\n\n       [[ 0.    , -1.238 ,  0.765 , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    , -0.311 , -0.587 , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    , -0.1503, -0.4507, ...,  0.    ,  0.    ,  0.    ],\n        ...,\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n\n       [[ 0.    ,  1.062 ,  0.2944, ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    , -0.9683, -1.493 , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    , -0.9956, -1.573 , ...,  0.    ,  0.    ,  0.    ],\n        ...,\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n\n       [[ 0.    , -0.949 ,  0.765 , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.2798, -0.5205, ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.49  , -0.4224, ...,  0.    ,  0.    ,  0.    ],\n        ...,\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  1.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]]],\n      dtype=float16)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('./data_matrix/14ptm/diamino13/Acetyl/train_x.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Shape of train set: ',train_x.shape)\n",
    "print('Shape of test set: ', test_x.shape)\n",
    "print('Shape of val set: ', val_x.shape)\n",
    "print('Shape of test no mod set: ', test_no_mod_x.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "          0         1         2         3         4         5         6   \\\n0   0.000000 -0.534180 -0.534180 -1.538086 -0.534180  0.294434 -1.538086   \n1   0.000000 -1.938477 -1.938477 -0.958496 -1.938477 -1.493164 -0.958496   \n2   0.000000 -1.903320 -1.903320 -0.955566 -1.903320 -1.573242 -0.955566   \n3   0.000000 -0.739258 -0.739258 -0.734863 -0.739258 -0.734863 -0.734863   \n4   0.000000 -0.196533 -0.196533  0.676758 -0.196533  0.231079  0.676758   \n5   0.000000 -0.862793 -0.862793 -1.410156 -0.862793 -0.862793 -1.410156   \n6   0.000000 -0.569824 -0.569824 -0.657227 -0.569824 -0.569824 -0.657227   \n7   0.000000 -1.576172 -1.576172 -0.951172 -1.576172 -1.627930 -0.951172   \n8   0.000000 -0.629395 -0.629395 -0.629395 -0.629395 -0.629395 -0.629395   \n9   0.000000 -0.134888 -0.134888 -0.134888 -0.134888 -0.134888 -0.134888   \n10  0.000000 -0.452148 -0.452148 -0.520020 -0.452148 -0.452148 -0.520020   \n11  0.000000 -0.032318 -0.032318 -0.032318 -0.032318 -0.032318 -0.032318   \n12  0.000000 -1.879883 -1.879883 -1.045898 -1.879883 -1.587891 -1.045898   \n13  0.000000 -1.068359  0.000000 -2.072266  0.000000 -1.244141  0.000000   \n14  0.000000 -3.876953  0.000000 -2.896484  0.000000 -2.451172  0.000000   \n15  0.000000 -3.806641  0.000000 -2.859375  0.000000 -2.529297  0.000000   \n16  0.000000 -1.478516  0.000000 -1.474609  0.000000 -1.469727  0.000000   \n17  0.000000 -0.393066  0.000000  0.480225  0.000000  0.907715  0.000000   \n18  0.000000 -1.725586  0.000000 -2.273438  0.000000 -2.273438  0.000000   \n19  0.000000 -1.139648  0.000000 -1.226562  0.000000 -1.226562  0.000000   \n20  0.000000 -3.152344  0.000000 -2.527344  0.000000 -2.578125  0.000000   \n21  0.000000 -1.258789  0.000000 -1.258789  0.000000 -1.258789  0.000000   \n22  0.000000 -0.269775  0.000000 -0.269775  0.000000 -0.269775  0.000000   \n23  0.000000 -0.904297  0.000000 -0.972168  0.000000 -0.972168  0.000000   \n24  0.000000 -0.064636  0.000000 -0.064636  0.000000 -0.064636  0.000000   \n25  0.000000 -3.759766  0.000000 -2.925781  0.000000 -2.632812  0.000000   \n26  0.239990  0.239990  0.239990  0.239990  0.239990  0.239990  0.239990   \n27  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n28  0.000000  0.083313  0.166626  0.250000  0.333252  0.416748  0.500000   \n29  1.083008  1.000000  0.916504  0.833496  0.750000  0.666504  0.583496   \n30  0.000000  0.000000  0.000000  0.000000  0.000000  1.000000  0.000000   \n31  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n32  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n33  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n34  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n35  0.000000  1.000000  1.000000  0.000000  1.000000  0.000000  0.000000   \n36  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n37  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n38  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n39  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n40  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n41  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n42  0.000000  0.000000  0.000000  1.000000  0.000000  0.000000  1.000000   \n43  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n44  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n45  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n46  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n47  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n48  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n49  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n50  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n\n          7         8         9   ...       42       43       44       45  \\\n0  -0.534180  0.294434 -1.538086  ...  0.00000  0.00000  0.00000  0.00000   \n1  -1.938477 -1.493164 -0.958496  ...  0.00000  0.00000  0.00000  0.00000   \n2  -1.903320 -1.573242 -0.955566  ...  0.00000  0.00000  0.00000  0.00000   \n3  -0.739258 -0.734863 -0.734863  ...  0.00000  0.00000  0.00000  0.00000   \n4  -0.196533  0.231079  0.676758  ...  0.00000  0.00000  0.00000  0.00000   \n5  -0.862793 -0.862793 -1.410156  ...  0.00000  0.00000  0.00000  0.00000   \n6  -0.569824 -0.569824 -0.657227  ...  0.00000  0.00000  0.00000  0.00000   \n7  -1.576172 -1.627930 -0.951172  ...  0.00000  0.00000  0.00000  0.00000   \n8  -0.629395 -0.629395 -0.629395  ...  0.00000  0.00000  0.00000  0.00000   \n9  -0.134888 -0.134888 -0.134888  ...  0.00000  0.00000  0.00000  0.00000   \n10 -0.452148 -0.452148 -0.520020  ...  0.00000  0.00000  0.00000  0.00000   \n11 -0.032318 -0.032318 -0.032318  ...  0.00000  0.00000  0.00000  0.00000   \n12 -1.879883 -1.587891 -1.045898  ...  0.00000  0.00000  0.00000  0.00000   \n13 -0.239746  0.000000 -2.072266  ...  0.00000  0.00000  0.00000  0.00000   \n14 -3.431641  0.000000 -2.896484  ...  0.00000  0.00000  0.00000  0.00000   \n15 -3.476562  0.000000 -2.859375  ...  0.00000  0.00000  0.00000  0.00000   \n16 -1.474609  0.000000 -1.474609  ...  0.00000  0.00000  0.00000  0.00000   \n17  0.034546  0.000000  0.480225  ...  0.00000  0.00000  0.00000  0.00000   \n18 -1.725586  0.000000 -2.273438  ...  0.00000  0.00000  0.00000  0.00000   \n19 -1.139648  0.000000 -1.226562  ...  0.00000  0.00000  0.00000  0.00000   \n20 -3.203125  0.000000 -2.527344  ...  0.00000  0.00000  0.00000  0.00000   \n21 -1.258789  0.000000 -1.258789  ...  0.00000  0.00000  0.00000  0.00000   \n22 -0.269775  0.000000 -0.269775  ...  0.00000  0.00000  0.00000  0.00000   \n23 -0.904297  0.000000 -0.972168  ...  0.00000  0.00000  0.00000  0.00000   \n24 -0.064636  0.000000 -0.064636  ...  0.00000  0.00000  0.00000  0.00000   \n25 -3.468750  0.000000 -2.925781  ...  0.00000  0.00000  0.00000  0.00000   \n26  0.239990  0.239990  0.239990  ...  0.23999  0.23999  0.23999  0.23999   \n27  1.000000  1.000000  1.000000  ...  0.00000  0.00000  0.00000  0.00000   \n28  0.583496  0.666504  0.750000  ...  0.00000  0.00000  0.00000  0.00000   \n29  0.500000  0.416748  0.333252  ...  0.00000  0.00000  0.00000  0.00000   \n30  0.000000  1.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n31  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n32  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n33  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n34  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n35  1.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n36  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n37  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n38  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n39  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n40  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n41  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n42  0.000000  0.000000  1.000000  ...  0.00000  0.00000  0.00000  0.00000   \n43  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n44  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n45  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n46  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n47  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n48  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n49  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n50  0.000000  0.000000  0.000000  ...  0.00000  0.00000  0.00000  0.00000   \n\n         46       47       48       49       50       51  \n0   0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n1   0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n2   0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n3   0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n4   0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n5   0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n6   0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n7   0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n8   0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n9   0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n10  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n11  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n12  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n13  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n14  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n15  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n16  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n17  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n18  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n19  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n20  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n21  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n22  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n23  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n24  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n25  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n26  0.23999  0.23999  0.23999  0.23999  0.23999  0.23999  \n27  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n28  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n29  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n30  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n31  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n32  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n33  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n34  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n35  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n36  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n37  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n38  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n39  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n40  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n41  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n42  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n43  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n44  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n45  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n46  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n47  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n48  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n49  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n50  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n\n[51 rows x 52 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>42</th>\n      <th>43</th>\n      <th>44</th>\n      <th>45</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n      <th>50</th>\n      <th>51</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>-0.534180</td>\n      <td>-0.534180</td>\n      <td>-1.538086</td>\n      <td>-0.534180</td>\n      <td>0.294434</td>\n      <td>-1.538086</td>\n      <td>-0.534180</td>\n      <td>0.294434</td>\n      <td>-1.538086</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>-1.938477</td>\n      <td>-1.938477</td>\n      <td>-0.958496</td>\n      <td>-1.938477</td>\n      <td>-1.493164</td>\n      <td>-0.958496</td>\n      <td>-1.938477</td>\n      <td>-1.493164</td>\n      <td>-0.958496</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>-1.903320</td>\n      <td>-1.903320</td>\n      <td>-0.955566</td>\n      <td>-1.903320</td>\n      <td>-1.573242</td>\n      <td>-0.955566</td>\n      <td>-1.903320</td>\n      <td>-1.573242</td>\n      <td>-0.955566</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>-0.739258</td>\n      <td>-0.739258</td>\n      <td>-0.734863</td>\n      <td>-0.739258</td>\n      <td>-0.734863</td>\n      <td>-0.734863</td>\n      <td>-0.739258</td>\n      <td>-0.734863</td>\n      <td>-0.734863</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>-0.196533</td>\n      <td>-0.196533</td>\n      <td>0.676758</td>\n      <td>-0.196533</td>\n      <td>0.231079</td>\n      <td>0.676758</td>\n      <td>-0.196533</td>\n      <td>0.231079</td>\n      <td>0.676758</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000000</td>\n      <td>-0.862793</td>\n      <td>-0.862793</td>\n      <td>-1.410156</td>\n      <td>-0.862793</td>\n      <td>-0.862793</td>\n      <td>-1.410156</td>\n      <td>-0.862793</td>\n      <td>-0.862793</td>\n      <td>-1.410156</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.000000</td>\n      <td>-0.569824</td>\n      <td>-0.569824</td>\n      <td>-0.657227</td>\n      <td>-0.569824</td>\n      <td>-0.569824</td>\n      <td>-0.657227</td>\n      <td>-0.569824</td>\n      <td>-0.569824</td>\n      <td>-0.657227</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.000000</td>\n      <td>-1.576172</td>\n      <td>-1.576172</td>\n      <td>-0.951172</td>\n      <td>-1.576172</td>\n      <td>-1.627930</td>\n      <td>-0.951172</td>\n      <td>-1.576172</td>\n      <td>-1.627930</td>\n      <td>-0.951172</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.000000</td>\n      <td>-0.629395</td>\n      <td>-0.629395</td>\n      <td>-0.629395</td>\n      <td>-0.629395</td>\n      <td>-0.629395</td>\n      <td>-0.629395</td>\n      <td>-0.629395</td>\n      <td>-0.629395</td>\n      <td>-0.629395</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.000000</td>\n      <td>-0.134888</td>\n      <td>-0.134888</td>\n      <td>-0.134888</td>\n      <td>-0.134888</td>\n      <td>-0.134888</td>\n      <td>-0.134888</td>\n      <td>-0.134888</td>\n      <td>-0.134888</td>\n      <td>-0.134888</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.000000</td>\n      <td>-0.452148</td>\n      <td>-0.452148</td>\n      <td>-0.520020</td>\n      <td>-0.452148</td>\n      <td>-0.452148</td>\n      <td>-0.520020</td>\n      <td>-0.452148</td>\n      <td>-0.452148</td>\n      <td>-0.520020</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.000000</td>\n      <td>-0.032318</td>\n      <td>-0.032318</td>\n      <td>-0.032318</td>\n      <td>-0.032318</td>\n      <td>-0.032318</td>\n      <td>-0.032318</td>\n      <td>-0.032318</td>\n      <td>-0.032318</td>\n      <td>-0.032318</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.000000</td>\n      <td>-1.879883</td>\n      <td>-1.879883</td>\n      <td>-1.045898</td>\n      <td>-1.879883</td>\n      <td>-1.587891</td>\n      <td>-1.045898</td>\n      <td>-1.879883</td>\n      <td>-1.587891</td>\n      <td>-1.045898</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.000000</td>\n      <td>-1.068359</td>\n      <td>0.000000</td>\n      <td>-2.072266</td>\n      <td>0.000000</td>\n      <td>-1.244141</td>\n      <td>0.000000</td>\n      <td>-0.239746</td>\n      <td>0.000000</td>\n      <td>-2.072266</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.000000</td>\n      <td>-3.876953</td>\n      <td>0.000000</td>\n      <td>-2.896484</td>\n      <td>0.000000</td>\n      <td>-2.451172</td>\n      <td>0.000000</td>\n      <td>-3.431641</td>\n      <td>0.000000</td>\n      <td>-2.896484</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.000000</td>\n      <td>-3.806641</td>\n      <td>0.000000</td>\n      <td>-2.859375</td>\n      <td>0.000000</td>\n      <td>-2.529297</td>\n      <td>0.000000</td>\n      <td>-3.476562</td>\n      <td>0.000000</td>\n      <td>-2.859375</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.000000</td>\n      <td>-1.478516</td>\n      <td>0.000000</td>\n      <td>-1.474609</td>\n      <td>0.000000</td>\n      <td>-1.469727</td>\n      <td>0.000000</td>\n      <td>-1.474609</td>\n      <td>0.000000</td>\n      <td>-1.474609</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.000000</td>\n      <td>-0.393066</td>\n      <td>0.000000</td>\n      <td>0.480225</td>\n      <td>0.000000</td>\n      <td>0.907715</td>\n      <td>0.000000</td>\n      <td>0.034546</td>\n      <td>0.000000</td>\n      <td>0.480225</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.000000</td>\n      <td>-1.725586</td>\n      <td>0.000000</td>\n      <td>-2.273438</td>\n      <td>0.000000</td>\n      <td>-2.273438</td>\n      <td>0.000000</td>\n      <td>-1.725586</td>\n      <td>0.000000</td>\n      <td>-2.273438</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.000000</td>\n      <td>-1.139648</td>\n      <td>0.000000</td>\n      <td>-1.226562</td>\n      <td>0.000000</td>\n      <td>-1.226562</td>\n      <td>0.000000</td>\n      <td>-1.139648</td>\n      <td>0.000000</td>\n      <td>-1.226562</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.000000</td>\n      <td>-3.152344</td>\n      <td>0.000000</td>\n      <td>-2.527344</td>\n      <td>0.000000</td>\n      <td>-2.578125</td>\n      <td>0.000000</td>\n      <td>-3.203125</td>\n      <td>0.000000</td>\n      <td>-2.527344</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.000000</td>\n      <td>-1.258789</td>\n      <td>0.000000</td>\n      <td>-1.258789</td>\n      <td>0.000000</td>\n      <td>-1.258789</td>\n      <td>0.000000</td>\n      <td>-1.258789</td>\n      <td>0.000000</td>\n      <td>-1.258789</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.000000</td>\n      <td>-0.269775</td>\n      <td>0.000000</td>\n      <td>-0.269775</td>\n      <td>0.000000</td>\n      <td>-0.269775</td>\n      <td>0.000000</td>\n      <td>-0.269775</td>\n      <td>0.000000</td>\n      <td>-0.269775</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.000000</td>\n      <td>-0.904297</td>\n      <td>0.000000</td>\n      <td>-0.972168</td>\n      <td>0.000000</td>\n      <td>-0.972168</td>\n      <td>0.000000</td>\n      <td>-0.904297</td>\n      <td>0.000000</td>\n      <td>-0.972168</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.000000</td>\n      <td>-0.064636</td>\n      <td>0.000000</td>\n      <td>-0.064636</td>\n      <td>0.000000</td>\n      <td>-0.064636</td>\n      <td>0.000000</td>\n      <td>-0.064636</td>\n      <td>0.000000</td>\n      <td>-0.064636</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.000000</td>\n      <td>-3.759766</td>\n      <td>0.000000</td>\n      <td>-2.925781</td>\n      <td>0.000000</td>\n      <td>-2.632812</td>\n      <td>0.000000</td>\n      <td>-3.468750</td>\n      <td>0.000000</td>\n      <td>-2.925781</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.239990</td>\n      <td>0.239990</td>\n      <td>0.239990</td>\n      <td>0.239990</td>\n      <td>0.239990</td>\n      <td>0.239990</td>\n      <td>0.239990</td>\n      <td>0.239990</td>\n      <td>0.239990</td>\n      <td>0.239990</td>\n      <td>...</td>\n      <td>0.23999</td>\n      <td>0.23999</td>\n      <td>0.23999</td>\n      <td>0.23999</td>\n      <td>0.23999</td>\n      <td>0.23999</td>\n      <td>0.23999</td>\n      <td>0.23999</td>\n      <td>0.23999</td>\n      <td>0.23999</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.000000</td>\n      <td>0.083313</td>\n      <td>0.166626</td>\n      <td>0.250000</td>\n      <td>0.333252</td>\n      <td>0.416748</td>\n      <td>0.500000</td>\n      <td>0.583496</td>\n      <td>0.666504</td>\n      <td>0.750000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>1.083008</td>\n      <td>1.000000</td>\n      <td>0.916504</td>\n      <td>0.833496</td>\n      <td>0.750000</td>\n      <td>0.666504</td>\n      <td>0.583496</td>\n      <td>0.500000</td>\n      <td>0.416748</td>\n      <td>0.333252</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n  </tbody>\n</table>\n<p>51 rows × 52 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the hyper parameters\n",
    "\n",
    "def get_config(lr= 1e-3, epoch= 300, batch= 64):\n",
    "    config={\n",
    "        \"learning_rate\" : lr,\n",
    "        \"epochs\" : epoch,\n",
    "        \"batch_size\" : batch\n",
    "    }\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Making the pytorch dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, sequences, retention):\n",
    "        self.sequences = sequences\n",
    "        self.retention = retention\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.retention)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.sequences[idx], self.retention[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Dataloaders\n",
    "\n",
    "train_dataset = MyDataset(train_x, train_y)\n",
    "val_dataset = MyDataset(val_x, val_y)\n",
    "test_dataset = MyDataset(test_x, test_y)\n",
    "test_no_mod_dataset = MyDataset(test_no_mod_x, test_no_mod_y)\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, shuffle=True, batch_size=config[\"batch_size\"])\n",
    "dataloader_val = DataLoader(val_dataset, pin_memory=True)\n",
    "dataloader_test = DataLoader(test_dataset, pin_memory=True)\n",
    "dataloader_test_no_mod = DataLoader(test_no_mod_dataset, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Network architecture\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet,self).__init__()\n",
    "        self.l = nn.ModuleList()\n",
    "\n",
    "        self.l.append(nn.Conv1d(in_channels=30, out_channels=100, kernel_size=(3,), stride=(1,), padding=(1,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "        self.l.append(nn.Dropout(p=0.2, inplace=False))\n",
    "\n",
    "        self.l.append(nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(3,), stride=(1,), padding=(1,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "        self.l.append(nn.Dropout(p=0.2, inplace=False))\n",
    "\n",
    "        self.l.append(nn.Conv1d(in_channels=100, out_channels=64, kernel_size=(4,), stride=(1,), padding=(1,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "        self.l.append(nn.Dropout(p=0.2, inplace=False))\n",
    "\n",
    "        self.l.append(nn.Flatten())\n",
    "\n",
    "        self.l.append(nn.Linear(in_features=64*51, out_features=1024))\n",
    "        self.l.append(nn.ReLU())\n",
    "        \n",
    "        self.l.append(nn.Linear(in_features=1024, out_features=512))\n",
    "        self.l.append(nn.ReLU())\n",
    "        self.l.append(nn.Linear(in_features=512, out_features=128))\n",
    "        self.l.append(nn.ReLU())\n",
    "\n",
    "        self.l.append(nn.Linear(in_features=128, out_features=1))\n",
    "        self.l.append(nn.Linear(1,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.l:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of MyNet(\n",
      "  (l): ModuleList(\n",
      "    (0): Conv1d(30, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Conv1d(100, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Conv1d(100, 64, kernel_size=(4,), stride=(1,), padding=(1,))\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Flatten(start_dim=1, end_dim=-1)\n",
      "    (10): Linear(in_features=3264, out_features=1024, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (17): Linear(in_features=1, out_features=1, bias=True)\n",
      "  )\n",
      ")>\n",
      "Total Parameters =   3998819\n",
      "torch.float32\n",
      "X is cuda: True\n",
      "X.shape: torch.Size([64, 30, 52])\n",
      "y.shape: torch.Size([64])\n",
      "outputs.shape: torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "# First look at Network\n",
    "\n",
    "print(MyNet().parameters)\n",
    "print(\"Total Parameters =  \",sum(p.numel() for p in MyNet().parameters()))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = MyNet()\n",
    "model.to(device)\n",
    "\n",
    "for batch in dataloader_train:\n",
    "    X, y = batch[0].to(device), batch[1].to(device)\n",
    "    X = X.float()\n",
    "    print(X.dtype)\n",
    "    print('X is cuda:',X.is_cuda)\n",
    "    print('X.shape:',X.shape)\n",
    "    print('y.shape:',y.shape)\n",
    "    outputs = model(X)\n",
    "    print('outputs.shape:',outputs.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 100, 52]           9,100\n",
      "              ReLU-2              [-1, 100, 52]               0\n",
      "           Dropout-3              [-1, 100, 52]               0\n",
      "            Conv1d-4              [-1, 100, 52]          30,100\n",
      "              ReLU-5              [-1, 100, 52]               0\n",
      "           Dropout-6              [-1, 100, 52]               0\n",
      "            Conv1d-7               [-1, 64, 51]          25,664\n",
      "              ReLU-8               [-1, 64, 51]               0\n",
      "           Dropout-9               [-1, 64, 51]               0\n",
      "          Flatten-10                 [-1, 3264]               0\n",
      "           Linear-11                 [-1, 1024]       3,343,360\n",
      "             ReLU-12                 [-1, 1024]               0\n",
      "           Linear-13                  [-1, 512]         524,800\n",
      "             ReLU-14                  [-1, 512]               0\n",
      "           Linear-15                  [-1, 128]          65,664\n",
      "             ReLU-16                  [-1, 128]               0\n",
      "           Linear-17                    [-1, 1]             129\n",
      "           Linear-18                    [-1, 1]               2\n",
      "================================================================\n",
      "Total params: 3,998,819\n",
      "Trainable params: 3,998,819\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.36\n",
      "Params size (MB): 15.25\n",
      "Estimated Total Size (MB): 15.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model,input_size=( 30,52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32md:\\pycharm virtual environments\\ms2d\\lib\\site-packages\\graphviz\\backend\\execute.py\u001B[0m in \u001B[0;36mrun_check\u001B[1;34m(cmd, input_lines, encoding, capture_output, quiet, **kwargs)\u001B[0m\n\u001B[0;32m     84\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 85\u001B[1;33m             \u001B[0mproc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msubprocess\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcmd\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     86\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001B[0m\n\u001B[0;32m    504\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 505\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0mPopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mpopenargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mprocess\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    506\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001B[0m\n\u001B[0;32m    950\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 951\u001B[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001B[0m\u001B[0;32m    952\u001B[0m                                 \u001B[0mpass_fds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcwd\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py\u001B[0m in \u001B[0;36m_execute_child\u001B[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001B[0m\n\u001B[0;32m   1419\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1420\u001B[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001B[0m\u001B[0;32m   1421\u001B[0m                                          \u001B[1;31m# no special security\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 2] The system cannot find the file specified",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mExecutableNotFound\u001B[0m                        Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_21760/916346358.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mMyConvNetVis\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"png\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[0mMyConvNetVis\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdirectory\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"graph\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m \u001B[0mMyConvNetVis\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\pycharm virtual environments\\ms2d\\lib\\site-packages\\graphviz\\_tools.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    170\u001B[0m                               category=category)\n\u001B[0;32m    171\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 172\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    173\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    174\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\pycharm virtual environments\\ms2d\\lib\\site-packages\\graphviz\\rendering.py\u001B[0m in \u001B[0;36mview\u001B[1;34m(self, filename, directory, cleanup, quiet, quiet_view)\u001B[0m\n\u001B[0;32m    180\u001B[0m             \u001B[1;32mand\u001B[0m \u001B[0mno\u001B[0m \u001B[0mway\u001B[0m \u001B[0mto\u001B[0m \u001B[0mretrieve\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mapplication\u001B[0m\u001B[0;31m'\u001B[0m\u001B[0ms\u001B[0m \u001B[0mexit\u001B[0m \u001B[0mstatus\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    181\u001B[0m         \"\"\"\n\u001B[1;32m--> 182\u001B[1;33m         return self.render(filename=filename, directory=directory, view=True,\n\u001B[0m\u001B[0;32m    183\u001B[0m                            cleanup=cleanup, quiet=quiet, quiet_view=quiet_view)\n",
      "\u001B[1;32md:\\pycharm virtual environments\\ms2d\\lib\\site-packages\\graphviz\\_tools.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    170\u001B[0m                               category=category)\n\u001B[0;32m    171\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 172\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    173\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    174\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\pycharm virtual environments\\ms2d\\lib\\site-packages\\graphviz\\rendering.py\u001B[0m in \u001B[0;36mrender\u001B[1;34m(self, filename, directory, view, cleanup, format, renderer, formatter, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001B[0m\n\u001B[0;32m    117\u001B[0m         \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 119\u001B[1;33m         \u001B[0mrendered\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_render\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    120\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    121\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcleanup\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\pycharm virtual environments\\ms2d\\lib\\site-packages\\graphviz\\_tools.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    170\u001B[0m                               category=category)\n\u001B[0;32m    171\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 172\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    173\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    174\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\pycharm virtual environments\\ms2d\\lib\\site-packages\\graphviz\\backend\\rendering.py\u001B[0m in \u001B[0;36mrender\u001B[1;34m(engine, format, filepath, renderer, formatter, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001B[0m\n\u001B[0;32m    315\u001B[0m     \u001B[0mcmd\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    316\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 317\u001B[1;33m     execute.run_check(cmd,\n\u001B[0m\u001B[0;32m    318\u001B[0m                       \u001B[0mcwd\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mfilepath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparent\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mfilepath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparts\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    319\u001B[0m                       \u001B[0mquiet\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mquiet\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\pycharm virtual environments\\ms2d\\lib\\site-packages\\graphviz\\backend\\execute.py\u001B[0m in \u001B[0;36mrun_check\u001B[1;34m(cmd, input_lines, encoding, capture_output, quiet, **kwargs)\u001B[0m\n\u001B[0;32m     86\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     87\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merrno\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0merrno\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mENOENT\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 88\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mExecutableNotFound\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcmd\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     89\u001B[0m         \u001B[1;32mraise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     90\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mExecutableNotFound\u001B[0m: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "model = MyNet()\n",
    "model.to(device)\n",
    "X, y = batch[0].to(device), batch[1].to(device)\n",
    "X = X.float()\n",
    "y = model(X)   \n",
    "MyConvNetVis = make_dot(y)\n",
    "MyConvNetVis.format = \"png\"\n",
    "MyConvNetVis.directory = \"graph\"\n",
    "MyConvNetVis.view()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "\n",
    "def train(model, loader, loss):\n",
    "    current_loss=0\n",
    "    model.train()\n",
    "\n",
    "    # reading the data\n",
    "    for idx,batch in enumerate(loader):\n",
    "        X,y = batch\n",
    "        # move data to gpu if possible\n",
    "        if device.type == 'cuda':\n",
    "            X,y = X.to(device), y.to(device)\n",
    "        # training steps\n",
    "        optimizer.zero_grad()\n",
    "        X = X.float()\n",
    "        y = y.float()\n",
    "        output = model(X)\n",
    "        loss_fn=loss(output,y.reshape(-1,1))\n",
    "        loss_fn.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss += loss_fn.item()*X.size(0)\n",
    "        correlation = np.corrcoef(output.cpu().detach().numpy().flat,y.cpu()).min()\n",
    "\n",
    "        if idx % 100 == 0:    # print every 2000 mini-batches\n",
    "            print('epoch: %d, train_loss: %.3f' %(epoch + 1, loss_fn.item()), \"Training correlation:\", correlation)\n",
    "\n",
    "            # showing some examples of prediction\n",
    "            # print(\"5 Random samples: \")\n",
    "            # for i in random.sample(range(0,len(output)),5):\n",
    "            #    print('predict: %.3f, real: %.3f'%(output[i].item(),y[i].item()))\n",
    "            # print('')\n",
    "\n",
    "    epoch_loss = current_loss / len(loader.dataset)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, loader, loss):\n",
    "    current_loss=0\n",
    "    model.eval()\n",
    "    outputs=[]\n",
    "    ys = []\n",
    "    # reading the data\n",
    "    for idx,batch in enumerate(loader):\n",
    "        X,y = batch\n",
    "        # move data to gpu if possible\n",
    "        if device.type == 'cuda':\n",
    "            X,y = X.to(device), y.to(device)\n",
    "        # validating steps\n",
    "\n",
    "        X = X.float()\n",
    "        y = y.float()\n",
    "        output = model(X)\n",
    "        ys.append(y.item())\n",
    "        outputs.append(output.item())\n",
    "        loss_fn=loss(output,y.reshape(-1,1))\n",
    "\n",
    "        current_loss += loss_fn.item()*X.size(0)\n",
    "        #print('loss: %.3f' %(loss_fn.item()))\n",
    "    #print('number of outputs: ',len(output))\n",
    "\n",
    "    correlation = np.corrcoef(outputs,ys).min()\n",
    "    epoch_loss = current_loss / len(loader.dataset)\n",
    "    return epoch_loss,correlation, outputs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33malirezak2\u001B[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/alirezak2/14ptms/runs/ons0zyyh\" target=\"_blank\">morning-blaze-30</a></strong> to <a href=\"https://wandb.ai/alirezak2/14ptms\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss: 949.113 Training correlation: 0.07334440106207112\n",
      "\n",
      " Epoch: 1  Train Loss: 223.0986  Validation Loss: 88.4937  Validation Correlation: 0.6196\n",
      "\n",
      "epoch: 2, train_loss: 95.728 Training correlation: 0.5206937549563372\n",
      "\n",
      " Epoch: 2  Train Loss: 79.2248  Validation Loss: 62.1701  Validation Correlation: 0.7950\n",
      "\n",
      "epoch: 3, train_loss: 57.451 Training correlation: 0.7729285472555345\n",
      "\n",
      " Epoch: 3  Train Loss: 50.8168  Validation Loss: 34.7557  Validation Correlation: 0.8615\n",
      "\n",
      "epoch: 4, train_loss: 41.161 Training correlation: 0.867093949859554\n",
      "\n",
      " Epoch: 4  Train Loss: 41.3189  Validation Loss: 30.1162  Validation Correlation: 0.8846\n",
      "\n",
      "epoch: 5, train_loss: 36.613 Training correlation: 0.8653718195139077\n",
      "\n",
      " Epoch: 5  Train Loss: 34.1702  Validation Loss: 26.8575  Validation Correlation: 0.8992\n",
      "\n",
      "epoch: 6, train_loss: 27.970 Training correlation: 0.8884483372796186\n",
      "\n",
      " Epoch: 6  Train Loss: 33.3212  Validation Loss: 24.6957  Validation Correlation: 0.9016\n",
      "\n",
      "epoch: 7, train_loss: 27.271 Training correlation: 0.9051056522054801\n",
      "\n",
      " Epoch: 7  Train Loss: 29.0651  Validation Loss: 23.9206  Validation Correlation: 0.9086\n",
      "\n",
      "epoch: 8, train_loss: 37.370 Training correlation: 0.813495087435232\n",
      "\n",
      " Epoch: 8  Train Loss: 28.1405  Validation Loss: 23.2806  Validation Correlation: 0.9141\n",
      "\n",
      "epoch: 9, train_loss: 21.325 Training correlation: 0.9323962089236918\n",
      "\n",
      " Epoch: 9  Train Loss: 28.5606  Validation Loss: 21.8605  Validation Correlation: 0.9170\n",
      "\n",
      "epoch: 10, train_loss: 25.582 Training correlation: 0.9151336285371294\n",
      "\n",
      " Epoch: 10  Train Loss: 25.3505  Validation Loss: 21.9108  Validation Correlation: 0.9226\n",
      "\n",
      "epoch: 11, train_loss: 21.451 Training correlation: 0.9035074413341176\n",
      "\n",
      " Epoch: 11  Train Loss: 23.6082  Validation Loss: 23.5734  Validation Correlation: 0.9243\n",
      "\n",
      "epoch: 12, train_loss: 14.512 Training correlation: 0.9523146977932021\n",
      "\n",
      " Epoch: 12  Train Loss: 21.5089  Validation Loss: 18.0181  Validation Correlation: 0.9305\n",
      "\n",
      "epoch: 13, train_loss: 15.165 Training correlation: 0.9088686063420177\n",
      "\n",
      " Epoch: 13  Train Loss: 23.3111  Validation Loss: 32.8449  Validation Correlation: 0.9351\n",
      "\n",
      "epoch: 14, train_loss: 23.997 Training correlation: 0.9366255969805602\n",
      "\n",
      " Epoch: 14  Train Loss: 22.5624  Validation Loss: 48.6267  Validation Correlation: 0.9307\n",
      "\n",
      "epoch: 15, train_loss: 51.043 Training correlation: 0.9437428468027823\n",
      "\n",
      " Epoch: 15  Train Loss: 26.1427  Validation Loss: 19.2724  Validation Correlation: 0.9400\n",
      "\n",
      "epoch: 16, train_loss: 17.525 Training correlation: 0.9395246456072145\n",
      "\n",
      " Epoch: 16  Train Loss: 19.9661  Validation Loss: 19.3558  Validation Correlation: 0.9400\n",
      "\n",
      "epoch: 17, train_loss: 18.462 Training correlation: 0.9522462929289283\n",
      "\n",
      " Epoch: 17  Train Loss: 18.2577  Validation Loss: 20.5376  Validation Correlation: 0.9398\n",
      "\n",
      "epoch: 18, train_loss: 19.393 Training correlation: 0.9456602575618592\n",
      "\n",
      " Epoch: 18  Train Loss: 17.1298  Validation Loss: 24.7413  Validation Correlation: 0.9422\n",
      "\n",
      "epoch: 19, train_loss: 21.333 Training correlation: 0.9526795820950087\n",
      "\n",
      " Epoch: 19  Train Loss: 18.9934  Validation Loss: 16.6047  Validation Correlation: 0.9440\n",
      "\n",
      "epoch: 20, train_loss: 9.684 Training correlation: 0.9612419363486325\n",
      "\n",
      " Epoch: 20  Train Loss: 17.9829  Validation Loss: 13.2392  Validation Correlation: 0.9487\n",
      "\n",
      "epoch: 21, train_loss: 10.499 Training correlation: 0.9602495739030614\n",
      "\n",
      " Epoch: 21  Train Loss: 16.0559  Validation Loss: 35.8713  Validation Correlation: 0.9438\n",
      "\n",
      "epoch: 22, train_loss: 23.342 Training correlation: 0.954376751293586\n",
      "\n",
      " Epoch: 22  Train Loss: 17.6456  Validation Loss: 13.3462  Validation Correlation: 0.9473\n",
      "\n",
      "epoch: 23, train_loss: 18.350 Training correlation: 0.9269266525961184\n",
      "\n",
      " Epoch: 23  Train Loss: 15.7788  Validation Loss: 25.5439  Validation Correlation: 0.9504\n",
      "\n",
      "epoch: 24, train_loss: 18.002 Training correlation: 0.9680294096452052\n",
      "\n",
      " Epoch: 24  Train Loss: 15.7841  Validation Loss: 13.7210  Validation Correlation: 0.9504\n",
      "\n",
      "epoch: 25, train_loss: 21.908 Training correlation: 0.9163233835684321\n",
      "\n",
      " Epoch: 25  Train Loss: 15.7732  Validation Loss: 27.6709  Validation Correlation: 0.9493\n",
      "\n",
      "epoch: 26, train_loss: 26.846 Training correlation: 0.9518223281993288\n",
      "\n",
      " Epoch: 26  Train Loss: 15.9382  Validation Loss: 21.8834  Validation Correlation: 0.9491\n",
      "\n",
      "epoch: 27, train_loss: 12.759 Training correlation: 0.9702094896651593\n",
      "\n",
      " Epoch: 27  Train Loss: 14.5321  Validation Loss: 15.7652  Validation Correlation: 0.9526\n",
      "\n",
      "epoch: 28, train_loss: 12.891 Training correlation: 0.940388814593972\n",
      "\n",
      " Epoch: 28  Train Loss: 15.4783  Validation Loss: 16.0074  Validation Correlation: 0.9539\n",
      "\n",
      "epoch: 29, train_loss: 14.781 Training correlation: 0.9649169733405909\n",
      "\n",
      " Epoch: 29  Train Loss: 15.6914  Validation Loss: 16.9942  Validation Correlation: 0.9547\n",
      "\n",
      "epoch: 30, train_loss: 11.016 Training correlation: 0.973813966698597\n",
      "\n",
      " Epoch: 30  Train Loss: 13.9527  Validation Loss: 10.7868  Validation Correlation: 0.9582\n",
      "\n",
      "epoch: 31, train_loss: 9.811 Training correlation: 0.9614849989039945\n",
      "\n",
      " Epoch: 31  Train Loss: 14.0916  Validation Loss: 22.6421  Validation Correlation: 0.9562\n",
      "\n",
      "epoch: 32, train_loss: 22.669 Training correlation: 0.957014779356767\n",
      "\n",
      " Epoch: 32  Train Loss: 14.5146  Validation Loss: 11.4998  Validation Correlation: 0.9571\n",
      "\n",
      "epoch: 33, train_loss: 9.330 Training correlation: 0.9711508127612798\n",
      "\n",
      " Epoch: 33  Train Loss: 15.4799  Validation Loss: 12.1031  Validation Correlation: 0.9571\n",
      "\n",
      "epoch: 34, train_loss: 9.630 Training correlation: 0.964719110032061\n",
      "\n",
      " Epoch: 34  Train Loss: 12.1800  Validation Loss: 10.4984  Validation Correlation: 0.9590\n",
      "\n",
      "epoch: 35, train_loss: 20.721 Training correlation: 0.9291882269778173\n",
      "\n",
      " Epoch: 35  Train Loss: 11.9014  Validation Loss: 9.9558  Validation Correlation: 0.9609\n",
      "\n",
      "epoch: 36, train_loss: 7.854 Training correlation: 0.9762759742035486\n",
      "\n",
      " Epoch: 36  Train Loss: 12.1494  Validation Loss: 10.2575  Validation Correlation: 0.9599\n",
      "\n",
      "epoch: 37, train_loss: 8.230 Training correlation: 0.9715889997781364\n",
      "\n",
      " Epoch: 37  Train Loss: 12.4358  Validation Loss: 14.1824  Validation Correlation: 0.9593\n",
      "\n",
      "epoch: 38, train_loss: 11.682 Training correlation: 0.9599028425539824\n",
      "\n",
      " Epoch: 38  Train Loss: 11.1247  Validation Loss: 11.8851  Validation Correlation: 0.9607\n",
      "\n",
      "epoch: 39, train_loss: 16.553 Training correlation: 0.9378675937434383\n",
      "\n",
      " Epoch: 39  Train Loss: 11.3301  Validation Loss: 18.5768  Validation Correlation: 0.9615\n",
      "\n",
      "epoch: 40, train_loss: 15.937 Training correlation: 0.938319327282538\n",
      "\n",
      " Epoch: 40  Train Loss: 11.7476  Validation Loss: 9.8051  Validation Correlation: 0.9620\n",
      "\n",
      "epoch: 41, train_loss: 16.074 Training correlation: 0.9477002831760715\n",
      "\n",
      " Epoch: 41  Train Loss: 11.7043  Validation Loss: 13.7399  Validation Correlation: 0.9643\n",
      "\n",
      "epoch: 42, train_loss: 15.779 Training correlation: 0.9393295047631625\n",
      "\n",
      " Epoch: 42  Train Loss: 10.9877  Validation Loss: 12.9240  Validation Correlation: 0.9634\n",
      "\n",
      "epoch: 43, train_loss: 8.468 Training correlation: 0.965037981678288\n",
      "\n",
      " Epoch: 43  Train Loss: 10.0641  Validation Loss: 10.6804  Validation Correlation: 0.9622\n",
      "\n",
      "epoch: 44, train_loss: 7.402 Training correlation: 0.9703891527967109\n",
      "\n",
      " Epoch: 44  Train Loss: 10.5477  Validation Loss: 14.9457  Validation Correlation: 0.9616\n",
      "\n",
      "epoch: 45, train_loss: 20.238 Training correlation: 0.9670168026475823\n",
      "\n",
      " Epoch: 45  Train Loss: 11.3617  Validation Loss: 10.5423  Validation Correlation: 0.9614\n",
      "\n",
      "epoch: 46, train_loss: 9.871 Training correlation: 0.9705252743248725\n",
      "\n",
      " Epoch: 46  Train Loss: 9.4059  Validation Loss: 10.6453  Validation Correlation: 0.9629\n",
      "\n",
      "epoch: 47, train_loss: 7.731 Training correlation: 0.9702941105225299\n",
      "\n",
      " Epoch: 47  Train Loss: 9.5845  Validation Loss: 15.1687  Validation Correlation: 0.9657\n",
      "\n",
      "epoch: 48, train_loss: 11.409 Training correlation: 0.9635498409464724\n",
      "\n",
      " Epoch: 48  Train Loss: 10.0127  Validation Loss: 22.4791  Validation Correlation: 0.9665\n",
      "\n",
      "epoch: 49, train_loss: 13.955 Training correlation: 0.973916295765283\n",
      "\n",
      " Epoch: 49  Train Loss: 10.8417  Validation Loss: 19.1032  Validation Correlation: 0.9659\n",
      "\n",
      "epoch: 50, train_loss: 11.596 Training correlation: 0.958182918597903\n",
      "\n",
      " Epoch: 50  Train Loss: 10.2602  Validation Loss: 12.6678  Validation Correlation: 0.9639\n",
      "\n",
      "epoch: 51, train_loss: 13.818 Training correlation: 0.9780772140115486\n",
      "\n",
      " Epoch: 51  Train Loss: 9.5933  Validation Loss: 13.9046  Validation Correlation: 0.9656\n",
      "\n",
      "epoch: 52, train_loss: 21.769 Training correlation: 0.9742494012787225\n",
      "\n",
      " Epoch: 52  Train Loss: 11.6772  Validation Loss: 35.5486  Validation Correlation: 0.9672\n",
      "\n",
      "epoch: 53, train_loss: 25.112 Training correlation: 0.9723324281254109\n",
      "\n",
      " Epoch: 53  Train Loss: 10.7573  Validation Loss: 9.1509  Validation Correlation: 0.9648\n",
      "\n",
      "epoch: 54, train_loss: 8.565 Training correlation: 0.9718948510975826\n",
      "\n",
      " Epoch: 54  Train Loss: 9.1018  Validation Loss: 32.9920  Validation Correlation: 0.9666\n",
      "\n",
      "epoch: 55, train_loss: 26.802 Training correlation: 0.9653357121294696\n",
      "\n",
      " Epoch: 55  Train Loss: 9.2461  Validation Loss: 18.8023  Validation Correlation: 0.9642\n",
      "\n",
      "epoch: 56, train_loss: 27.627 Training correlation: 0.9750378312349259\n",
      "\n",
      " Epoch: 56  Train Loss: 10.2706  Validation Loss: 12.3551  Validation Correlation: 0.9678\n",
      "\n",
      "epoch: 57, train_loss: 7.938 Training correlation: 0.9709873675743503\n",
      "\n",
      " Epoch: 57  Train Loss: 8.3062  Validation Loss: 8.4612  Validation Correlation: 0.9672\n",
      "\n",
      "epoch: 58, train_loss: 5.854 Training correlation: 0.9838672764311999\n",
      "\n",
      " Epoch: 58  Train Loss: 8.1515  Validation Loss: 8.4160  Validation Correlation: 0.9685\n",
      "\n",
      "epoch: 59, train_loss: 8.847 Training correlation: 0.9697995355367398\n",
      "\n",
      " Epoch: 59  Train Loss: 8.1909  Validation Loss: 8.2645  Validation Correlation: 0.9681\n",
      "\n",
      "epoch: 60, train_loss: 18.254 Training correlation: 0.9199692000991251\n",
      "\n",
      " Epoch: 60  Train Loss: 8.0513  Validation Loss: 9.4340  Validation Correlation: 0.9681\n",
      "\n",
      "epoch: 61, train_loss: 5.673 Training correlation: 0.9761842949344676\n",
      "\n",
      " Epoch: 61  Train Loss: 7.5155  Validation Loss: 9.9913  Validation Correlation: 0.9674\n",
      "\n",
      "epoch: 62, train_loss: 7.100 Training correlation: 0.9735147151573258\n",
      "\n",
      " Epoch: 62  Train Loss: 8.5367  Validation Loss: 10.6137  Validation Correlation: 0.9709\n",
      "\n",
      "epoch: 63, train_loss: 8.443 Training correlation: 0.9638812610544413\n",
      "\n",
      " Epoch: 63  Train Loss: 7.7809  Validation Loss: 7.8118  Validation Correlation: 0.9704\n",
      "\n",
      "epoch: 64, train_loss: 5.381 Training correlation: 0.9828849538480512\n",
      "\n",
      " Epoch: 64  Train Loss: 7.5734  Validation Loss: 34.7393  Validation Correlation: 0.9686\n",
      "\n",
      "epoch: 65, train_loss: 20.862 Training correlation: 0.9810216835920001\n",
      "\n",
      " Epoch: 65  Train Loss: 10.2673  Validation Loss: 20.5080  Validation Correlation: 0.9698\n",
      "\n",
      "epoch: 66, train_loss: 12.847 Training correlation: 0.9723625390686197\n",
      "\n",
      " Epoch: 66  Train Loss: 8.6551  Validation Loss: 7.8296  Validation Correlation: 0.9695\n",
      "\n",
      "epoch: 67, train_loss: 11.531 Training correlation: 0.9623293992982315\n",
      "\n",
      " Epoch: 67  Train Loss: 7.9064  Validation Loss: 12.9556  Validation Correlation: 0.9695\n",
      "\n",
      "epoch: 68, train_loss: 7.124 Training correlation: 0.9782393503694157\n",
      "\n",
      " Epoch: 68  Train Loss: 7.4004  Validation Loss: 12.1899  Validation Correlation: 0.9732\n",
      "\n",
      "epoch: 69, train_loss: 6.050 Training correlation: 0.9819662312127095\n",
      "\n",
      " Epoch: 69  Train Loss: 7.7990  Validation Loss: 13.7206  Validation Correlation: 0.9720\n",
      "\n",
      "epoch: 70, train_loss: 8.618 Training correlation: 0.9782453069321025\n",
      "\n",
      " Epoch: 70  Train Loss: 7.2970  Validation Loss: 11.7594  Validation Correlation: 0.9700\n",
      "\n",
      "epoch: 71, train_loss: 17.492 Training correlation: 0.9799610786197821\n",
      "\n",
      " Epoch: 71  Train Loss: 8.3298  Validation Loss: 9.4369  Validation Correlation: 0.9708\n",
      "\n",
      "epoch: 72, train_loss: 4.481 Training correlation: 0.9868151421212189\n",
      "\n",
      " Epoch: 72  Train Loss: 6.5351  Validation Loss: 11.9996  Validation Correlation: 0.9711\n",
      "\n",
      "epoch: 73, train_loss: 13.187 Training correlation: 0.9614453239337498\n",
      "\n",
      " Epoch: 73  Train Loss: 7.6859  Validation Loss: 18.0427  Validation Correlation: 0.9712\n",
      "\n",
      "epoch: 74, train_loss: 12.219 Training correlation: 0.9838446185458753\n",
      "\n",
      " Epoch: 74  Train Loss: 6.9804  Validation Loss: 10.8832  Validation Correlation: 0.9701\n",
      "\n",
      "epoch: 75, train_loss: 5.567 Training correlation: 0.9785820313311985\n",
      "\n",
      " Epoch: 75  Train Loss: 6.4121  Validation Loss: 7.3652  Validation Correlation: 0.9722\n",
      "\n",
      "epoch: 76, train_loss: 13.085 Training correlation: 0.9712932120068065\n",
      "\n",
      " Epoch: 76  Train Loss: 6.9453  Validation Loss: 7.0290  Validation Correlation: 0.9736\n",
      "\n",
      "epoch: 77, train_loss: 4.944 Training correlation: 0.976190079989482\n",
      "\n",
      " Epoch: 77  Train Loss: 6.7420  Validation Loss: 7.4805  Validation Correlation: 0.9709\n",
      "\n",
      "epoch: 78, train_loss: 4.735 Training correlation: 0.9883106224826138\n",
      "\n",
      " Epoch: 78  Train Loss: 7.1906  Validation Loss: 8.7876  Validation Correlation: 0.9729\n",
      "\n",
      "epoch: 79, train_loss: 5.063 Training correlation: 0.977719115999896\n",
      "\n",
      " Epoch: 79  Train Loss: 6.9930  Validation Loss: 8.7344  Validation Correlation: 0.9710\n",
      "\n",
      "epoch: 80, train_loss: 4.993 Training correlation: 0.9822496174526453\n",
      "\n",
      " Epoch: 80  Train Loss: 6.9518  Validation Loss: 7.9209  Validation Correlation: 0.9734\n",
      "\n",
      "epoch: 81, train_loss: 8.454 Training correlation: 0.9814356302542093\n",
      "\n",
      " Epoch: 81  Train Loss: 7.2389  Validation Loss: 31.9371  Validation Correlation: 0.9725\n",
      "\n",
      "epoch: 82, train_loss: 18.499 Training correlation: 0.9758558917631676\n",
      "\n",
      " Epoch: 82  Train Loss: 8.8237  Validation Loss: 19.1767  Validation Correlation: 0.9753\n",
      "\n",
      "epoch: 83, train_loss: 11.011 Training correlation: 0.9641473569792632\n",
      "\n",
      " Epoch: 83  Train Loss: 7.5818  Validation Loss: 6.7675  Validation Correlation: 0.9740\n",
      "\n",
      "epoch: 84, train_loss: 6.793 Training correlation: 0.9791771178432931\n",
      "\n",
      " Epoch: 84  Train Loss: 6.0061  Validation Loss: 11.1149  Validation Correlation: 0.9736\n",
      "\n",
      "epoch: 85, train_loss: 5.583 Training correlation: 0.9798745299200989\n",
      "\n",
      " Epoch: 85  Train Loss: 6.1728  Validation Loss: 11.4489  Validation Correlation: 0.9745\n",
      "\n",
      "epoch: 86, train_loss: 21.774 Training correlation: 0.9455468408921088\n",
      "\n",
      " Epoch: 86  Train Loss: 8.4607  Validation Loss: 8.4775  Validation Correlation: 0.9730\n",
      "\n",
      "epoch: 87, train_loss: 4.566 Training correlation: 0.9803216104154219\n",
      "\n",
      " Epoch: 87  Train Loss: 6.5151  Validation Loss: 10.8696  Validation Correlation: 0.9729\n",
      "\n",
      "epoch: 88, train_loss: 4.213 Training correlation: 0.9891939300605674\n",
      "\n",
      " Epoch: 88  Train Loss: 5.9962  Validation Loss: 9.8765  Validation Correlation: 0.9716\n",
      "\n",
      "epoch: 89, train_loss: 13.604 Training correlation: 0.9728060746592092\n",
      "\n",
      " Epoch: 89  Train Loss: 5.8607  Validation Loss: 10.2426  Validation Correlation: 0.9740\n",
      "\n",
      "epoch: 90, train_loss: 4.524 Training correlation: 0.9823890452353756\n",
      "\n",
      " Epoch: 90  Train Loss: 5.2776  Validation Loss: 9.1223  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 91, train_loss: 4.729 Training correlation: 0.9867032614798548\n",
      "\n",
      " Epoch: 91  Train Loss: 5.5536  Validation Loss: 11.9488  Validation Correlation: 0.9748\n",
      "\n",
      "epoch: 92, train_loss: 4.655 Training correlation: 0.9848520269063585\n",
      "\n",
      " Epoch: 92  Train Loss: 5.6643  Validation Loss: 6.6525  Validation Correlation: 0.9739\n",
      "\n",
      "epoch: 93, train_loss: 5.284 Training correlation: 0.9734685505386953\n",
      "\n",
      " Epoch: 93  Train Loss: 5.9947  Validation Loss: 8.1546  Validation Correlation: 0.9725\n",
      "\n",
      "epoch: 94, train_loss: 4.144 Training correlation: 0.9884221340888141\n",
      "\n",
      " Epoch: 94  Train Loss: 5.8021  Validation Loss: 16.1023  Validation Correlation: 0.9748\n",
      "\n",
      "epoch: 95, train_loss: 6.026 Training correlation: 0.9773639086410201\n",
      "\n",
      " Epoch: 95  Train Loss: 6.5939  Validation Loss: 6.6854  Validation Correlation: 0.9748\n",
      "\n",
      "epoch: 96, train_loss: 6.925 Training correlation: 0.9743349979240046\n",
      "\n",
      " Epoch: 96  Train Loss: 5.4012  Validation Loss: 10.6840  Validation Correlation: 0.9741\n",
      "\n",
      "epoch: 97, train_loss: 6.780 Training correlation: 0.9762896785731212\n",
      "\n",
      " Epoch: 97  Train Loss: 14.3560  Validation Loss: 11.5168  Validation Correlation: 0.9688\n",
      "\n",
      "epoch: 98, train_loss: 6.822 Training correlation: 0.9776760052613018\n",
      "\n",
      " Epoch: 98  Train Loss: 6.8321  Validation Loss: 13.8009  Validation Correlation: 0.9735\n",
      "\n",
      "epoch: 99, train_loss: 13.640 Training correlation: 0.9663929013885789\n",
      "\n",
      " Epoch: 99  Train Loss: 7.5259  Validation Loss: 6.9964  Validation Correlation: 0.9728\n",
      "\n",
      "epoch: 100, train_loss: 4.498 Training correlation: 0.9841807812928826\n",
      "\n",
      " Epoch: 100  Train Loss: 5.8737  Validation Loss: 7.2207  Validation Correlation: 0.9724\n",
      "\n",
      "epoch: 101, train_loss: 4.358 Training correlation: 0.99031372545494\n",
      "\n",
      " Epoch: 101  Train Loss: 5.7459  Validation Loss: 7.4038  Validation Correlation: 0.9738\n",
      "\n",
      "epoch: 102, train_loss: 4.812 Training correlation: 0.9819495371574112\n",
      "\n",
      " Epoch: 102  Train Loss: 5.8546  Validation Loss: 13.2781  Validation Correlation: 0.9722\n",
      "\n",
      "epoch: 103, train_loss: 8.646 Training correlation: 0.9826801523212939\n",
      "\n",
      " Epoch: 103  Train Loss: 5.8767  Validation Loss: 7.1516  Validation Correlation: 0.9749\n",
      "\n",
      "epoch: 104, train_loss: 10.098 Training correlation: 0.9687437779293396\n",
      "\n",
      " Epoch: 104  Train Loss: 6.2365  Validation Loss: 7.1015  Validation Correlation: 0.9738\n",
      "\n",
      "epoch: 105, train_loss: 5.785 Training correlation: 0.9868898809124456\n",
      "\n",
      " Epoch: 105  Train Loss: 5.4225  Validation Loss: 8.2140  Validation Correlation: 0.9720\n",
      "\n",
      "epoch: 106, train_loss: 5.457 Training correlation: 0.9841002747276822\n",
      "\n",
      " Epoch: 106  Train Loss: 6.3072  Validation Loss: 7.1415  Validation Correlation: 0.9719\n",
      "\n",
      "epoch: 107, train_loss: 9.197 Training correlation: 0.9646912825678329\n",
      "\n",
      " Epoch: 107  Train Loss: 5.2217  Validation Loss: 6.7918  Validation Correlation: 0.9736\n",
      "\n",
      "epoch: 108, train_loss: 4.859 Training correlation: 0.9828323102330703\n",
      "\n",
      " Epoch: 108  Train Loss: 5.8815  Validation Loss: 8.4647  Validation Correlation: 0.9724\n",
      "\n",
      "epoch: 109, train_loss: 11.940 Training correlation: 0.979806744052949\n",
      "\n",
      " Epoch: 109  Train Loss: 6.7896  Validation Loss: 11.3607  Validation Correlation: 0.9759\n",
      "\n",
      "epoch: 110, train_loss: 6.358 Training correlation: 0.9735915466427818\n",
      "\n",
      " Epoch: 110  Train Loss: 5.9765  Validation Loss: 7.2952  Validation Correlation: 0.9740\n",
      "\n",
      "epoch: 111, train_loss: 3.555 Training correlation: 0.9862144576044856\n",
      "\n",
      " Epoch: 111  Train Loss: 5.1402  Validation Loss: 6.2366  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 112, train_loss: 5.605 Training correlation: 0.9829413611036887\n",
      "\n",
      " Epoch: 112  Train Loss: 5.1798  Validation Loss: 8.1994  Validation Correlation: 0.9743\n",
      "\n",
      "epoch: 113, train_loss: 9.631 Training correlation: 0.9884683470863088\n",
      "\n",
      " Epoch: 113  Train Loss: 6.0805  Validation Loss: 9.4743  Validation Correlation: 0.9743\n",
      "\n",
      "epoch: 114, train_loss: 5.935 Training correlation: 0.9812200539275076\n",
      "\n",
      " Epoch: 114  Train Loss: 5.5203  Validation Loss: 6.8013  Validation Correlation: 0.9747\n",
      "\n",
      "epoch: 115, train_loss: 4.673 Training correlation: 0.9861234186845896\n",
      "\n",
      " Epoch: 115  Train Loss: 5.1120  Validation Loss: 9.3137  Validation Correlation: 0.9750\n",
      "\n",
      "epoch: 116, train_loss: 6.371 Training correlation: 0.9768289032028747\n",
      "\n",
      " Epoch: 116  Train Loss: 5.5233  Validation Loss: 6.8689  Validation Correlation: 0.9758\n",
      "\n",
      "epoch: 117, train_loss: 6.398 Training correlation: 0.9881078606065673\n",
      "\n",
      " Epoch: 117  Train Loss: 5.0829  Validation Loss: 6.7854  Validation Correlation: 0.9749\n",
      "\n",
      "epoch: 118, train_loss: 5.131 Training correlation: 0.9881275163367349\n",
      "\n",
      " Epoch: 118  Train Loss: 4.9849  Validation Loss: 12.7801  Validation Correlation: 0.9753\n",
      "\n",
      "epoch: 119, train_loss: 7.796 Training correlation: 0.9797613483148779\n",
      "\n",
      " Epoch: 119  Train Loss: 4.5943  Validation Loss: 11.3039  Validation Correlation: 0.9751\n",
      "\n",
      "epoch: 120, train_loss: 4.418 Training correlation: 0.9886168477938577\n",
      "\n",
      " Epoch: 120  Train Loss: 5.4966  Validation Loss: 6.3004  Validation Correlation: 0.9754\n",
      "\n",
      "epoch: 121, train_loss: 6.107 Training correlation: 0.978335002961963\n",
      "\n",
      " Epoch: 121  Train Loss: 4.7841  Validation Loss: 6.9264  Validation Correlation: 0.9747\n",
      "\n",
      "epoch: 122, train_loss: 6.124 Training correlation: 0.9839826979067892\n",
      "\n",
      " Epoch: 122  Train Loss: 5.0587  Validation Loss: 6.7286  Validation Correlation: 0.9739\n",
      "\n",
      "epoch: 123, train_loss: 4.646 Training correlation: 0.9877660872479052\n",
      "\n",
      " Epoch: 123  Train Loss: 4.6047  Validation Loss: 6.8559  Validation Correlation: 0.9748\n",
      "\n",
      "epoch: 124, train_loss: 3.398 Training correlation: 0.987116274703096\n",
      "\n",
      " Epoch: 124  Train Loss: 4.6752  Validation Loss: 10.4706  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 125, train_loss: 5.839 Training correlation: 0.9873968392734811\n",
      "\n",
      " Epoch: 125  Train Loss: 4.9156  Validation Loss: 6.1021  Validation Correlation: 0.9765\n",
      "\n",
      "epoch: 126, train_loss: 5.117 Training correlation: 0.9805647330905625\n",
      "\n",
      " Epoch: 126  Train Loss: 5.5087  Validation Loss: 12.9289  Validation Correlation: 0.9755\n",
      "\n",
      "epoch: 127, train_loss: 15.034 Training correlation: 0.9870422678082801\n",
      "\n",
      " Epoch: 127  Train Loss: 6.1709  Validation Loss: 7.5990  Validation Correlation: 0.9744\n",
      "\n",
      "epoch: 128, train_loss: 5.725 Training correlation: 0.97734275048248\n",
      "\n",
      " Epoch: 128  Train Loss: 4.9551  Validation Loss: 7.0853  Validation Correlation: 0.9753\n",
      "\n",
      "epoch: 129, train_loss: 6.626 Training correlation: 0.9804087527058839\n",
      "\n",
      " Epoch: 129  Train Loss: 4.9804  Validation Loss: 6.1220  Validation Correlation: 0.9762\n",
      "\n",
      "epoch: 130, train_loss: 3.473 Training correlation: 0.9888337746623002\n",
      "\n",
      " Epoch: 130  Train Loss: 4.7388  Validation Loss: 6.1727  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 131, train_loss: 7.675 Training correlation: 0.9804960279367942\n",
      "\n",
      " Epoch: 131  Train Loss: 4.4987  Validation Loss: 6.6689  Validation Correlation: 0.9755\n",
      "\n",
      "epoch: 132, train_loss: 2.296 Training correlation: 0.9880120657835869\n",
      "\n",
      " Epoch: 132  Train Loss: 4.3676  Validation Loss: 6.2786  Validation Correlation: 0.9757\n",
      "\n",
      "epoch: 133, train_loss: 4.285 Training correlation: 0.9833493936334725\n",
      "\n",
      " Epoch: 133  Train Loss: 4.6680  Validation Loss: 6.6006  Validation Correlation: 0.9749\n",
      "\n",
      "epoch: 134, train_loss: 3.558 Training correlation: 0.9885794691989919\n",
      "\n",
      " Epoch: 134  Train Loss: 5.2879  Validation Loss: 7.3713  Validation Correlation: 0.9742\n",
      "\n",
      "epoch: 135, train_loss: 4.876 Training correlation: 0.9866549477402599\n",
      "\n",
      " Epoch: 135  Train Loss: 4.7108  Validation Loss: 7.6459  Validation Correlation: 0.9750\n",
      "\n",
      "epoch: 136, train_loss: 3.013 Training correlation: 0.9863738940110005\n",
      "\n",
      " Epoch: 136  Train Loss: 4.1375  Validation Loss: 6.5730  Validation Correlation: 0.9742\n",
      "\n",
      "epoch: 137, train_loss: 4.065 Training correlation: 0.9890139494361581\n",
      "\n",
      " Epoch: 137  Train Loss: 4.0876  Validation Loss: 10.4398  Validation Correlation: 0.9744\n",
      "\n",
      "epoch: 138, train_loss: 8.488 Training correlation: 0.9735571829345284\n",
      "\n",
      " Epoch: 138  Train Loss: 4.1879  Validation Loss: 12.4282  Validation Correlation: 0.9748\n",
      "\n",
      "epoch: 139, train_loss: 7.264 Training correlation: 0.9856601183217047\n",
      "\n",
      " Epoch: 139  Train Loss: 5.3957  Validation Loss: 7.6285  Validation Correlation: 0.9745\n",
      "\n",
      "epoch: 140, train_loss: 6.659 Training correlation: 0.9885577718399773\n",
      "\n",
      " Epoch: 140  Train Loss: 5.6849  Validation Loss: 6.5853  Validation Correlation: 0.9748\n",
      "\n",
      "epoch: 141, train_loss: 3.819 Training correlation: 0.9862907010885984\n",
      "\n",
      " Epoch: 141  Train Loss: 4.3411  Validation Loss: 10.6849  Validation Correlation: 0.9732\n",
      "\n",
      "epoch: 142, train_loss: 5.259 Training correlation: 0.9798292170299909\n",
      "\n",
      " Epoch: 142  Train Loss: 4.5742  Validation Loss: 7.3792  Validation Correlation: 0.9752\n",
      "\n",
      "epoch: 143, train_loss: 5.912 Training correlation: 0.974382227333146\n",
      "\n",
      " Epoch: 143  Train Loss: 4.0288  Validation Loss: 6.3006  Validation Correlation: 0.9762\n",
      "\n",
      "epoch: 144, train_loss: 3.556 Training correlation: 0.9842066594739907\n",
      "\n",
      " Epoch: 144  Train Loss: 4.1370  Validation Loss: 12.4607  Validation Correlation: 0.9769\n",
      "\n",
      "epoch: 145, train_loss: 6.091 Training correlation: 0.9849189299465141\n",
      "\n",
      " Epoch: 145  Train Loss: 4.5658  Validation Loss: 9.4725  Validation Correlation: 0.9756\n",
      "\n",
      "epoch: 146, train_loss: 8.435 Training correlation: 0.9894806444028338\n",
      "\n",
      " Epoch: 146  Train Loss: 6.2148  Validation Loss: 8.9627  Validation Correlation: 0.9746\n",
      "\n",
      "epoch: 147, train_loss: 11.305 Training correlation: 0.9844418474950624\n",
      "\n",
      " Epoch: 147  Train Loss: 5.7113  Validation Loss: 12.4267  Validation Correlation: 0.9740\n",
      "\n",
      "epoch: 148, train_loss: 6.910 Training correlation: 0.9862777728407582\n",
      "\n",
      " Epoch: 148  Train Loss: 5.2704  Validation Loss: 9.4955  Validation Correlation: 0.9746\n",
      "\n",
      "epoch: 149, train_loss: 3.987 Training correlation: 0.9862643610000846\n",
      "\n",
      " Epoch: 149  Train Loss: 4.2156  Validation Loss: 6.7883  Validation Correlation: 0.9743\n",
      "\n",
      "epoch: 150, train_loss: 3.413 Training correlation: 0.9903627742226949\n",
      "\n",
      " Epoch: 150  Train Loss: 4.5383  Validation Loss: 13.1858  Validation Correlation: 0.9745\n",
      "\n",
      "epoch: 151, train_loss: 18.699 Training correlation: 0.9781654468178034\n",
      "\n",
      " Epoch: 151  Train Loss: 5.6060  Validation Loss: 8.8822  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 152, train_loss: 4.029 Training correlation: 0.9844375783690663\n",
      "\n",
      " Epoch: 152  Train Loss: 4.3437  Validation Loss: 6.8056  Validation Correlation: 0.9749\n",
      "\n",
      "epoch: 153, train_loss: 4.203 Training correlation: 0.9884368874614385\n",
      "\n",
      " Epoch: 153  Train Loss: 4.9841  Validation Loss: 9.6588  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 154, train_loss: 6.099 Training correlation: 0.9843036081259263\n",
      "\n",
      " Epoch: 154  Train Loss: 4.1757  Validation Loss: 6.1438  Validation Correlation: 0.9762\n",
      "\n",
      "epoch: 155, train_loss: 4.421 Training correlation: 0.9857065971761348\n",
      "\n",
      " Epoch: 155  Train Loss: 4.5615  Validation Loss: 6.3132  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 156, train_loss: 3.149 Training correlation: 0.9872600467382104\n",
      "\n",
      " Epoch: 156  Train Loss: 4.2289  Validation Loss: 6.2651  Validation Correlation: 0.9754\n",
      "\n",
      "epoch: 157, train_loss: 3.458 Training correlation: 0.9832695099038447\n",
      "\n",
      " Epoch: 157  Train Loss: 4.3368  Validation Loss: 6.7111  Validation Correlation: 0.9742\n",
      "\n",
      "epoch: 158, train_loss: 4.970 Training correlation: 0.9851131085471273\n",
      "\n",
      " Epoch: 158  Train Loss: 4.7091  Validation Loss: 7.4055  Validation Correlation: 0.9754\n",
      "\n",
      "epoch: 159, train_loss: 10.317 Training correlation: 0.98250203821955\n",
      "\n",
      " Epoch: 159  Train Loss: 4.6417  Validation Loss: 6.7292  Validation Correlation: 0.9743\n",
      "\n",
      "epoch: 160, train_loss: 6.076 Training correlation: 0.9769747177844663\n",
      "\n",
      " Epoch: 160  Train Loss: 4.9283  Validation Loss: 6.3880  Validation Correlation: 0.9756\n",
      "\n",
      "epoch: 161, train_loss: 2.249 Training correlation: 0.9922940951987548\n",
      "\n",
      " Epoch: 161  Train Loss: 4.4636  Validation Loss: 8.6022  Validation Correlation: 0.9765\n",
      "\n",
      "epoch: 162, train_loss: 9.959 Training correlation: 0.9871827401207157\n",
      "\n",
      " Epoch: 162  Train Loss: 4.9260  Validation Loss: 6.2417  Validation Correlation: 0.9756\n",
      "\n",
      "epoch: 163, train_loss: 4.135 Training correlation: 0.9835950619504964\n",
      "\n",
      " Epoch: 163  Train Loss: 6.1028  Validation Loss: 10.3886  Validation Correlation: 0.9750\n",
      "\n",
      "epoch: 164, train_loss: 7.418 Training correlation: 0.9854081655659728\n",
      "\n",
      " Epoch: 164  Train Loss: 4.2974  Validation Loss: 10.1109  Validation Correlation: 0.9755\n",
      "\n",
      "epoch: 165, train_loss: 5.068 Training correlation: 0.9857049334870576\n",
      "\n",
      " Epoch: 165  Train Loss: 4.3470  Validation Loss: 8.0338  Validation Correlation: 0.9743\n",
      "\n",
      "epoch: 166, train_loss: 3.700 Training correlation: 0.988013150896587\n",
      "\n",
      " Epoch: 166  Train Loss: 4.0952  Validation Loss: 9.7025  Validation Correlation: 0.9742\n",
      "\n",
      "epoch: 167, train_loss: 4.958 Training correlation: 0.9843715267604294\n",
      "\n",
      " Epoch: 167  Train Loss: 3.8400  Validation Loss: 6.0828  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 168, train_loss: 4.379 Training correlation: 0.9882513891239095\n",
      "\n",
      " Epoch: 168  Train Loss: 4.4337  Validation Loss: 8.9225  Validation Correlation: 0.9756\n",
      "\n",
      "epoch: 169, train_loss: 3.524 Training correlation: 0.9875568783063957\n",
      "\n",
      " Epoch: 169  Train Loss: 3.9977  Validation Loss: 7.2689  Validation Correlation: 0.9762\n",
      "\n",
      "epoch: 170, train_loss: 3.665 Training correlation: 0.986409272876086\n",
      "\n",
      " Epoch: 170  Train Loss: 4.5872  Validation Loss: 6.6648  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 171, train_loss: 4.199 Training correlation: 0.987519032125483\n",
      "\n",
      " Epoch: 171  Train Loss: 3.8927  Validation Loss: 6.8181  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 172, train_loss: 2.294 Training correlation: 0.9913723816566878\n",
      "\n",
      " Epoch: 172  Train Loss: 3.8925  Validation Loss: 6.2962  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 173, train_loss: 2.969 Training correlation: 0.9907234738576227\n",
      "\n",
      " Epoch: 173  Train Loss: 3.9868  Validation Loss: 6.5762  Validation Correlation: 0.9754\n",
      "\n",
      "epoch: 174, train_loss: 2.929 Training correlation: 0.9874224971683684\n",
      "\n",
      " Epoch: 174  Train Loss: 3.7221  Validation Loss: 6.6156  Validation Correlation: 0.9754\n",
      "\n",
      "epoch: 175, train_loss: 3.273 Training correlation: 0.9879268793223405\n",
      "\n",
      " Epoch: 175  Train Loss: 4.3328  Validation Loss: 9.6536  Validation Correlation: 0.9766\n",
      "\n",
      "epoch: 176, train_loss: 5.117 Training correlation: 0.9868729893159945\n",
      "\n",
      " Epoch: 176  Train Loss: 4.7849  Validation Loss: 6.9699  Validation Correlation: 0.9741\n",
      "\n",
      "epoch: 177, train_loss: 3.812 Training correlation: 0.9870221728899023\n",
      "\n",
      " Epoch: 177  Train Loss: 4.1023  Validation Loss: 7.3409  Validation Correlation: 0.9754\n",
      "\n",
      "epoch: 178, train_loss: 2.818 Training correlation: 0.9898750038556822\n",
      "\n",
      " Epoch: 178  Train Loss: 3.9937  Validation Loss: 20.7756  Validation Correlation: 0.9765\n",
      "\n",
      "epoch: 179, train_loss: 14.045 Training correlation: 0.9824843579204181\n",
      "\n",
      " Epoch: 179  Train Loss: 4.7010  Validation Loss: 8.8305  Validation Correlation: 0.9750\n",
      "\n",
      "epoch: 180, train_loss: 11.673 Training correlation: 0.9839818927518363\n",
      "\n",
      " Epoch: 180  Train Loss: 4.4548  Validation Loss: 6.7838  Validation Correlation: 0.9757\n",
      "\n",
      "epoch: 181, train_loss: 2.839 Training correlation: 0.9884485351645907\n",
      "\n",
      " Epoch: 181  Train Loss: 4.0067  Validation Loss: 13.0525  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 182, train_loss: 7.334 Training correlation: 0.9903584630324196\n",
      "\n",
      " Epoch: 182  Train Loss: 4.3827  Validation Loss: 6.2395  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 183, train_loss: 5.469 Training correlation: 0.9900045626430358\n",
      "\n",
      " Epoch: 183  Train Loss: 4.0843  Validation Loss: 6.3515  Validation Correlation: 0.9759\n",
      "\n",
      "epoch: 184, train_loss: 3.732 Training correlation: 0.9860332391650061\n",
      "\n",
      " Epoch: 184  Train Loss: 4.1762  Validation Loss: 8.3277  Validation Correlation: 0.9742\n",
      "\n",
      "epoch: 185, train_loss: 3.364 Training correlation: 0.9874501738883182\n",
      "\n",
      " Epoch: 185  Train Loss: 4.0704  Validation Loss: 12.9468  Validation Correlation: 0.9751\n",
      "\n",
      "epoch: 186, train_loss: 6.964 Training correlation: 0.9879223498333416\n",
      "\n",
      " Epoch: 186  Train Loss: 5.1267  Validation Loss: 8.8212  Validation Correlation: 0.9752\n",
      "\n",
      "epoch: 187, train_loss: 4.128 Training correlation: 0.9907441743113505\n",
      "\n",
      " Epoch: 187  Train Loss: 3.8638  Validation Loss: 7.4996  Validation Correlation: 0.9753\n",
      "\n",
      "epoch: 188, train_loss: 3.343 Training correlation: 0.9862722156808011\n",
      "\n",
      " Epoch: 188  Train Loss: 3.8394  Validation Loss: 6.9601  Validation Correlation: 0.9746\n",
      "\n",
      "epoch: 189, train_loss: 2.875 Training correlation: 0.9893629639082029\n",
      "\n",
      " Epoch: 189  Train Loss: 3.7639  Validation Loss: 6.3779  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 190, train_loss: 3.097 Training correlation: 0.9868168013882055\n",
      "\n",
      " Epoch: 190  Train Loss: 3.8702  Validation Loss: 6.2597  Validation Correlation: 0.9771\n",
      "\n",
      "epoch: 191, train_loss: 3.169 Training correlation: 0.9854299689633986\n",
      "\n",
      " Epoch: 191  Train Loss: 3.9421  Validation Loss: 9.7048  Validation Correlation: 0.9758\n",
      "\n",
      "epoch: 192, train_loss: 3.317 Training correlation: 0.9906218717461751\n",
      "\n",
      " Epoch: 192  Train Loss: 3.5842  Validation Loss: 7.0793  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 193, train_loss: 8.118 Training correlation: 0.9810875332813418\n",
      "\n",
      " Epoch: 193  Train Loss: 4.6057  Validation Loss: 13.2359  Validation Correlation: 0.9749\n",
      "\n",
      "epoch: 194, train_loss: 7.721 Training correlation: 0.9862708765069489\n",
      "\n",
      " Epoch: 194  Train Loss: 4.1762  Validation Loss: 6.5367  Validation Correlation: 0.9744\n",
      "\n",
      "epoch: 195, train_loss: 5.123 Training correlation: 0.9741775420859362\n",
      "\n",
      " Epoch: 195  Train Loss: 3.9905  Validation Loss: 7.9657  Validation Correlation: 0.9749\n",
      "\n",
      "epoch: 196, train_loss: 3.278 Training correlation: 0.9897822512882084\n",
      "\n",
      " Epoch: 196  Train Loss: 3.6381  Validation Loss: 6.2739  Validation Correlation: 0.9758\n",
      "\n",
      "epoch: 197, train_loss: 3.693 Training correlation: 0.9890410941848103\n",
      "\n",
      " Epoch: 197  Train Loss: 4.1736  Validation Loss: 11.9011  Validation Correlation: 0.9769\n",
      "\n",
      "epoch: 198, train_loss: 3.533 Training correlation: 0.9902106103649978\n",
      "\n",
      " Epoch: 198  Train Loss: 3.8016  Validation Loss: 8.5058  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 199, train_loss: 3.325 Training correlation: 0.988290268676803\n",
      "\n",
      " Epoch: 199  Train Loss: 3.4977  Validation Loss: 6.7136  Validation Correlation: 0.9753\n",
      "\n",
      "epoch: 200, train_loss: 3.781 Training correlation: 0.9877086506358201\n",
      "\n",
      " Epoch: 200  Train Loss: 3.2502  Validation Loss: 6.8618  Validation Correlation: 0.9759\n",
      "\n",
      "epoch: 201, train_loss: 7.019 Training correlation: 0.9893146199428436\n",
      "\n",
      " Epoch: 201  Train Loss: 3.8398  Validation Loss: 10.7776  Validation Correlation: 0.9751\n",
      "\n",
      "epoch: 202, train_loss: 6.752 Training correlation: 0.9852002629568285\n",
      "\n",
      " Epoch: 202  Train Loss: 4.6199  Validation Loss: 6.5901  Validation Correlation: 0.9756\n",
      "\n",
      "epoch: 203, train_loss: 3.262 Training correlation: 0.9868292719560783\n",
      "\n",
      " Epoch: 203  Train Loss: 3.9158  Validation Loss: 8.9614  Validation Correlation: 0.9757\n",
      "\n",
      "epoch: 204, train_loss: 4.017 Training correlation: 0.9857408688367513\n",
      "\n",
      " Epoch: 204  Train Loss: 3.8215  Validation Loss: 6.0229  Validation Correlation: 0.9770\n",
      "\n",
      "epoch: 205, train_loss: 5.350 Training correlation: 0.9733262465272933\n",
      "\n",
      " Epoch: 205  Train Loss: 3.4901  Validation Loss: 7.8141  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 206, train_loss: 2.361 Training correlation: 0.9914188916107759\n",
      "\n",
      " Epoch: 206  Train Loss: 3.2861  Validation Loss: 9.8115  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 207, train_loss: 4.160 Training correlation: 0.9876903792007314\n",
      "\n",
      " Epoch: 207  Train Loss: 3.5689  Validation Loss: 11.0607  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 208, train_loss: 4.548 Training correlation: 0.9927743339691928\n",
      "\n",
      " Epoch: 208  Train Loss: 4.1435  Validation Loss: 6.2073  Validation Correlation: 0.9756\n",
      "\n",
      "epoch: 209, train_loss: 2.796 Training correlation: 0.990905173444968\n",
      "\n",
      " Epoch: 209  Train Loss: 3.7818  Validation Loss: 6.6613  Validation Correlation: 0.9748\n",
      "\n",
      "epoch: 210, train_loss: 2.511 Training correlation: 0.991196816582941\n",
      "\n",
      " Epoch: 210  Train Loss: 3.3182  Validation Loss: 6.9120  Validation Correlation: 0.9755\n",
      "\n",
      "epoch: 211, train_loss: 3.450 Training correlation: 0.9859300804786407\n",
      "\n",
      " Epoch: 211  Train Loss: 3.3506  Validation Loss: 6.9199  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 212, train_loss: 3.839 Training correlation: 0.986069552613547\n",
      "\n",
      " Epoch: 212  Train Loss: 3.3235  Validation Loss: 6.5510  Validation Correlation: 0.9769\n",
      "\n",
      "epoch: 213, train_loss: 2.219 Training correlation: 0.9920165868264962\n",
      "\n",
      " Epoch: 213  Train Loss: 3.6794  Validation Loss: 6.7876  Validation Correlation: 0.9752\n",
      "\n",
      "epoch: 214, train_loss: 3.221 Training correlation: 0.9856938050917835\n",
      "\n",
      " Epoch: 214  Train Loss: 3.7306  Validation Loss: 6.5196  Validation Correlation: 0.9758\n",
      "\n",
      "epoch: 215, train_loss: 2.992 Training correlation: 0.9882998871592873\n",
      "\n",
      " Epoch: 215  Train Loss: 3.4030  Validation Loss: 7.0587  Validation Correlation: 0.9765\n",
      "\n",
      "epoch: 216, train_loss: 3.089 Training correlation: 0.9880864075870421\n",
      "\n",
      " Epoch: 216  Train Loss: 3.3458  Validation Loss: 8.2267  Validation Correlation: 0.9749\n",
      "\n",
      "epoch: 217, train_loss: 2.897 Training correlation: 0.9914189956361875\n",
      "\n",
      " Epoch: 217  Train Loss: 3.1787  Validation Loss: 7.4029  Validation Correlation: 0.9773\n",
      "\n",
      "epoch: 218, train_loss: 5.566 Training correlation: 0.9914607444671165\n",
      "\n",
      " Epoch: 218  Train Loss: 3.3529  Validation Loss: 10.8085  Validation Correlation: 0.9764\n",
      "\n",
      "epoch: 219, train_loss: 6.105 Training correlation: 0.9876146611108333\n",
      "\n",
      " Epoch: 219  Train Loss: 3.3190  Validation Loss: 6.3386  Validation Correlation: 0.9755\n",
      "\n",
      "epoch: 220, train_loss: 2.846 Training correlation: 0.9917201582617033\n",
      "\n",
      " Epoch: 220  Train Loss: 3.1125  Validation Loss: 8.2813  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 221, train_loss: 4.405 Training correlation: 0.986356783168211\n",
      "\n",
      " Epoch: 221  Train Loss: 3.4491  Validation Loss: 17.8229  Validation Correlation: 0.9752\n",
      "\n",
      "epoch: 222, train_loss: 11.379 Training correlation: 0.9874206673471388\n",
      "\n",
      " Epoch: 222  Train Loss: 4.0127  Validation Loss: 10.5759  Validation Correlation: 0.9748\n",
      "\n",
      "epoch: 223, train_loss: 6.926 Training correlation: 0.9828842186813334\n",
      "\n",
      " Epoch: 223  Train Loss: 3.1067  Validation Loss: 7.3826  Validation Correlation: 0.9762\n",
      "\n",
      "epoch: 224, train_loss: 5.975 Training correlation: 0.9863342085655745\n",
      "\n",
      " Epoch: 224  Train Loss: 4.1839  Validation Loss: 7.1357  Validation Correlation: 0.9750\n",
      "\n",
      "epoch: 225, train_loss: 4.207 Training correlation: 0.9903622438659871\n",
      "\n",
      " Epoch: 225  Train Loss: 3.2017  Validation Loss: 9.1040  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 226, train_loss: 3.370 Training correlation: 0.9929178983017338\n",
      "\n",
      " Epoch: 226  Train Loss: 3.4881  Validation Loss: 6.1430  Validation Correlation: 0.9762\n",
      "\n",
      "epoch: 227, train_loss: 2.287 Training correlation: 0.9925457452170624\n",
      "\n",
      " Epoch: 227  Train Loss: 3.2908  Validation Loss: 6.3484  Validation Correlation: 0.9754\n",
      "\n",
      "epoch: 228, train_loss: 4.685 Training correlation: 0.9808917504475928\n",
      "\n",
      " Epoch: 228  Train Loss: 3.3195  Validation Loss: 7.6592  Validation Correlation: 0.9740\n",
      "\n",
      "epoch: 229, train_loss: 2.880 Training correlation: 0.9894702460539533\n",
      "\n",
      " Epoch: 229  Train Loss: 3.3005  Validation Loss: 8.2256  Validation Correlation: 0.9748\n",
      "\n",
      "epoch: 230, train_loss: 3.919 Training correlation: 0.9843748018717804\n",
      "\n",
      " Epoch: 230  Train Loss: 3.2561  Validation Loss: 6.8916  Validation Correlation: 0.9747\n",
      "\n",
      "epoch: 231, train_loss: 3.403 Training correlation: 0.9879039433302411\n",
      "\n",
      " Epoch: 231  Train Loss: 3.4956  Validation Loss: 5.8863  Validation Correlation: 0.9772\n",
      "\n",
      "epoch: 232, train_loss: 4.305 Training correlation: 0.9864190578663936\n",
      "\n",
      " Epoch: 232  Train Loss: 3.0216  Validation Loss: 6.3898  Validation Correlation: 0.9752\n",
      "\n",
      "epoch: 233, train_loss: 3.378 Training correlation: 0.9898959467724848\n",
      "\n",
      " Epoch: 233  Train Loss: 3.1815  Validation Loss: 7.5120  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 234, train_loss: 2.540 Training correlation: 0.9926624998584209\n",
      "\n",
      " Epoch: 234  Train Loss: 2.8628  Validation Loss: 6.8888  Validation Correlation: 0.9754\n",
      "\n",
      "epoch: 235, train_loss: 2.907 Training correlation: 0.988977471462457\n",
      "\n",
      " Epoch: 235  Train Loss: 3.1061  Validation Loss: 7.1455  Validation Correlation: 0.9747\n",
      "\n",
      "epoch: 236, train_loss: 3.447 Training correlation: 0.9881957275089707\n",
      "\n",
      " Epoch: 236  Train Loss: 3.5089  Validation Loss: 6.0562  Validation Correlation: 0.9770\n",
      "\n",
      "epoch: 237, train_loss: 3.262 Training correlation: 0.9890770041162569\n",
      "\n",
      " Epoch: 237  Train Loss: 2.9728  Validation Loss: 7.1715  Validation Correlation: 0.9756\n",
      "\n",
      "epoch: 238, train_loss: 2.292 Training correlation: 0.9902542793466033\n",
      "\n",
      " Epoch: 238  Train Loss: 3.3331  Validation Loss: 31.0913  Validation Correlation: 0.9755\n",
      "\n",
      "epoch: 239, train_loss: 39.054 Training correlation: 0.9897597176321329\n",
      "\n",
      " Epoch: 239  Train Loss: 5.7773  Validation Loss: 7.8509  Validation Correlation: 0.9742\n",
      "\n",
      "epoch: 240, train_loss: 4.169 Training correlation: 0.9833368332173518\n",
      "\n",
      " Epoch: 240  Train Loss: 3.3838  Validation Loss: 6.7843  Validation Correlation: 0.9753\n",
      "\n",
      "epoch: 241, train_loss: 2.645 Training correlation: 0.9900984194204187\n",
      "\n",
      " Epoch: 241  Train Loss: 3.3251  Validation Loss: 6.2259  Validation Correlation: 0.9762\n",
      "\n",
      "epoch: 242, train_loss: 4.727 Training correlation: 0.985735486337222\n",
      "\n",
      " Epoch: 242  Train Loss: 3.6077  Validation Loss: 6.5163  Validation Correlation: 0.9748\n",
      "\n",
      "epoch: 243, train_loss: 4.095 Training correlation: 0.9850303837044133\n",
      "\n",
      " Epoch: 243  Train Loss: 3.0503  Validation Loss: 6.7198  Validation Correlation: 0.9752\n",
      "\n",
      "epoch: 244, train_loss: 2.583 Training correlation: 0.992475650388239\n",
      "\n",
      " Epoch: 244  Train Loss: 3.4454  Validation Loss: 7.9599  Validation Correlation: 0.9744\n",
      "\n",
      "epoch: 245, train_loss: 2.954 Training correlation: 0.9900858925550751\n",
      "\n",
      " Epoch: 245  Train Loss: 3.1279  Validation Loss: 8.4604  Validation Correlation: 0.9740\n",
      "\n",
      "epoch: 246, train_loss: 2.965 Training correlation: 0.9906318581669459\n",
      "\n",
      " Epoch: 246  Train Loss: 3.9223  Validation Loss: 8.2505  Validation Correlation: 0.9749\n",
      "\n",
      "epoch: 247, train_loss: 2.808 Training correlation: 0.9914305995940904\n",
      "\n",
      " Epoch: 247  Train Loss: 2.9655  Validation Loss: 6.9530  Validation Correlation: 0.9754\n",
      "\n",
      "epoch: 248, train_loss: 6.797 Training correlation: 0.9889747553811361\n",
      "\n",
      " Epoch: 248  Train Loss: 3.9324  Validation Loss: 6.3433  Validation Correlation: 0.9753\n",
      "\n",
      "epoch: 249, train_loss: 4.617 Training correlation: 0.9854778551118494\n",
      "\n",
      " Epoch: 249  Train Loss: 3.0353  Validation Loss: 10.2094  Validation Correlation: 0.9757\n",
      "\n",
      "epoch: 250, train_loss: 3.913 Training correlation: 0.9859550962528573\n",
      "\n",
      " Epoch: 250  Train Loss: 2.9122  Validation Loss: 6.0305  Validation Correlation: 0.9774\n",
      "\n",
      "epoch: 251, train_loss: 3.336 Training correlation: 0.9890612562819987\n",
      "\n",
      " Epoch: 251  Train Loss: 3.2832  Validation Loss: 6.5705  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 252, train_loss: 4.522 Training correlation: 0.9885288928520349\n",
      "\n",
      " Epoch: 252  Train Loss: 3.3257  Validation Loss: 6.0980  Validation Correlation: 0.9766\n",
      "\n",
      "epoch: 253, train_loss: 3.303 Training correlation: 0.9914754205623205\n",
      "\n",
      " Epoch: 253  Train Loss: 3.1021  Validation Loss: 6.5467  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 254, train_loss: 3.136 Training correlation: 0.9881072237658611\n",
      "\n",
      " Epoch: 254  Train Loss: 3.1166  Validation Loss: 8.1860  Validation Correlation: 0.9764\n",
      "\n",
      "epoch: 255, train_loss: 2.940 Training correlation: 0.9888464220749604\n",
      "\n",
      " Epoch: 255  Train Loss: 3.3627  Validation Loss: 10.8219  Validation Correlation: 0.9772\n",
      "\n",
      "epoch: 256, train_loss: 12.348 Training correlation: 0.9898578404677874\n",
      "\n",
      " Epoch: 256  Train Loss: 4.0471  Validation Loss: 8.1811  Validation Correlation: 0.9775\n",
      "\n",
      "epoch: 257, train_loss: 8.320 Training correlation: 0.9880997587311275\n",
      "\n",
      " Epoch: 257  Train Loss: 3.6390  Validation Loss: 8.1615  Validation Correlation: 0.9764\n",
      "\n",
      "epoch: 258, train_loss: 6.026 Training correlation: 0.9927438024342827\n",
      "\n",
      " Epoch: 258  Train Loss: 3.3635  Validation Loss: 6.2541  Validation Correlation: 0.9771\n",
      "\n",
      "epoch: 259, train_loss: 3.917 Training correlation: 0.9935863018502671\n",
      "\n",
      " Epoch: 259  Train Loss: 3.4194  Validation Loss: 6.1349  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 260, train_loss: 2.287 Training correlation: 0.9925091551071893\n",
      "\n",
      " Epoch: 260  Train Loss: 2.9133  Validation Loss: 6.4577  Validation Correlation: 0.9766\n",
      "\n",
      "epoch: 261, train_loss: 2.090 Training correlation: 0.9918048918413079\n",
      "\n",
      " Epoch: 261  Train Loss: 2.8752  Validation Loss: 5.6817  Validation Correlation: 0.9782\n",
      "\n",
      "epoch: 262, train_loss: 1.990 Training correlation: 0.9938060836797423\n",
      "\n",
      " Epoch: 262  Train Loss: 2.7269  Validation Loss: 6.6120  Validation Correlation: 0.9777\n",
      "\n",
      "epoch: 263, train_loss: 3.860 Training correlation: 0.9861805979161367\n",
      "\n",
      " Epoch: 263  Train Loss: 3.0676  Validation Loss: 6.2240  Validation Correlation: 0.9776\n",
      "\n",
      "epoch: 264, train_loss: 2.680 Training correlation: 0.9881891847922594\n",
      "\n",
      " Epoch: 264  Train Loss: 3.1171  Validation Loss: 6.1998  Validation Correlation: 0.9762\n",
      "\n",
      "epoch: 265, train_loss: 2.508 Training correlation: 0.992767174892851\n",
      "\n",
      " Epoch: 265  Train Loss: 3.3602  Validation Loss: 7.7833  Validation Correlation: 0.9767\n",
      "\n",
      "epoch: 266, train_loss: 2.472 Training correlation: 0.992132062248124\n",
      "\n",
      " Epoch: 266  Train Loss: 3.5003  Validation Loss: 7.8877  Validation Correlation: 0.9757\n",
      "\n",
      "epoch: 267, train_loss: 2.593 Training correlation: 0.9906438976402071\n",
      "\n",
      " Epoch: 267  Train Loss: 3.6560  Validation Loss: 8.1679  Validation Correlation: 0.9752\n",
      "\n",
      "epoch: 268, train_loss: 7.370 Training correlation: 0.9932613419688923\n",
      "\n",
      " Epoch: 268  Train Loss: 3.3909  Validation Loss: 6.2906  Validation Correlation: 0.9758\n",
      "\n",
      "epoch: 269, train_loss: 1.776 Training correlation: 0.9944785643573107\n",
      "\n",
      " Epoch: 269  Train Loss: 3.1552  Validation Loss: 7.1836  Validation Correlation: 0.9752\n",
      "\n",
      "epoch: 270, train_loss: 2.091 Training correlation: 0.9899307009231414\n",
      "\n",
      " Epoch: 270  Train Loss: 3.2358  Validation Loss: 6.4472  Validation Correlation: 0.9758\n",
      "\n",
      "epoch: 271, train_loss: 2.969 Training correlation: 0.989340843440479\n",
      "\n",
      " Epoch: 271  Train Loss: 2.7821  Validation Loss: 6.4746  Validation Correlation: 0.9755\n",
      "\n",
      "epoch: 272, train_loss: 3.570 Training correlation: 0.9877401377586607\n",
      "\n",
      " Epoch: 272  Train Loss: 2.7372  Validation Loss: 6.5669  Validation Correlation: 0.9755\n",
      "\n",
      "epoch: 273, train_loss: 4.034 Training correlation: 0.9888480776986908\n",
      "\n",
      " Epoch: 273  Train Loss: 2.6363  Validation Loss: 7.0535  Validation Correlation: 0.9755\n",
      "\n",
      "epoch: 274, train_loss: 2.144 Training correlation: 0.9935825850712484\n",
      "\n",
      " Epoch: 274  Train Loss: 2.6835  Validation Loss: 7.6333  Validation Correlation: 0.9764\n",
      "\n",
      "epoch: 275, train_loss: 2.947 Training correlation: 0.987411803015052\n",
      "\n",
      " Epoch: 275  Train Loss: 2.8948  Validation Loss: 11.4388  Validation Correlation: 0.9768\n",
      "\n",
      "epoch: 276, train_loss: 6.408 Training correlation: 0.9858535560622596\n",
      "\n",
      " Epoch: 276  Train Loss: 2.9729  Validation Loss: 8.3185  Validation Correlation: 0.9764\n",
      "\n",
      "epoch: 277, train_loss: 2.842 Training correlation: 0.9922575161573632\n",
      "\n",
      " Epoch: 277  Train Loss: 2.9978  Validation Loss: 6.0642  Validation Correlation: 0.9765\n",
      "\n",
      "epoch: 278, train_loss: 2.507 Training correlation: 0.9928611400335442\n",
      "\n",
      " Epoch: 278  Train Loss: 2.6189  Validation Loss: 6.6596  Validation Correlation: 0.9771\n",
      "\n",
      "epoch: 279, train_loss: 2.608 Training correlation: 0.991585563653342\n",
      "\n",
      " Epoch: 279  Train Loss: 2.8074  Validation Loss: 11.1703  Validation Correlation: 0.9772\n",
      "\n",
      "epoch: 280, train_loss: 6.032 Training correlation: 0.9902660289003103\n",
      "\n",
      " Epoch: 280  Train Loss: 2.9891  Validation Loss: 8.3404  Validation Correlation: 0.9766\n",
      "\n",
      "epoch: 281, train_loss: 2.372 Training correlation: 0.9916855701146362\n",
      "\n",
      " Epoch: 281  Train Loss: 2.8555  Validation Loss: 6.0645  Validation Correlation: 0.9771\n",
      "\n",
      "epoch: 282, train_loss: 2.310 Training correlation: 0.9922741106501718\n",
      "\n",
      " Epoch: 282  Train Loss: 3.1867  Validation Loss: 6.1593  Validation Correlation: 0.9764\n",
      "\n",
      "epoch: 283, train_loss: 3.480 Training correlation: 0.9882259875364027\n",
      "\n",
      " Epoch: 283  Train Loss: 3.2793  Validation Loss: 6.8212  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 284, train_loss: 3.013 Training correlation: 0.9882775947797525\n",
      "\n",
      " Epoch: 284  Train Loss: 2.8912  Validation Loss: 6.5843  Validation Correlation: 0.9767\n",
      "\n",
      "epoch: 285, train_loss: 1.977 Training correlation: 0.9931560587185636\n",
      "\n",
      " Epoch: 285  Train Loss: 2.6545  Validation Loss: 7.1935  Validation Correlation: 0.9768\n",
      "\n",
      "epoch: 286, train_loss: 5.666 Training correlation: 0.9888844513327592\n",
      "\n",
      " Epoch: 286  Train Loss: 2.9030  Validation Loss: 6.3685  Validation Correlation: 0.9772\n",
      "\n",
      "epoch: 287, train_loss: 1.511 Training correlation: 0.9941521777225093\n",
      "\n",
      " Epoch: 287  Train Loss: 2.8151  Validation Loss: 8.2448  Validation Correlation: 0.9773\n",
      "\n",
      "epoch: 288, train_loss: 2.899 Training correlation: 0.9906706589320484\n",
      "\n",
      " Epoch: 288  Train Loss: 2.8373  Validation Loss: 6.1591  Validation Correlation: 0.9775\n",
      "\n",
      "epoch: 289, train_loss: 1.972 Training correlation: 0.9918589907041642\n",
      "\n",
      " Epoch: 289  Train Loss: 2.4721  Validation Loss: 7.5484  Validation Correlation: 0.9770\n",
      "\n",
      "epoch: 290, train_loss: 5.880 Training correlation: 0.9934540372205849\n",
      "\n",
      " Epoch: 290  Train Loss: 2.6107  Validation Loss: 6.2308  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 291, train_loss: 3.998 Training correlation: 0.9892206508275904\n",
      "\n",
      " Epoch: 291  Train Loss: 2.7544  Validation Loss: 6.3052  Validation Correlation: 0.9759\n",
      "\n",
      "epoch: 292, train_loss: 3.791 Training correlation: 0.9903557605090704\n",
      "\n",
      " Epoch: 292  Train Loss: 2.8864  Validation Loss: 5.9983  Validation Correlation: 0.9767\n",
      "\n",
      "epoch: 293, train_loss: 1.808 Training correlation: 0.993796222594749\n",
      "\n",
      " Epoch: 293  Train Loss: 2.5237  Validation Loss: 7.3342  Validation Correlation: 0.9764\n",
      "\n",
      "epoch: 294, train_loss: 2.967 Training correlation: 0.9909889693689694\n",
      "\n",
      " Epoch: 294  Train Loss: 2.7785  Validation Loss: 6.7841  Validation Correlation: 0.9758\n",
      "\n",
      "epoch: 295, train_loss: 2.219 Training correlation: 0.9944675507283289\n",
      "\n",
      " Epoch: 295  Train Loss: 2.6360  Validation Loss: 6.6469  Validation Correlation: 0.9765\n",
      "\n",
      "epoch: 296, train_loss: 2.003 Training correlation: 0.9927056055181888\n",
      "\n",
      " Epoch: 296  Train Loss: 2.8308  Validation Loss: 6.4433  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 297, train_loss: 3.472 Training correlation: 0.98812963060479\n",
      "\n",
      " Epoch: 297  Train Loss: 2.9905  Validation Loss: 6.2847  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 298, train_loss: 3.934 Training correlation: 0.989818727649242\n",
      "\n",
      " Epoch: 298  Train Loss: 2.9537  Validation Loss: 6.6649  Validation Correlation: 0.9756\n",
      "\n",
      "epoch: 299, train_loss: 2.537 Training correlation: 0.9908721451960605\n",
      "\n",
      " Epoch: 299  Train Loss: 3.1079  Validation Loss: 6.5750  Validation Correlation: 0.9769\n",
      "\n",
      "epoch: 300, train_loss: 2.101 Training correlation: 0.9908605664183714\n",
      "\n",
      " Epoch: 300  Train Loss: 2.7428  Validation Loss: 9.4041  Validation Correlation: 0.9759\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running the training\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.init(config= config, project=\"14ptms\", entity=\"alirezak2\",)\n",
    "\n",
    "model = MyNet()\n",
    "model.to(device)\n",
    "\n",
    "# Hyper parameters\n",
    "\n",
    "epochs =wandb.config[\"epochs\"]\n",
    "learning_rate = wandb.config[\"learning_rate\"]\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "\n",
    "wandb.watch(model)\n",
    "\n",
    "# Training the model\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_train = train(model, dataloader_train, loss_function )\n",
    "    loss_valid, corr, output, y = validation(model, dataloader_val, loss_function )\n",
    "\n",
    "    print('\\n Epoch: {}  Train Loss: {:.4f}  Validation Loss: {:.4f}  Validation Correlation: {:.4f}\\n'\n",
    "          .format(epoch + 1, loss_train, loss_valid, corr))\n",
    "\n",
    "\n",
    "    wandb.log({\n",
    "        \"Epoch\": epoch,\n",
    "        \"Train Loss Averaged\": loss_train,\n",
    "        \"Correlation\": corr,\n",
    "        \"Valid Loss Averaged\": loss_valid,\n",
    "        #\"Valid Acc\": acc_valid\n",
    "        })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set Loss: 2.4689  Test set Correlation: 0.9934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "loss_test, corr_test, output_test, y_test = validation(model, dataloader_test, loss_function)\n",
    "print('\\n Test set Loss: {:.4f}  Test set Correlation: {:.4f}\\n'\n",
    "      .format(loss_test, corr_test))\n",
    "output = output_test\n",
    "y = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set no mod Loss: 49.9177  Test set no mod Correlation: 0.9535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_test_no_mod, corr_tes_no_mod, output_test_no_mod, y_test_no_mod = validation(model, dataloader_test_no_mod, loss_function)\n",
    "print('\\n Test set no mod Loss: {:.4f}  Test set no mod Correlation: {:.4f}\\n'\n",
    "      .format(loss_test_no_mod, corr_tes_no_mod))\n",
    "output = output_test_no_mod\n",
    "y = y_test_no_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mod = \"acetyl\"\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax1 = fig.add_subplot(111)\n",
    "mae_test = mean_absolute_error(output_test,y_test)\n",
    "mae_no_mod = mean_absolute_error(output_test_no_mod,y_test_no_mod)\n",
    "\n",
    "ax1.scatter(y_test_no_mod,output_test_no_mod,c='r',label=mod+\n",
    "            ' not_encoded (MAE: {:.3f}, R: {:.3f})'.format(mae_no_mod, corr_tes_no_mod),s=3)\n",
    "ax1.scatter(y_test,output_test,c='b',label=mod+\n",
    "            ' encoded (MAE: {:.3f}, R: {:.3f})'.format(mae_test, corr_test),s=3)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('observed retention time')\n",
    "plt.ylabel('predicted retention time')\n",
    "plt.axis('scaled')\n",
    "ax1.plot([0, 60], [0, 60], ls=\"--\", c=\".5\")\n",
    "plt.xlim(0,60)\n",
    "plt.ylim(0,60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default data, without normalization\n",
      "Pearson Correlation: \n",
      " [[1.         0.95166141]\n",
      " [0.95166141 1.        ]]\n",
      "RSE= 0.4425666399462667\n",
      "R-Square= 0.9056594349815884\n",
      "rmse= 0.44071100644878364\n",
      "mae= 6.3081758251748825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2a5a9f2beb0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Metrics\n",
    "output_arrayed= np.array(output).reshape(-1,1)\n",
    "y_arrayed= np.array(y).reshape(-1,1)\n",
    "linear_regressor = LinearRegression()\n",
    "linear_regressor.fit(output_arrayed,y_arrayed)\n",
    "\n",
    "num_data = len(y)\n",
    "\n",
    "pear = np.corrcoef(output,y)\n",
    "mse = mean_squared_error(output,y)\n",
    "rmse = math.sqrt(mse/num_data)\n",
    "rse = math.sqrt(mse/(num_data-2))\n",
    "rsquare=linear_regressor.score(output_arrayed,y_arrayed)\n",
    "mae=mean_absolute_error(output,y)\n",
    "\n",
    "print('default data, without normalization')\n",
    "print('Pearson Correlation: \\n', pear)\n",
    "print('RSE=',rse)\n",
    "print('R-Square=',rsquare)\n",
    "print('rmse=',rmse)\n",
    "print('mae=',mae)\n",
    "plt.scatter(output,y,s=0.2,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "path = './saved_models/14ptms/300_acetyl2.pth'\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the model\n",
    "path = './saved_models/14ptms/300_acetyl2.pth'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.loss.MSELoss"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = get_config()\n",
    "type(config[\"loss_function\"]())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jri0qqqs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12872... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Correlation</td><td>▁▅▆▆▆▆▇▇▆▆▇▇▇▇█▇▇▇▇█████████████████████</td></tr><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>No mod Test set Correlation</td><td>▁██▆▇▆▇▇▆▆▆▇▇▇▇▇▇▇▆▇▆▇▇▇▆▆▆▆▆▆▇▇▇▆▆▇▇▇▆▇</td></tr><tr><td>Test set Correlation</td><td>▁▇▇▇▇▇█▇▇▆▇███████▇█████████████████████</td></tr><tr><td>Total Parameters</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Loss Averaged</td><td>█▅▄▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Valid Loss Averaged</td><td>█▆▃▄▄▄▃▃▂▃▂▂▂▂▂▁▂▂▁▂▁▁▂▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Correlation</td><td>0.97158</td></tr><tr><td>Epoch</td><td>400</td></tr><tr><td>No mod Test set Correlation</td><td>0.95129</td></tr><tr><td>Test set Correlation</td><td>0.98887</td></tr><tr><td>Total Parameters</td><td>3943077</td></tr><tr><td>Train Loss Averaged</td><td>0.98294</td></tr><tr><td>Valid Loss Averaged</td><td>42.27864</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">rich-snowball-26</strong>: <a href=\"https://wandb.ai/alirezak2/Hyper%20Parameters/runs/jri0qqqs\" target=\"_blank\">https://wandb.ai/alirezak2/Hyper%20Parameters/runs/jri0qqqs</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220329_135202-jri0qqqs\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jri0qqqs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/alirezak2/Hyper%20Parameters/runs/ly4xnuwm\" target=\"_blank\">rural-snow-27</a></strong> to <a href=\"https://wandb.ai/alirezak2/Hyper%20Parameters\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of MyNet(\n",
      "  (l): ModuleList(\n",
      "    (0): Conv1d(30, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (3): ReLU()\n",
      "    (4): Dropout(p=0.15, inplace=False)\n",
      "    (5): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.15, inplace=False)\n",
      "    (8): MaxPool1d(kernel_size=(5,), stride=(1,), padding=(2,), dilation=1, ceil_mode=False)\n",
      "    (9): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.15, inplace=False)\n",
      "    (12): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (13): ReLU()\n",
      "    (14): Dropout(p=0.15, inplace=False)\n",
      "    (15): MaxPool1d(kernel_size=(5,), stride=(1,), padding=(2,), dilation=1, ceil_mode=False)\n",
      "    (16): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (17): ReLU()\n",
      "    (18): Dropout(p=0.15, inplace=False)\n",
      "    (19): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (20): ReLU()\n",
      "    (21): Dropout(p=0.15, inplace=False)\n",
      "    (22): MaxPool1d(kernel_size=(5,), stride=(1,), padding=(2,), dilation=1, ceil_mode=False)\n",
      "    (23): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (24): ReLU()\n",
      "    (25): Dropout(p=0.15, inplace=False)\n",
      "    (26): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (27): ReLU()\n",
      "    (28): Dropout(p=0.15, inplace=False)\n",
      "    (29): MaxPool1d(kernel_size=(5,), stride=(1,), padding=(2,), dilation=1, ceil_mode=False)\n",
      "    (30): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (31): ReLU()\n",
      "    (32): Dropout(p=0.15, inplace=False)\n",
      "    (33): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (34): ReLU()\n",
      "    (35): Dropout(p=0.15, inplace=False)\n",
      "    (36): MaxPool1d(kernel_size=(5,), stride=(1,), padding=(2,), dilation=1, ceil_mode=False)\n",
      "    (37): Conv1d(100, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (38): ReLU()\n",
      "    (39): Dropout(p=0.15, inplace=False)\n",
      "    (40): MaxPool1d(kernel_size=(5,), stride=(1,), padding=(2,), dilation=1, ceil_mode=False)\n",
      "    (41): Flatten(start_dim=1, end_dim=-1)\n",
      "    (42): Linear(in_features=1664, out_features=1024, bias=True)\n",
      "    (43): ReLU()\n",
      "    (44): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (45): ReLU()\n",
      "    (46): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (47): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (48): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")>\n",
      "Total Parameters =   3943077\n",
      "torch.float32\n",
      "X is cuda: True\n",
      "X.shape: torch.Size([64, 30, 52])\n",
      "y.shape: torch.Size([64])\n",
      "outputs.shape: torch.Size([64, 1])\n",
      "epoch: 1, train_loss: 964.776 Training correlation: -0.05439678714623569\n",
      "\n",
      " Epoch: 1  Train Loss: 199.7426  Validation Loss: 520.5597  Validation Correlation: 0.7267\n",
      "\n",
      "epoch: 2, train_loss: 120.000 Training correlation: 0.7771551307510758\n",
      "\n",
      " Epoch: 2  Train Loss: 62.1765  Validation Loss: 572.7029  Validation Correlation: 0.8300\n",
      "\n",
      "epoch: 3, train_loss: 36.465 Training correlation: 0.8444552520804227\n",
      "\n",
      " Epoch: 3  Train Loss: 41.7886  Validation Loss: 554.3162  Validation Correlation: 0.8610\n",
      "\n",
      "epoch: 4, train_loss: 23.870 Training correlation: 0.8964608120491776\n",
      "\n",
      " Epoch: 4  Train Loss: 44.5713  Validation Loss: 492.1952  Validation Correlation: 0.8756\n",
      "\n",
      "epoch: 5, train_loss: 26.142 Training correlation: 0.912183486073288\n",
      "\n",
      " Epoch: 5  Train Loss: 29.7864  Validation Loss: 464.2175  Validation Correlation: 0.8887\n",
      "\n",
      "epoch: 6, train_loss: 48.570 Training correlation: 0.9006596012239106\n",
      "\n",
      " Epoch: 6  Train Loss: 29.5657  Validation Loss: 548.5860  Validation Correlation: 0.8962\n",
      "\n",
      "epoch: 7, train_loss: 34.415 Training correlation: 0.9416058437605819\n",
      "\n",
      " Epoch: 7  Train Loss: 30.1113  Validation Loss: 512.1544  Validation Correlation: 0.9054\n",
      "\n",
      "epoch: 8, train_loss: 56.714 Training correlation: 0.815730200667048\n",
      "\n",
      " Epoch: 8  Train Loss: 26.8114  Validation Loss: 453.8708  Validation Correlation: 0.9146\n",
      "\n",
      "epoch: 9, train_loss: 24.311 Training correlation: 0.9224144778967898\n",
      "\n",
      " Epoch: 9  Train Loss: 21.8341  Validation Loss: 455.5732  Validation Correlation: 0.9226\n",
      "\n",
      "epoch: 10, train_loss: 25.131 Training correlation: 0.8867488280865554\n",
      "\n",
      " Epoch: 10  Train Loss: 21.9696  Validation Loss: 473.3838  Validation Correlation: 0.9256\n",
      "\n",
      "epoch: 11, train_loss: 31.783 Training correlation: 0.9467920811885807\n",
      "\n",
      " Epoch: 11  Train Loss: 24.0799  Validation Loss: 433.2484  Validation Correlation: 0.9305\n",
      "\n",
      "epoch: 12, train_loss: 32.114 Training correlation: 0.951797534884179\n",
      "\n",
      " Epoch: 12  Train Loss: 24.9899  Validation Loss: 391.7202  Validation Correlation: 0.9308\n",
      "\n",
      "epoch: 13, train_loss: 31.282 Training correlation: 0.9533552624890045\n",
      "\n",
      " Epoch: 13  Train Loss: 20.3713  Validation Loss: 337.6127  Validation Correlation: 0.9347\n",
      "\n",
      "epoch: 14, train_loss: 9.954 Training correlation: 0.9639779049063563\n",
      "\n",
      " Epoch: 14  Train Loss: 17.5339  Validation Loss: 338.9237  Validation Correlation: 0.9365\n",
      "\n",
      "epoch: 15, train_loss: 8.047 Training correlation: 0.962519514441833\n",
      "\n",
      " Epoch: 15  Train Loss: 17.0188  Validation Loss: 343.0476  Validation Correlation: 0.9401\n",
      "\n",
      "epoch: 16, train_loss: 16.270 Training correlation: 0.9483761994021132\n",
      "\n",
      " Epoch: 16  Train Loss: 16.7694  Validation Loss: 274.8499  Validation Correlation: 0.9404\n",
      "\n",
      "epoch: 17, train_loss: 19.095 Training correlation: 0.9595553516952409\n",
      "\n",
      " Epoch: 17  Train Loss: 18.8788  Validation Loss: 268.4420  Validation Correlation: 0.9418\n",
      "\n",
      "epoch: 18, train_loss: 12.755 Training correlation: 0.9720786565270598\n",
      "\n",
      " Epoch: 18  Train Loss: 16.0018  Validation Loss: 266.9847  Validation Correlation: 0.9449\n",
      "\n",
      "epoch: 19, train_loss: 13.135 Training correlation: 0.9715989090252962\n",
      "\n",
      " Epoch: 19  Train Loss: 15.0145  Validation Loss: 250.1212  Validation Correlation: 0.9418\n",
      "\n",
      "epoch: 20, train_loss: 20.848 Training correlation: 0.9511746120139823\n",
      "\n",
      " Epoch: 20  Train Loss: 13.4716  Validation Loss: 258.6880  Validation Correlation: 0.9468\n",
      "\n",
      "epoch: 21, train_loss: 11.700 Training correlation: 0.9674803814607067\n",
      "\n",
      " Epoch: 21  Train Loss: 14.2090  Validation Loss: 275.9005  Validation Correlation: 0.9501\n",
      "\n",
      "epoch: 22, train_loss: 5.835 Training correlation: 0.9700881522159119\n",
      "\n",
      " Epoch: 22  Train Loss: 12.3011  Validation Loss: 276.5498  Validation Correlation: 0.9498\n",
      "\n",
      "epoch: 23, train_loss: 8.009 Training correlation: 0.9711603004994043\n",
      "\n",
      " Epoch: 23  Train Loss: 11.4345  Validation Loss: 321.4495  Validation Correlation: 0.9503\n",
      "\n",
      "epoch: 24, train_loss: 27.405 Training correlation: 0.9774565801992402\n",
      "\n",
      " Epoch: 24  Train Loss: 14.8009  Validation Loss: 277.4855  Validation Correlation: 0.9505\n",
      "\n",
      "epoch: 25, train_loss: 11.545 Training correlation: 0.9614419421329469\n",
      "\n",
      " Epoch: 25  Train Loss: 13.7283  Validation Loss: 298.5714  Validation Correlation: 0.9511\n",
      "\n",
      "epoch: 26, train_loss: 14.354 Training correlation: 0.9826728407534091\n",
      "\n",
      " Epoch: 26  Train Loss: 12.3633  Validation Loss: 266.8009  Validation Correlation: 0.9532\n",
      "\n",
      "epoch: 27, train_loss: 16.608 Training correlation: 0.9435898119455266\n",
      "\n",
      " Epoch: 27  Train Loss: 10.6661  Validation Loss: 238.0540  Validation Correlation: 0.9501\n",
      "\n",
      "epoch: 28, train_loss: 12.998 Training correlation: 0.9573743965219649\n",
      "\n",
      " Epoch: 28  Train Loss: 11.1025  Validation Loss: 260.4809  Validation Correlation: 0.9515\n",
      "\n",
      "epoch: 29, train_loss: 19.789 Training correlation: 0.9508766818911611\n",
      "\n",
      " Epoch: 29  Train Loss: 11.6041  Validation Loss: 220.9547  Validation Correlation: 0.9478\n",
      "\n",
      "epoch: 30, train_loss: 9.367 Training correlation: 0.9740782833333906\n",
      "\n",
      " Epoch: 30  Train Loss: 12.1291  Validation Loss: 202.9353  Validation Correlation: 0.9507\n",
      "\n",
      "epoch: 31, train_loss: 9.184 Training correlation: 0.9775404220931996\n",
      "\n",
      " Epoch: 31  Train Loss: 10.1643  Validation Loss: 258.4692  Validation Correlation: 0.9549\n",
      "\n",
      "epoch: 32, train_loss: 18.906 Training correlation: 0.9628143010236296\n",
      "\n",
      " Epoch: 32  Train Loss: 10.3044  Validation Loss: 238.7004  Validation Correlation: 0.9547\n",
      "\n",
      "epoch: 33, train_loss: 20.957 Training correlation: 0.9363374478396219\n",
      "\n",
      " Epoch: 33  Train Loss: 9.7908  Validation Loss: 253.8123  Validation Correlation: 0.9569\n",
      "\n",
      "epoch: 34, train_loss: 16.786 Training correlation: 0.9681502480830991\n",
      "\n",
      " Epoch: 34  Train Loss: 10.2143  Validation Loss: 230.8986  Validation Correlation: 0.9550\n",
      "\n",
      "epoch: 35, train_loss: 8.420 Training correlation: 0.9759777512222099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch: 35  Train Loss: 9.5109  Validation Loss: 229.9494  Validation Correlation: 0.9526\n",
      "\n",
      "epoch: 36, train_loss: 14.767 Training correlation: 0.9617120156511486\n",
      "\n",
      " Epoch: 36  Train Loss: 9.0792  Validation Loss: 196.1069  Validation Correlation: 0.9510\n",
      "\n",
      "epoch: 37, train_loss: 8.648 Training correlation: 0.9676953724010984\n",
      "\n",
      " Epoch: 37  Train Loss: 8.2891  Validation Loss: 226.4238  Validation Correlation: 0.9562\n",
      "\n",
      "epoch: 38, train_loss: 19.872 Training correlation: 0.934532203790244\n",
      "\n",
      " Epoch: 38  Train Loss: 7.9383  Validation Loss: 253.8570  Validation Correlation: 0.9573\n",
      "\n",
      "epoch: 39, train_loss: 17.009 Training correlation: 0.9817043585902671\n",
      "\n",
      " Epoch: 39  Train Loss: 9.5253  Validation Loss: 191.5226  Validation Correlation: 0.9542\n",
      "\n",
      "epoch: 40, train_loss: 5.354 Training correlation: 0.979302117598117\n",
      "\n",
      " Epoch: 40  Train Loss: 7.2303  Validation Loss: 200.6464  Validation Correlation: 0.9550\n",
      "\n",
      "epoch: 41, train_loss: 6.879 Training correlation: 0.972537999018373\n",
      "\n",
      " Epoch: 41  Train Loss: 7.0996  Validation Loss: 203.4954  Validation Correlation: 0.9575\n",
      "\n",
      "epoch: 42, train_loss: 5.105 Training correlation: 0.9796113545654594\n",
      "\n",
      " Epoch: 42  Train Loss: 7.1097  Validation Loss: 176.1520  Validation Correlation: 0.9597\n",
      "\n",
      "epoch: 43, train_loss: 14.090 Training correlation: 0.9585742920674827\n",
      "\n",
      " Epoch: 43  Train Loss: 8.2304  Validation Loss: 145.2586  Validation Correlation: 0.9534\n",
      "\n",
      "epoch: 44, train_loss: 27.145 Training correlation: 0.9835014959496597\n",
      "\n",
      " Epoch: 44  Train Loss: 9.1661  Validation Loss: 188.2483  Validation Correlation: 0.9545\n",
      "\n",
      "epoch: 45, train_loss: 5.387 Training correlation: 0.9784151056160801\n",
      "\n",
      " Epoch: 45  Train Loss: 6.9017  Validation Loss: 202.9632  Validation Correlation: 0.9572\n",
      "\n",
      "epoch: 46, train_loss: 5.111 Training correlation: 0.9818353997128129\n",
      "\n",
      " Epoch: 46  Train Loss: 6.8235  Validation Loss: 143.7954  Validation Correlation: 0.9528\n",
      "\n",
      "epoch: 47, train_loss: 25.893 Training correlation: 0.9858480072061236\n",
      "\n",
      " Epoch: 47  Train Loss: 7.6392  Validation Loss: 168.0647  Validation Correlation: 0.9582\n",
      "\n",
      "epoch: 48, train_loss: 6.370 Training correlation: 0.9758264934681439\n",
      "\n",
      " Epoch: 48  Train Loss: 7.1850  Validation Loss: 197.4072  Validation Correlation: 0.9597\n",
      "\n",
      "epoch: 49, train_loss: 9.321 Training correlation: 0.9772999198636002\n",
      "\n",
      " Epoch: 49  Train Loss: 7.0430  Validation Loss: 205.5128  Validation Correlation: 0.9585\n",
      "\n",
      "epoch: 50, train_loss: 12.523 Training correlation: 0.9853639409874766\n",
      "\n",
      " Epoch: 50  Train Loss: 8.0014  Validation Loss: 197.9110  Validation Correlation: 0.9595\n",
      "\n",
      "epoch: 51, train_loss: 11.803 Training correlation: 0.9875356042857001\n",
      "\n",
      " Epoch: 51  Train Loss: 6.6743  Validation Loss: 171.9197  Validation Correlation: 0.9607\n",
      "\n",
      "epoch: 52, train_loss: 5.923 Training correlation: 0.9778178474970799\n",
      "\n",
      " Epoch: 52  Train Loss: 6.4432  Validation Loss: 164.4722  Validation Correlation: 0.9581\n",
      "\n",
      "epoch: 53, train_loss: 5.608 Training correlation: 0.9792877428649513\n",
      "\n",
      " Epoch: 53  Train Loss: 6.2947  Validation Loss: 173.8252  Validation Correlation: 0.9572\n",
      "\n",
      "epoch: 54, train_loss: 3.879 Training correlation: 0.9855569038597206\n",
      "\n",
      " Epoch: 54  Train Loss: 6.0190  Validation Loss: 194.6962  Validation Correlation: 0.9563\n",
      "\n",
      "epoch: 55, train_loss: 8.855 Training correlation: 0.9822187136988765\n",
      "\n",
      " Epoch: 55  Train Loss: 6.8486  Validation Loss: 162.8475  Validation Correlation: 0.9616\n",
      "\n",
      "epoch: 56, train_loss: 4.985 Training correlation: 0.9824941885896763\n",
      "\n",
      " Epoch: 56  Train Loss: 5.9250  Validation Loss: 157.9770  Validation Correlation: 0.9606\n",
      "\n",
      "epoch: 57, train_loss: 4.207 Training correlation: 0.9811588160232546\n",
      "\n",
      " Epoch: 57  Train Loss: 6.3523  Validation Loss: 142.3050  Validation Correlation: 0.9623\n",
      "\n",
      "epoch: 58, train_loss: 6.496 Training correlation: 0.9809161134224245\n",
      "\n",
      " Epoch: 58  Train Loss: 5.4267  Validation Loss: 154.5247  Validation Correlation: 0.9623\n",
      "\n",
      "epoch: 59, train_loss: 4.299 Training correlation: 0.9876768103217292\n",
      "\n",
      " Epoch: 59  Train Loss: 5.3560  Validation Loss: 141.4092  Validation Correlation: 0.9606\n",
      "\n",
      "epoch: 60, train_loss: 4.656 Training correlation: 0.9921172525753903\n",
      "\n",
      " Epoch: 60  Train Loss: 5.7573  Validation Loss: 212.3393  Validation Correlation: 0.9609\n",
      "\n",
      "epoch: 61, train_loss: 35.669 Training correlation: 0.9716300566485536\n",
      "\n",
      " Epoch: 61  Train Loss: 7.7099  Validation Loss: 188.2799  Validation Correlation: 0.9636\n",
      "\n",
      "epoch: 62, train_loss: 16.785 Training correlation: 0.979307666731202\n",
      "\n",
      " Epoch: 62  Train Loss: 7.9353  Validation Loss: 149.4142  Validation Correlation: 0.9628\n",
      "\n",
      "epoch: 63, train_loss: 3.998 Training correlation: 0.9857796429155169\n",
      "\n",
      " Epoch: 63  Train Loss: 5.0946  Validation Loss: 150.0589  Validation Correlation: 0.9605\n",
      "\n",
      "epoch: 64, train_loss: 5.972 Training correlation: 0.9826450261423384\n",
      "\n",
      " Epoch: 64  Train Loss: 4.7133  Validation Loss: 123.9277  Validation Correlation: 0.9647\n",
      "\n",
      "epoch: 65, train_loss: 8.711 Training correlation: 0.9818881211266431\n",
      "\n",
      " Epoch: 65  Train Loss: 5.4416  Validation Loss: 165.3058  Validation Correlation: 0.9617\n",
      "\n",
      "epoch: 66, train_loss: 5.860 Training correlation: 0.98659918852392\n",
      "\n",
      " Epoch: 66  Train Loss: 4.7323  Validation Loss: 132.1152  Validation Correlation: 0.9635\n",
      "\n",
      "epoch: 67, train_loss: 4.281 Training correlation: 0.9892689497395792\n",
      "\n",
      " Epoch: 67  Train Loss: 4.7031  Validation Loss: 112.7711  Validation Correlation: 0.9555\n",
      "\n",
      "epoch: 68, train_loss: 13.694 Training correlation: 0.9891036023666028\n",
      "\n",
      " Epoch: 68  Train Loss: 5.8207  Validation Loss: 129.8233  Validation Correlation: 0.9626\n",
      "\n",
      "epoch: 69, train_loss: 4.006 Training correlation: 0.9879164837756419\n",
      "\n",
      " Epoch: 69  Train Loss: 4.5417  Validation Loss: 137.4028  Validation Correlation: 0.9634\n",
      "\n",
      "epoch: 70, train_loss: 5.704 Training correlation: 0.9829303687258347\n",
      "\n",
      " Epoch: 70  Train Loss: 4.7696  Validation Loss: 112.3011  Validation Correlation: 0.9617\n",
      "\n",
      "epoch: 71, train_loss: 9.338 Training correlation: 0.9804496855870138\n",
      "\n",
      " Epoch: 71  Train Loss: 5.2627  Validation Loss: 161.6723  Validation Correlation: 0.9618\n",
      "\n",
      "epoch: 72, train_loss: 11.389 Training correlation: 0.9889446105503563\n",
      "\n",
      " Epoch: 72  Train Loss: 4.4459  Validation Loss: 133.9252  Validation Correlation: 0.9615\n",
      "\n",
      "epoch: 73, train_loss: 3.577 Training correlation: 0.9877619151554634\n",
      "\n",
      " Epoch: 73  Train Loss: 4.3675  Validation Loss: 151.4438  Validation Correlation: 0.9652\n",
      "\n",
      "epoch: 74, train_loss: 5.024 Training correlation: 0.986929439620616\n",
      "\n",
      " Epoch: 74  Train Loss: 4.3945  Validation Loss: 135.2476  Validation Correlation: 0.9600\n",
      "\n",
      "epoch: 75, train_loss: 2.711 Training correlation: 0.9922739946383118\n",
      "\n",
      " Epoch: 75  Train Loss: 3.7796  Validation Loss: 86.7746  Validation Correlation: 0.9645\n",
      "\n",
      "epoch: 76, train_loss: 24.489 Training correlation: 0.9854457317090591\n",
      "\n",
      " Epoch: 76  Train Loss: 6.3554  Validation Loss: 137.8943  Validation Correlation: 0.9623\n",
      "\n",
      "epoch: 77, train_loss: 2.876 Training correlation: 0.9932516233395778\n",
      "\n",
      " Epoch: 77  Train Loss: 4.9815  Validation Loss: 140.9050  Validation Correlation: 0.9644\n",
      "\n",
      "epoch: 78, train_loss: 3.307 Training correlation: 0.9911886810899931\n",
      "\n",
      " Epoch: 78  Train Loss: 5.6736  Validation Loss: 120.5159  Validation Correlation: 0.9632\n",
      "\n",
      "epoch: 79, train_loss: 4.863 Training correlation: 0.9860307771440908\n",
      "\n",
      " Epoch: 79  Train Loss: 3.6576  Validation Loss: 101.7836  Validation Correlation: 0.9659\n",
      "\n",
      "epoch: 80, train_loss: 8.539 Training correlation: 0.9872973100088885\n",
      "\n",
      " Epoch: 80  Train Loss: 4.0893  Validation Loss: 128.9875  Validation Correlation: 0.9648\n",
      "\n",
      "epoch: 81, train_loss: 2.054 Training correlation: 0.993423004828802\n",
      "\n",
      " Epoch: 81  Train Loss: 3.5084  Validation Loss: 121.7916  Validation Correlation: 0.9630\n",
      "\n",
      "epoch: 82, train_loss: 3.264 Training correlation: 0.9865733560742087\n",
      "\n",
      " Epoch: 82  Train Loss: 3.7611  Validation Loss: 133.9350  Validation Correlation: 0.9649\n",
      "\n",
      "epoch: 83, train_loss: 3.376 Training correlation: 0.9904889327195396\n",
      "\n",
      " Epoch: 83  Train Loss: 4.3809  Validation Loss: 117.3047  Validation Correlation: 0.9625\n",
      "\n",
      "epoch: 84, train_loss: 4.197 Training correlation: 0.9837010316894359\n",
      "\n",
      " Epoch: 84  Train Loss: 3.5861  Validation Loss: 128.4023  Validation Correlation: 0.9660\n",
      "\n",
      "epoch: 85, train_loss: 2.684 Training correlation: 0.9887618987837041\n",
      "\n",
      " Epoch: 85  Train Loss: 3.5897  Validation Loss: 145.2827  Validation Correlation: 0.9665\n",
      "\n",
      "epoch: 86, train_loss: 5.508 Training correlation: 0.9886788579313037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch: 86  Train Loss: 4.6564  Validation Loss: 99.2613  Validation Correlation: 0.9658\n",
      "\n",
      "epoch: 87, train_loss: 14.628 Training correlation: 0.99057106042267\n",
      "\n",
      " Epoch: 87  Train Loss: 5.1494  Validation Loss: 138.2736  Validation Correlation: 0.9614\n",
      "\n",
      "epoch: 88, train_loss: 4.699 Training correlation: 0.983903436004073\n",
      "\n",
      " Epoch: 88  Train Loss: 4.7768  Validation Loss: 124.1355  Validation Correlation: 0.9664\n",
      "\n",
      "epoch: 89, train_loss: 3.568 Training correlation: 0.984974075784924\n",
      "\n",
      " Epoch: 89  Train Loss: 4.5622  Validation Loss: 95.2543  Validation Correlation: 0.9635\n",
      "\n",
      "epoch: 90, train_loss: 6.722 Training correlation: 0.992289275887873\n",
      "\n",
      " Epoch: 90  Train Loss: 3.4100  Validation Loss: 104.0640  Validation Correlation: 0.9654\n",
      "\n",
      "epoch: 91, train_loss: 6.594 Training correlation: 0.9899498585965285\n",
      "\n",
      " Epoch: 91  Train Loss: 3.8763  Validation Loss: 109.4785  Validation Correlation: 0.9637\n",
      "\n",
      "epoch: 92, train_loss: 6.953 Training correlation: 0.9893509916152531\n",
      "\n",
      " Epoch: 92  Train Loss: 3.7310  Validation Loss: 120.5024  Validation Correlation: 0.9640\n",
      "\n",
      "epoch: 93, train_loss: 3.047 Training correlation: 0.9906772398688836\n",
      "\n",
      " Epoch: 93  Train Loss: 3.1318  Validation Loss: 131.2028  Validation Correlation: 0.9675\n",
      "\n",
      "epoch: 94, train_loss: 3.705 Training correlation: 0.9906999840338818\n",
      "\n",
      " Epoch: 94  Train Loss: 3.1306  Validation Loss: 114.8799  Validation Correlation: 0.9663\n",
      "\n",
      "epoch: 95, train_loss: 2.455 Training correlation: 0.9932442052284469\n",
      "\n",
      " Epoch: 95  Train Loss: 3.1652  Validation Loss: 140.5218  Validation Correlation: 0.9684\n",
      "\n",
      "epoch: 96, train_loss: 5.054 Training correlation: 0.9946698630814603\n",
      "\n",
      " Epoch: 96  Train Loss: 3.1040  Validation Loss: 117.7778  Validation Correlation: 0.9682\n",
      "\n",
      "epoch: 97, train_loss: 2.295 Training correlation: 0.9902973997746952\n",
      "\n",
      " Epoch: 97  Train Loss: 3.3599  Validation Loss: 103.1962  Validation Correlation: 0.9647\n",
      "\n",
      "epoch: 98, train_loss: 3.337 Training correlation: 0.9907029825297117\n",
      "\n",
      " Epoch: 98  Train Loss: 4.7058  Validation Loss: 123.0366  Validation Correlation: 0.9666\n",
      "\n",
      "epoch: 99, train_loss: 3.462 Training correlation: 0.9912873477058184\n",
      "\n",
      " Epoch: 99  Train Loss: 3.2952  Validation Loss: 109.3063  Validation Correlation: 0.9683\n",
      "\n",
      "epoch: 100, train_loss: 2.830 Training correlation: 0.990736081978932\n",
      "\n",
      " Epoch: 100  Train Loss: 2.8249  Validation Loss: 106.1029  Validation Correlation: 0.9670\n",
      "\n",
      "epoch: 101, train_loss: 4.126 Training correlation: 0.9853535925206842\n",
      "\n",
      " Epoch: 101  Train Loss: 2.9996  Validation Loss: 120.3315  Validation Correlation: 0.9676\n",
      "\n",
      "epoch: 102, train_loss: 1.740 Training correlation: 0.9955101742324052\n",
      "\n",
      " Epoch: 102  Train Loss: 3.5651  Validation Loss: 118.5353  Validation Correlation: 0.9648\n",
      "\n",
      "epoch: 103, train_loss: 1.927 Training correlation: 0.9907395765113758\n",
      "\n",
      " Epoch: 103  Train Loss: 3.7208  Validation Loss: 122.9346  Validation Correlation: 0.9655\n",
      "\n",
      "epoch: 104, train_loss: 2.580 Training correlation: 0.9910055431818167\n",
      "\n",
      " Epoch: 104  Train Loss: 3.2332  Validation Loss: 112.5355  Validation Correlation: 0.9664\n",
      "\n",
      "epoch: 105, train_loss: 1.823 Training correlation: 0.9914989681349087\n",
      "\n",
      " Epoch: 105  Train Loss: 2.7591  Validation Loss: 105.2407  Validation Correlation: 0.9669\n",
      "\n",
      "epoch: 106, train_loss: 2.545 Training correlation: 0.9923442606273998\n",
      "\n",
      " Epoch: 106  Train Loss: 3.1953  Validation Loss: 103.1436  Validation Correlation: 0.9686\n",
      "\n",
      "epoch: 107, train_loss: 2.640 Training correlation: 0.990082593406226\n",
      "\n",
      " Epoch: 107  Train Loss: 3.3456  Validation Loss: 131.0983  Validation Correlation: 0.9681\n",
      "\n",
      "epoch: 108, train_loss: 7.338 Training correlation: 0.988417870820241\n",
      "\n",
      " Epoch: 108  Train Loss: 3.0826  Validation Loss: 85.1974  Validation Correlation: 0.9704\n",
      "\n",
      "epoch: 109, train_loss: 9.031 Training correlation: 0.9869560314943735\n",
      "\n",
      " Epoch: 109  Train Loss: 3.1245  Validation Loss: 108.4624  Validation Correlation: 0.9716\n",
      "\n",
      "epoch: 110, train_loss: 1.540 Training correlation: 0.9953083028022668\n",
      "\n",
      " Epoch: 110  Train Loss: 2.4855  Validation Loss: 120.8735  Validation Correlation: 0.9703\n",
      "\n",
      "epoch: 111, train_loss: 4.796 Training correlation: 0.9869647414776483\n",
      "\n",
      " Epoch: 111  Train Loss: 2.7105  Validation Loss: 119.9225  Validation Correlation: 0.9704\n",
      "\n",
      "epoch: 112, train_loss: 2.885 Training correlation: 0.9933785209852937\n",
      "\n",
      " Epoch: 112  Train Loss: 2.9695  Validation Loss: 126.2357  Validation Correlation: 0.9683\n",
      "\n",
      "epoch: 113, train_loss: 6.515 Training correlation: 0.9893674657299985\n",
      "\n",
      " Epoch: 113  Train Loss: 2.8853  Validation Loss: 99.0037  Validation Correlation: 0.9720\n",
      "\n",
      "epoch: 114, train_loss: 3.259 Training correlation: 0.9900359668259351\n",
      "\n",
      " Epoch: 114  Train Loss: 4.5915  Validation Loss: 108.8805  Validation Correlation: 0.9720\n",
      "\n",
      "epoch: 115, train_loss: 1.629 Training correlation: 0.9942648154838621\n",
      "\n",
      " Epoch: 115  Train Loss: 3.3218  Validation Loss: 107.1456  Validation Correlation: 0.9676\n",
      "\n",
      "epoch: 116, train_loss: 1.285 Training correlation: 0.9943427130778338\n",
      "\n",
      " Epoch: 116  Train Loss: 3.3340  Validation Loss: 95.7840  Validation Correlation: 0.9683\n",
      "\n",
      "epoch: 117, train_loss: 3.318 Training correlation: 0.9908734742111808\n",
      "\n",
      " Epoch: 117  Train Loss: 3.2889  Validation Loss: 89.4799  Validation Correlation: 0.9715\n",
      "\n",
      "epoch: 118, train_loss: 4.325 Training correlation: 0.9935471883334611\n",
      "\n",
      " Epoch: 118  Train Loss: 2.8253  Validation Loss: 96.0619  Validation Correlation: 0.9711\n",
      "\n",
      "epoch: 119, train_loss: 2.670 Training correlation: 0.9888036962271889\n",
      "\n",
      " Epoch: 119  Train Loss: 2.4948  Validation Loss: 135.6956  Validation Correlation: 0.9706\n",
      "\n",
      "epoch: 120, train_loss: 11.360 Training correlation: 0.9846503453657043\n",
      "\n",
      " Epoch: 120  Train Loss: 3.4641  Validation Loss: 107.8583  Validation Correlation: 0.9716\n",
      "\n",
      "epoch: 121, train_loss: 1.751 Training correlation: 0.9940944792368318\n",
      "\n",
      " Epoch: 121  Train Loss: 2.6261  Validation Loss: 84.0198  Validation Correlation: 0.9654\n",
      "\n",
      "epoch: 122, train_loss: 9.651 Training correlation: 0.99248903931987\n",
      "\n",
      " Epoch: 122  Train Loss: 3.6836  Validation Loss: 87.6633  Validation Correlation: 0.9669\n",
      "\n",
      "epoch: 123, train_loss: 4.179 Training correlation: 0.9926913669291908\n",
      "\n",
      " Epoch: 123  Train Loss: 2.6497  Validation Loss: 93.6790  Validation Correlation: 0.9712\n",
      "\n",
      "epoch: 124, train_loss: 3.346 Training correlation: 0.9887553753872598\n",
      "\n",
      " Epoch: 124  Train Loss: 2.9956  Validation Loss: 94.5539  Validation Correlation: 0.9696\n",
      "\n",
      "epoch: 125, train_loss: 1.923 Training correlation: 0.9932683155533395\n",
      "\n",
      " Epoch: 125  Train Loss: 2.8321  Validation Loss: 107.7038  Validation Correlation: 0.9701\n",
      "\n",
      "epoch: 126, train_loss: 2.749 Training correlation: 0.9890607961576572\n",
      "\n",
      " Epoch: 126  Train Loss: 3.0486  Validation Loss: 100.2055  Validation Correlation: 0.9717\n",
      "\n",
      "epoch: 127, train_loss: 1.512 Training correlation: 0.9939221590716927\n",
      "\n",
      " Epoch: 127  Train Loss: 2.4957  Validation Loss: 112.5236  Validation Correlation: 0.9722\n",
      "\n",
      "epoch: 128, train_loss: 2.776 Training correlation: 0.9942450470107967\n",
      "\n",
      " Epoch: 128  Train Loss: 3.4388  Validation Loss: 60.0181  Validation Correlation: 0.9717\n",
      "\n",
      "epoch: 129, train_loss: 23.573 Training correlation: 0.9930102243960106\n",
      "\n",
      " Epoch: 129  Train Loss: 5.7015  Validation Loss: 118.0250  Validation Correlation: 0.9693\n",
      "\n",
      "epoch: 130, train_loss: 6.773 Training correlation: 0.9924197731720263\n",
      "\n",
      " Epoch: 130  Train Loss: 2.7041  Validation Loss: 100.4505  Validation Correlation: 0.9737\n",
      "\n",
      "epoch: 131, train_loss: 2.563 Training correlation: 0.9917524357490447\n",
      "\n",
      " Epoch: 131  Train Loss: 2.0753  Validation Loss: 87.0108  Validation Correlation: 0.9712\n",
      "\n",
      "epoch: 132, train_loss: 3.723 Training correlation: 0.9895636518080309\n",
      "\n",
      " Epoch: 132  Train Loss: 2.3855  Validation Loss: 93.8048  Validation Correlation: 0.9705\n",
      "\n",
      "epoch: 133, train_loss: 1.899 Training correlation: 0.9932982598559974\n",
      "\n",
      " Epoch: 133  Train Loss: 2.4942  Validation Loss: 86.5694  Validation Correlation: 0.9728\n",
      "\n",
      "epoch: 134, train_loss: 2.331 Training correlation: 0.9926104402162461\n",
      "\n",
      " Epoch: 134  Train Loss: 2.2251  Validation Loss: 86.8254  Validation Correlation: 0.9732\n",
      "\n",
      "epoch: 135, train_loss: 2.088 Training correlation: 0.994760836722075\n",
      "\n",
      " Epoch: 135  Train Loss: 2.0244  Validation Loss: 95.3472  Validation Correlation: 0.9726\n",
      "\n",
      "epoch: 136, train_loss: 1.793 Training correlation: 0.9904825039664409\n",
      "\n",
      " Epoch: 136  Train Loss: 2.3730  Validation Loss: 72.9052  Validation Correlation: 0.9724\n",
      "\n",
      "epoch: 137, train_loss: 8.045 Training correlation: 0.9954156258387448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch: 137  Train Loss: 2.7013  Validation Loss: 81.9489  Validation Correlation: 0.9717\n",
      "\n",
      "epoch: 138, train_loss: 3.241 Training correlation: 0.9962489064677895\n",
      "\n",
      " Epoch: 138  Train Loss: 2.8621  Validation Loss: 75.6588  Validation Correlation: 0.9718\n",
      "\n",
      "epoch: 139, train_loss: 5.574 Training correlation: 0.9921981158577179\n",
      "\n",
      " Epoch: 139  Train Loss: 2.2877  Validation Loss: 90.6491  Validation Correlation: 0.9726\n",
      "\n",
      "epoch: 140, train_loss: 1.566 Training correlation: 0.9957469542007278\n",
      "\n",
      " Epoch: 140  Train Loss: 1.8493  Validation Loss: 95.3125  Validation Correlation: 0.9700\n",
      "\n",
      "epoch: 141, train_loss: 1.656 Training correlation: 0.9939380398271448\n",
      "\n",
      " Epoch: 141  Train Loss: 2.1959  Validation Loss: 84.5687  Validation Correlation: 0.9703\n",
      "\n",
      "epoch: 142, train_loss: 3.223 Training correlation: 0.9925733888227584\n",
      "\n",
      " Epoch: 142  Train Loss: 2.6206  Validation Loss: 89.9503  Validation Correlation: 0.9746\n",
      "\n",
      "epoch: 143, train_loss: 2.436 Training correlation: 0.994752048948668\n",
      "\n",
      " Epoch: 143  Train Loss: 2.7466  Validation Loss: 81.3113  Validation Correlation: 0.9731\n",
      "\n",
      "epoch: 144, train_loss: 3.904 Training correlation: 0.9899619443417706\n",
      "\n",
      " Epoch: 144  Train Loss: 2.5460  Validation Loss: 100.2097  Validation Correlation: 0.9707\n",
      "\n",
      "epoch: 145, train_loss: 3.869 Training correlation: 0.991174474367515\n",
      "\n",
      " Epoch: 145  Train Loss: 3.0942  Validation Loss: 90.8122  Validation Correlation: 0.9727\n",
      "\n",
      "epoch: 146, train_loss: 2.710 Training correlation: 0.9882518856905294\n",
      "\n",
      " Epoch: 146  Train Loss: 2.1551  Validation Loss: 73.9899  Validation Correlation: 0.9749\n",
      "\n",
      "epoch: 147, train_loss: 7.852 Training correlation: 0.9958603424373113\n",
      "\n",
      " Epoch: 147  Train Loss: 5.2969  Validation Loss: 104.0652  Validation Correlation: 0.9703\n",
      "\n",
      "epoch: 148, train_loss: 4.532 Training correlation: 0.9891688241165031\n",
      "\n",
      " Epoch: 148  Train Loss: 2.8230  Validation Loss: 109.7264  Validation Correlation: 0.9714\n",
      "\n",
      "epoch: 149, train_loss: 3.328 Training correlation: 0.9917401879195001\n",
      "\n",
      " Epoch: 149  Train Loss: 2.8611  Validation Loss: 101.3654  Validation Correlation: 0.9717\n",
      "\n",
      "epoch: 150, train_loss: 1.963 Training correlation: 0.9893973532392168\n",
      "\n",
      " Epoch: 150  Train Loss: 2.1956  Validation Loss: 89.8343  Validation Correlation: 0.9713\n",
      "\n",
      "epoch: 151, train_loss: 2.413 Training correlation: 0.9916619136060673\n",
      "\n",
      " Epoch: 151  Train Loss: 2.5417  Validation Loss: 102.5273  Validation Correlation: 0.9724\n",
      "\n",
      "epoch: 152, train_loss: 2.104 Training correlation: 0.996295753147058\n",
      "\n",
      " Epoch: 152  Train Loss: 2.2804  Validation Loss: 85.5124  Validation Correlation: 0.9708\n",
      "\n",
      "epoch: 153, train_loss: 3.207 Training correlation: 0.9844090573200357\n",
      "\n",
      " Epoch: 153  Train Loss: 2.0603  Validation Loss: 98.3532  Validation Correlation: 0.9718\n",
      "\n",
      "epoch: 154, train_loss: 1.575 Training correlation: 0.9961022800865951\n",
      "\n",
      " Epoch: 154  Train Loss: 1.9581  Validation Loss: 78.1433  Validation Correlation: 0.9726\n",
      "\n",
      "epoch: 155, train_loss: 4.298 Training correlation: 0.9910254385390537\n",
      "\n",
      " Epoch: 155  Train Loss: 1.9619  Validation Loss: 92.9210  Validation Correlation: 0.9710\n",
      "\n",
      "epoch: 156, train_loss: 1.713 Training correlation: 0.9945251682481563\n",
      "\n",
      " Epoch: 156  Train Loss: 1.9836  Validation Loss: 89.9305  Validation Correlation: 0.9715\n",
      "\n",
      "epoch: 157, train_loss: 1.293 Training correlation: 0.99474793563441\n",
      "\n",
      " Epoch: 157  Train Loss: 2.1346  Validation Loss: 93.1406  Validation Correlation: 0.9720\n",
      "\n",
      "epoch: 158, train_loss: 1.095 Training correlation: 0.9964656691970478\n",
      "\n",
      " Epoch: 158  Train Loss: 2.2411  Validation Loss: 99.8326  Validation Correlation: 0.9731\n",
      "\n",
      "epoch: 159, train_loss: 1.950 Training correlation: 0.9926777873071595\n",
      "\n",
      " Epoch: 159  Train Loss: 2.1113  Validation Loss: 92.4660  Validation Correlation: 0.9714\n",
      "\n",
      "epoch: 160, train_loss: 1.283 Training correlation: 0.9937767344598\n",
      "\n",
      " Epoch: 160  Train Loss: 2.0767  Validation Loss: 71.6636  Validation Correlation: 0.9720\n",
      "\n",
      "epoch: 161, train_loss: 4.759 Training correlation: 0.9927440864869511\n",
      "\n",
      " Epoch: 161  Train Loss: 2.0626  Validation Loss: 88.0895  Validation Correlation: 0.9750\n",
      "\n",
      "epoch: 162, train_loss: 1.013 Training correlation: 0.9969917708149492\n",
      "\n",
      " Epoch: 162  Train Loss: 2.0671  Validation Loss: 107.0634  Validation Correlation: 0.9726\n",
      "\n",
      "epoch: 163, train_loss: 4.412 Training correlation: 0.9933278420987515\n",
      "\n",
      " Epoch: 163  Train Loss: 2.0705  Validation Loss: 113.9350  Validation Correlation: 0.9738\n",
      "\n",
      "epoch: 164, train_loss: 8.588 Training correlation: 0.99300418205483\n",
      "\n",
      " Epoch: 164  Train Loss: 2.2416  Validation Loss: 88.5650  Validation Correlation: 0.9714\n",
      "\n",
      "epoch: 165, train_loss: 2.038 Training correlation: 0.9931954839487307\n",
      "\n",
      " Epoch: 165  Train Loss: 1.7510  Validation Loss: 89.6396  Validation Correlation: 0.9738\n",
      "\n",
      "epoch: 166, train_loss: 1.269 Training correlation: 0.9950440951381765\n",
      "\n",
      " Epoch: 166  Train Loss: 1.9132  Validation Loss: 78.1263  Validation Correlation: 0.9733\n",
      "\n",
      "epoch: 167, train_loss: 1.398 Training correlation: 0.9959567849122731\n",
      "\n",
      " Epoch: 167  Train Loss: 2.4394  Validation Loss: 110.8691  Validation Correlation: 0.9753\n",
      "\n",
      "epoch: 168, train_loss: 6.479 Training correlation: 0.9935903878976873\n",
      "\n",
      " Epoch: 168  Train Loss: 2.2662  Validation Loss: 91.9230  Validation Correlation: 0.9742\n",
      "\n",
      "epoch: 169, train_loss: 2.241 Training correlation: 0.9944591574235528\n",
      "\n",
      " Epoch: 169  Train Loss: 2.6523  Validation Loss: 65.8599  Validation Correlation: 0.9724\n",
      "\n",
      "epoch: 170, train_loss: 5.732 Training correlation: 0.9915916850599802\n",
      "\n",
      " Epoch: 170  Train Loss: 2.2909  Validation Loss: 71.5904  Validation Correlation: 0.9735\n",
      "\n",
      "epoch: 171, train_loss: 3.378 Training correlation: 0.9954274269861292\n",
      "\n",
      " Epoch: 171  Train Loss: 2.5136  Validation Loss: 83.4252  Validation Correlation: 0.9747\n",
      "\n",
      "epoch: 172, train_loss: 2.355 Training correlation: 0.9882742342122836\n",
      "\n",
      " Epoch: 172  Train Loss: 2.3089  Validation Loss: 95.6976  Validation Correlation: 0.9745\n",
      "\n",
      "epoch: 173, train_loss: 2.694 Training correlation: 0.9951366816326398\n",
      "\n",
      " Epoch: 173  Train Loss: 2.5728  Validation Loss: 108.9409  Validation Correlation: 0.9744\n",
      "\n",
      "epoch: 174, train_loss: 6.181 Training correlation: 0.9947267691194086\n",
      "\n",
      " Epoch: 174  Train Loss: 2.7717  Validation Loss: 80.4806  Validation Correlation: 0.9730\n",
      "\n",
      "epoch: 175, train_loss: 1.509 Training correlation: 0.9953128670552734\n",
      "\n",
      " Epoch: 175  Train Loss: 1.9653  Validation Loss: 90.6480  Validation Correlation: 0.9733\n",
      "\n",
      "epoch: 176, train_loss: 1.974 Training correlation: 0.993006904023302\n",
      "\n",
      " Epoch: 176  Train Loss: 1.6284  Validation Loss: 77.5011  Validation Correlation: 0.9744\n",
      "\n",
      "epoch: 177, train_loss: 1.358 Training correlation: 0.9971809634471366\n",
      "\n",
      " Epoch: 177  Train Loss: 1.6350  Validation Loss: 86.5471  Validation Correlation: 0.9738\n",
      "\n",
      "epoch: 178, train_loss: 1.116 Training correlation: 0.995153797099923\n",
      "\n",
      " Epoch: 178  Train Loss: 1.9754  Validation Loss: 102.0336  Validation Correlation: 0.9732\n",
      "\n",
      "epoch: 179, train_loss: 5.223 Training correlation: 0.9952755523400747\n",
      "\n",
      " Epoch: 179  Train Loss: 2.4085  Validation Loss: 80.1674  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 180, train_loss: 1.364 Training correlation: 0.9958494839100106\n",
      "\n",
      " Epoch: 180  Train Loss: 1.8206  Validation Loss: 97.0477  Validation Correlation: 0.9758\n",
      "\n",
      "epoch: 181, train_loss: 3.132 Training correlation: 0.9961406469761768\n",
      "\n",
      " Epoch: 181  Train Loss: 2.0942  Validation Loss: 93.3713  Validation Correlation: 0.9745\n",
      "\n",
      "epoch: 182, train_loss: 2.631 Training correlation: 0.9965338112193524\n",
      "\n",
      " Epoch: 182  Train Loss: 2.2842  Validation Loss: 84.7422  Validation Correlation: 0.9740\n",
      "\n",
      "epoch: 183, train_loss: 1.598 Training correlation: 0.9954566317547419\n",
      "\n",
      " Epoch: 183  Train Loss: 2.2070  Validation Loss: 85.5706  Validation Correlation: 0.9722\n",
      "\n",
      "epoch: 184, train_loss: 1.562 Training correlation: 0.9944891860728257\n",
      "\n",
      " Epoch: 184  Train Loss: 1.5046  Validation Loss: 84.5392  Validation Correlation: 0.9729\n",
      "\n",
      "epoch: 185, train_loss: 1.586 Training correlation: 0.9960772618463329\n",
      "\n",
      " Epoch: 185  Train Loss: 1.4841  Validation Loss: 71.2983  Validation Correlation: 0.9753\n",
      "\n",
      "epoch: 186, train_loss: 2.536 Training correlation: 0.9950738559471395\n",
      "\n",
      " Epoch: 186  Train Loss: 1.7609  Validation Loss: 85.9675  Validation Correlation: 0.9744\n",
      "\n",
      "epoch: 187, train_loss: 2.439 Training correlation: 0.9933418005087888\n",
      "\n",
      " Epoch: 187  Train Loss: 1.6987  Validation Loss: 81.2285  Validation Correlation: 0.9744\n",
      "\n",
      "epoch: 188, train_loss: 1.410 Training correlation: 0.9946612908195626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch: 188  Train Loss: 1.6759  Validation Loss: 56.3248  Validation Correlation: 0.9746\n",
      "\n",
      "epoch: 189, train_loss: 8.137 Training correlation: 0.9954976931240619\n",
      "\n",
      " Epoch: 189  Train Loss: 2.0933  Validation Loss: 92.2208  Validation Correlation: 0.9747\n",
      "\n",
      "epoch: 190, train_loss: 3.039 Training correlation: 0.9948410332278169\n",
      "\n",
      " Epoch: 190  Train Loss: 1.6627  Validation Loss: 87.5297  Validation Correlation: 0.9735\n",
      "\n",
      "epoch: 191, train_loss: 1.462 Training correlation: 0.9947516087442471\n",
      "\n",
      " Epoch: 191  Train Loss: 1.8960  Validation Loss: 78.1049  Validation Correlation: 0.9745\n",
      "\n",
      "epoch: 192, train_loss: 1.180 Training correlation: 0.9962985329067432\n",
      "\n",
      " Epoch: 192  Train Loss: 2.1578  Validation Loss: 100.7660  Validation Correlation: 0.9730\n",
      "\n",
      "epoch: 193, train_loss: 6.314 Training correlation: 0.9958457054369397\n",
      "\n",
      " Epoch: 193  Train Loss: 2.5252  Validation Loss: 76.2604  Validation Correlation: 0.9755\n",
      "\n",
      "epoch: 194, train_loss: 1.846 Training correlation: 0.995898450779482\n",
      "\n",
      " Epoch: 194  Train Loss: 2.0780  Validation Loss: 94.7063  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 195, train_loss: 3.007 Training correlation: 0.9940875332427701\n",
      "\n",
      " Epoch: 195  Train Loss: 1.5725  Validation Loss: 93.9521  Validation Correlation: 0.9749\n",
      "\n",
      "epoch: 196, train_loss: 3.363 Training correlation: 0.9938886656815009\n",
      "\n",
      " Epoch: 196  Train Loss: 2.4934  Validation Loss: 72.6659  Validation Correlation: 0.9737\n",
      "\n",
      "epoch: 197, train_loss: 1.731 Training correlation: 0.9932771412355487\n",
      "\n",
      " Epoch: 197  Train Loss: 2.1974  Validation Loss: 66.7219  Validation Correlation: 0.9732\n",
      "\n",
      "epoch: 198, train_loss: 2.064 Training correlation: 0.9955894924104017\n",
      "\n",
      " Epoch: 198  Train Loss: 1.7345  Validation Loss: 82.3032  Validation Correlation: 0.9764\n",
      "\n",
      "epoch: 199, train_loss: 1.273 Training correlation: 0.9943477709676611\n",
      "\n",
      " Epoch: 199  Train Loss: 1.6576  Validation Loss: 82.5063  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 200, train_loss: 1.121 Training correlation: 0.996772730091324\n",
      "\n",
      " Epoch: 200  Train Loss: 1.4737  Validation Loss: 91.8921  Validation Correlation: 0.9754\n",
      "\n",
      "epoch: 201, train_loss: 3.405 Training correlation: 0.9949107715949143\n",
      "\n",
      " Epoch: 201  Train Loss: 2.0849  Validation Loss: 66.1984  Validation Correlation: 0.9745\n",
      "\n",
      "epoch: 202, train_loss: 2.439 Training correlation: 0.9937840805110553\n",
      "\n",
      " Epoch: 202  Train Loss: 1.8233  Validation Loss: 85.4489  Validation Correlation: 0.9748\n",
      "\n",
      "epoch: 203, train_loss: 1.839 Training correlation: 0.9961550801753873\n",
      "\n",
      " Epoch: 203  Train Loss: 2.1642  Validation Loss: 67.0847  Validation Correlation: 0.9740\n",
      "\n",
      "epoch: 204, train_loss: 1.498 Training correlation: 0.9966133581993344\n",
      "\n",
      " Epoch: 204  Train Loss: 1.7672  Validation Loss: 104.2491  Validation Correlation: 0.9753\n",
      "\n",
      "epoch: 205, train_loss: 6.729 Training correlation: 0.9939923924427343\n",
      "\n",
      " Epoch: 205  Train Loss: 3.3868  Validation Loss: 88.2583  Validation Correlation: 0.9743\n",
      "\n",
      "epoch: 206, train_loss: 2.862 Training correlation: 0.9954629633575037\n",
      "\n",
      " Epoch: 206  Train Loss: 2.0063  Validation Loss: 75.9333  Validation Correlation: 0.9769\n",
      "\n",
      "epoch: 207, train_loss: 1.376 Training correlation: 0.9943037754666528\n",
      "\n",
      " Epoch: 207  Train Loss: 1.4570  Validation Loss: 73.6555  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 208, train_loss: 1.132 Training correlation: 0.9959819289600685\n",
      "\n",
      " Epoch: 208  Train Loss: 1.4072  Validation Loss: 74.3316  Validation Correlation: 0.9759\n",
      "\n",
      "epoch: 209, train_loss: 0.979 Training correlation: 0.9967811924533648\n",
      "\n",
      " Epoch: 209  Train Loss: 1.4930  Validation Loss: 89.3880  Validation Correlation: 0.9756\n",
      "\n",
      "epoch: 210, train_loss: 3.103 Training correlation: 0.9967839715453513\n",
      "\n",
      " Epoch: 210  Train Loss: 1.9343  Validation Loss: 89.5170  Validation Correlation: 0.9745\n",
      "\n",
      "epoch: 211, train_loss: 3.678 Training correlation: 0.9943398017248289\n",
      "\n",
      " Epoch: 211  Train Loss: 1.9124  Validation Loss: 71.0515  Validation Correlation: 0.9748\n",
      "\n",
      "epoch: 212, train_loss: 1.577 Training correlation: 0.996439670188193\n",
      "\n",
      " Epoch: 212  Train Loss: 1.6596  Validation Loss: 89.9888  Validation Correlation: 0.9751\n",
      "\n",
      "epoch: 213, train_loss: 4.381 Training correlation: 0.9935277344064494\n",
      "\n",
      " Epoch: 213  Train Loss: 1.7481  Validation Loss: 81.2352  Validation Correlation: 0.9755\n",
      "\n",
      "epoch: 214, train_loss: 1.211 Training correlation: 0.9963667910070694\n",
      "\n",
      " Epoch: 214  Train Loss: 1.7541  Validation Loss: 81.6905  Validation Correlation: 0.9764\n",
      "\n",
      "epoch: 215, train_loss: 1.370 Training correlation: 0.9931890402789902\n",
      "\n",
      " Epoch: 215  Train Loss: 1.7030  Validation Loss: 87.0004  Validation Correlation: 0.9758\n",
      "\n",
      "epoch: 216, train_loss: 2.991 Training correlation: 0.996535968194744\n",
      "\n",
      " Epoch: 216  Train Loss: 1.7636  Validation Loss: 81.4784  Validation Correlation: 0.9746\n",
      "\n",
      "epoch: 217, train_loss: 2.796 Training correlation: 0.9945261808325133\n",
      "\n",
      " Epoch: 217  Train Loss: 1.8052  Validation Loss: 67.2354  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 218, train_loss: 1.728 Training correlation: 0.996794947211896\n",
      "\n",
      " Epoch: 218  Train Loss: 1.6004  Validation Loss: 65.3570  Validation Correlation: 0.9755\n",
      "\n",
      "epoch: 219, train_loss: 1.960 Training correlation: 0.9959301280711818\n",
      "\n",
      " Epoch: 219  Train Loss: 1.3786  Validation Loss: 62.0669  Validation Correlation: 0.9762\n",
      "\n",
      "epoch: 220, train_loss: 3.234 Training correlation: 0.9949027171629712\n",
      "\n",
      " Epoch: 220  Train Loss: 1.5010  Validation Loss: 74.7696  Validation Correlation: 0.9755\n",
      "\n",
      "epoch: 221, train_loss: 0.988 Training correlation: 0.9969382552312647\n",
      "\n",
      " Epoch: 221  Train Loss: 1.4170  Validation Loss: 69.3280  Validation Correlation: 0.9744\n",
      "\n",
      "epoch: 222, train_loss: 1.161 Training correlation: 0.9963318433448073\n",
      "\n",
      " Epoch: 222  Train Loss: 1.4949  Validation Loss: 61.6819  Validation Correlation: 0.9754\n",
      "\n",
      "epoch: 223, train_loss: 2.355 Training correlation: 0.996846907859787\n",
      "\n",
      " Epoch: 223  Train Loss: 1.9050  Validation Loss: 53.3552  Validation Correlation: 0.9741\n",
      "\n",
      "epoch: 224, train_loss: 4.830 Training correlation: 0.9970456453602301\n",
      "\n",
      " Epoch: 224  Train Loss: 2.8596  Validation Loss: 71.2685  Validation Correlation: 0.9764\n",
      "\n",
      "epoch: 225, train_loss: 0.919 Training correlation: 0.9962332716722663\n",
      "\n",
      " Epoch: 225  Train Loss: 1.9894  Validation Loss: 76.0822  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 226, train_loss: 1.527 Training correlation: 0.9966509909340934\n",
      "\n",
      " Epoch: 226  Train Loss: 1.4468  Validation Loss: 72.9705  Validation Correlation: 0.9777\n",
      "\n",
      "epoch: 227, train_loss: 1.100 Training correlation: 0.9963718332832197\n",
      "\n",
      " Epoch: 227  Train Loss: 1.4744  Validation Loss: 58.2335  Validation Correlation: 0.9767\n",
      "\n",
      "epoch: 228, train_loss: 2.567 Training correlation: 0.995303314525471\n",
      "\n",
      " Epoch: 228  Train Loss: 1.4673  Validation Loss: 52.5816  Validation Correlation: 0.9772\n",
      "\n",
      "epoch: 229, train_loss: 5.705 Training correlation: 0.9961756764376135\n",
      "\n",
      " Epoch: 229  Train Loss: 2.0878  Validation Loss: 67.4041  Validation Correlation: 0.9752\n",
      "\n",
      "epoch: 230, train_loss: 0.874 Training correlation: 0.9968030716638157\n",
      "\n",
      " Epoch: 230  Train Loss: 1.4442  Validation Loss: 71.9049  Validation Correlation: 0.9757\n",
      "\n",
      "epoch: 231, train_loss: 1.332 Training correlation: 0.9952185852391539\n",
      "\n",
      " Epoch: 231  Train Loss: 2.5672  Validation Loss: 70.4196  Validation Correlation: 0.9762\n",
      "\n",
      "epoch: 232, train_loss: 1.514 Training correlation: 0.9958274572302767\n",
      "\n",
      " Epoch: 232  Train Loss: 1.4958  Validation Loss: 67.4551  Validation Correlation: 0.9778\n",
      "\n",
      "epoch: 233, train_loss: 2.147 Training correlation: 0.9933061154864199\n",
      "\n",
      " Epoch: 233  Train Loss: 1.6211  Validation Loss: 72.2069  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 234, train_loss: 1.326 Training correlation: 0.99537572330643\n",
      "\n",
      " Epoch: 234  Train Loss: 1.6234  Validation Loss: 74.8550  Validation Correlation: 0.9765\n",
      "\n",
      "epoch: 235, train_loss: 1.373 Training correlation: 0.9956480796728109\n",
      "\n",
      " Epoch: 235  Train Loss: 1.5548  Validation Loss: 90.2679  Validation Correlation: 0.9767\n",
      "\n",
      "epoch: 236, train_loss: 3.886 Training correlation: 0.9962300385320786\n",
      "\n",
      " Epoch: 236  Train Loss: 1.7550  Validation Loss: 62.6711  Validation Correlation: 0.9759\n",
      "\n",
      "epoch: 237, train_loss: 1.711 Training correlation: 0.9954561559873983\n",
      "\n",
      " Epoch: 237  Train Loss: 1.4506  Validation Loss: 63.0612  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 238, train_loss: 2.311 Training correlation: 0.9952737055109552\n",
      "\n",
      " Epoch: 238  Train Loss: 2.5347  Validation Loss: 74.9230  Validation Correlation: 0.9741\n",
      "\n",
      "epoch: 239, train_loss: 2.050 Training correlation: 0.9950686388306453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch: 239  Train Loss: 1.7793  Validation Loss: 59.6394  Validation Correlation: 0.9787\n",
      "\n",
      "epoch: 240, train_loss: 1.501 Training correlation: 0.9972945295519603\n",
      "\n",
      " Epoch: 240  Train Loss: 1.3998  Validation Loss: 69.1851  Validation Correlation: 0.9787\n",
      "\n",
      "epoch: 241, train_loss: 1.303 Training correlation: 0.9956457659409651\n",
      "\n",
      " Epoch: 241  Train Loss: 1.4847  Validation Loss: 67.0071  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 242, train_loss: 1.002 Training correlation: 0.9962371500850917\n",
      "\n",
      " Epoch: 242  Train Loss: 1.4502  Validation Loss: 61.6720  Validation Correlation: 0.9772\n",
      "\n",
      "epoch: 243, train_loss: 1.404 Training correlation: 0.9971062841588653\n",
      "\n",
      " Epoch: 243  Train Loss: 1.5213  Validation Loss: 70.9507  Validation Correlation: 0.9765\n",
      "\n",
      "epoch: 244, train_loss: 1.237 Training correlation: 0.9955813473434506\n",
      "\n",
      " Epoch: 244  Train Loss: 1.4390  Validation Loss: 66.8435  Validation Correlation: 0.9773\n",
      "\n",
      "epoch: 245, train_loss: 0.779 Training correlation: 0.9978311944403474\n",
      "\n",
      " Epoch: 245  Train Loss: 1.4883  Validation Loss: 73.8860  Validation Correlation: 0.9773\n",
      "\n",
      "epoch: 246, train_loss: 1.821 Training correlation: 0.9953344083394267\n",
      "\n",
      " Epoch: 246  Train Loss: 1.4971  Validation Loss: 71.0221  Validation Correlation: 0.9765\n",
      "\n",
      "epoch: 247, train_loss: 0.931 Training correlation: 0.9962986617923545\n",
      "\n",
      " Epoch: 247  Train Loss: 1.2336  Validation Loss: 71.2857  Validation Correlation: 0.9773\n",
      "\n",
      "epoch: 248, train_loss: 1.287 Training correlation: 0.9955339307415053\n",
      "\n",
      " Epoch: 248  Train Loss: 1.2231  Validation Loss: 55.8076  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 249, train_loss: 3.447 Training correlation: 0.9955772546136435\n",
      "\n",
      " Epoch: 249  Train Loss: 1.4776  Validation Loss: 52.1890  Validation Correlation: 0.9769\n",
      "\n",
      "epoch: 250, train_loss: 5.779 Training correlation: 0.9958625243480894\n",
      "\n",
      " Epoch: 250  Train Loss: 1.6527  Validation Loss: 67.1838  Validation Correlation: 0.9769\n",
      "\n",
      "epoch: 251, train_loss: 1.888 Training correlation: 0.9938212350137511\n",
      "\n",
      " Epoch: 251  Train Loss: 1.1975  Validation Loss: 62.0157  Validation Correlation: 0.9773\n",
      "\n",
      "epoch: 252, train_loss: 1.469 Training correlation: 0.9965918602802735\n",
      "\n",
      " Epoch: 252  Train Loss: 2.2404  Validation Loss: 64.5765  Validation Correlation: 0.9777\n",
      "\n",
      "epoch: 253, train_loss: 1.004 Training correlation: 0.9962235193806903\n",
      "\n",
      " Epoch: 253  Train Loss: 1.6692  Validation Loss: 62.6911  Validation Correlation: 0.9762\n",
      "\n",
      "epoch: 254, train_loss: 0.810 Training correlation: 0.9975573983763696\n",
      "\n",
      " Epoch: 254  Train Loss: 1.5807  Validation Loss: 61.3434  Validation Correlation: 0.9788\n",
      "\n",
      "epoch: 255, train_loss: 1.481 Training correlation: 0.9966137441007984\n",
      "\n",
      " Epoch: 255  Train Loss: 1.3829  Validation Loss: 41.8313  Validation Correlation: 0.9764\n",
      "\n",
      "epoch: 256, train_loss: 8.963 Training correlation: 0.9956016987886644\n",
      "\n",
      " Epoch: 256  Train Loss: 1.8757  Validation Loss: 67.2334  Validation Correlation: 0.9764\n",
      "\n",
      "epoch: 257, train_loss: 1.076 Training correlation: 0.9969289402476451\n",
      "\n",
      " Epoch: 257  Train Loss: 1.3305  Validation Loss: 61.0933  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 258, train_loss: 1.410 Training correlation: 0.9955466509752066\n",
      "\n",
      " Epoch: 258  Train Loss: 1.3532  Validation Loss: 72.2263  Validation Correlation: 0.9773\n",
      "\n",
      "epoch: 259, train_loss: 1.540 Training correlation: 0.9961720376901797\n",
      "\n",
      " Epoch: 259  Train Loss: 1.2989  Validation Loss: 52.5186  Validation Correlation: 0.9774\n",
      "\n",
      "epoch: 260, train_loss: 3.663 Training correlation: 0.9963017711562163\n",
      "\n",
      " Epoch: 260  Train Loss: 1.5475  Validation Loss: 65.0938  Validation Correlation: 0.9748\n",
      "\n",
      "epoch: 261, train_loss: 1.150 Training correlation: 0.996765419549269\n",
      "\n",
      " Epoch: 261  Train Loss: 1.7721  Validation Loss: 69.9606  Validation Correlation: 0.9770\n",
      "\n",
      "epoch: 262, train_loss: 1.276 Training correlation: 0.9961315417799195\n",
      "\n",
      " Epoch: 262  Train Loss: 1.2671  Validation Loss: 58.7257  Validation Correlation: 0.9780\n",
      "\n",
      "epoch: 263, train_loss: 2.230 Training correlation: 0.9946635636771008\n",
      "\n",
      " Epoch: 263  Train Loss: 1.2992  Validation Loss: 62.7004  Validation Correlation: 0.9767\n",
      "\n",
      "epoch: 264, train_loss: 1.272 Training correlation: 0.9948697136594459\n",
      "\n",
      " Epoch: 264  Train Loss: 1.3470  Validation Loss: 75.3407  Validation Correlation: 0.9770\n",
      "\n",
      "epoch: 265, train_loss: 2.520 Training correlation: 0.9958596379379888\n",
      "\n",
      " Epoch: 265  Train Loss: 1.3199  Validation Loss: 75.7425  Validation Correlation: 0.9762\n",
      "\n",
      "epoch: 266, train_loss: 2.071 Training correlation: 0.9962793742898766\n",
      "\n",
      " Epoch: 266  Train Loss: 1.3981  Validation Loss: 55.4797  Validation Correlation: 0.9787\n",
      "\n",
      "epoch: 267, train_loss: 2.531 Training correlation: 0.9964855722737526\n",
      "\n",
      " Epoch: 267  Train Loss: 1.1989  Validation Loss: 59.6861  Validation Correlation: 0.9781\n",
      "\n",
      "epoch: 268, train_loss: 1.628 Training correlation: 0.99717545542543\n",
      "\n",
      " Epoch: 268  Train Loss: 1.2308  Validation Loss: 66.2226  Validation Correlation: 0.9777\n",
      "\n",
      "epoch: 269, train_loss: 0.921 Training correlation: 0.9970789818653558\n",
      "\n",
      " Epoch: 269  Train Loss: 1.1649  Validation Loss: 63.2176  Validation Correlation: 0.9787\n",
      "\n",
      "epoch: 270, train_loss: 1.037 Training correlation: 0.9963220298489945\n",
      "\n",
      " Epoch: 270  Train Loss: 1.2671  Validation Loss: 81.3914  Validation Correlation: 0.9759\n",
      "\n",
      "epoch: 271, train_loss: 4.039 Training correlation: 0.9974594007600013\n",
      "\n",
      " Epoch: 271  Train Loss: 2.2622  Validation Loss: 58.6404  Validation Correlation: 0.9769\n",
      "\n",
      "epoch: 272, train_loss: 3.065 Training correlation: 0.9885504967773524\n",
      "\n",
      " Epoch: 272  Train Loss: 1.8242  Validation Loss: 60.9527  Validation Correlation: 0.9723\n",
      "\n",
      "epoch: 273, train_loss: 1.750 Training correlation: 0.9940426330610486\n",
      "\n",
      " Epoch: 273  Train Loss: 1.6483  Validation Loss: 69.4465  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 274, train_loss: 1.148 Training correlation: 0.9963949057447123\n",
      "\n",
      " Epoch: 274  Train Loss: 1.6659  Validation Loss: 61.6773  Validation Correlation: 0.9773\n",
      "\n",
      "epoch: 275, train_loss: 0.889 Training correlation: 0.9969488440476312\n",
      "\n",
      " Epoch: 275  Train Loss: 1.1355  Validation Loss: 58.0131  Validation Correlation: 0.9766\n",
      "\n",
      "epoch: 276, train_loss: 1.438 Training correlation: 0.9964528304769069\n",
      "\n",
      " Epoch: 276  Train Loss: 1.1474  Validation Loss: 58.9322  Validation Correlation: 0.9769\n",
      "\n",
      "epoch: 277, train_loss: 1.209 Training correlation: 0.9965612838768259\n",
      "\n",
      " Epoch: 277  Train Loss: 1.0412  Validation Loss: 72.6567  Validation Correlation: 0.9776\n",
      "\n",
      "epoch: 278, train_loss: 1.587 Training correlation: 0.9950572957503011\n",
      "\n",
      " Epoch: 278  Train Loss: 1.3700  Validation Loss: 62.1729  Validation Correlation: 0.9772\n",
      "\n",
      "epoch: 279, train_loss: 0.967 Training correlation: 0.9967786464742152\n",
      "\n",
      " Epoch: 279  Train Loss: 1.2443  Validation Loss: 70.6798  Validation Correlation: 0.9769\n",
      "\n",
      "epoch: 280, train_loss: 0.737 Training correlation: 0.9979836351046545\n",
      "\n",
      " Epoch: 280  Train Loss: 1.1557  Validation Loss: 84.3169  Validation Correlation: 0.9766\n",
      "\n",
      "epoch: 281, train_loss: 5.823 Training correlation: 0.9965172973188844\n",
      "\n",
      " Epoch: 281  Train Loss: 1.1665  Validation Loss: 64.8079  Validation Correlation: 0.9770\n",
      "\n",
      "epoch: 282, train_loss: 0.981 Training correlation: 0.995969672241911\n",
      "\n",
      " Epoch: 282  Train Loss: 1.0301  Validation Loss: 71.4542  Validation Correlation: 0.9771\n",
      "\n",
      "epoch: 283, train_loss: 1.728 Training correlation: 0.9971990842120405\n",
      "\n",
      " Epoch: 283  Train Loss: 1.1715  Validation Loss: 59.6045  Validation Correlation: 0.9758\n",
      "\n",
      "epoch: 284, train_loss: 1.186 Training correlation: 0.9963497025186122\n",
      "\n",
      " Epoch: 284  Train Loss: 1.2774  Validation Loss: 65.6999  Validation Correlation: 0.9765\n",
      "\n",
      "epoch: 285, train_loss: 1.024 Training correlation: 0.9967312194626021\n",
      "\n",
      " Epoch: 285  Train Loss: 1.1677  Validation Loss: 57.3471  Validation Correlation: 0.9768\n",
      "\n",
      "epoch: 286, train_loss: 1.587 Training correlation: 0.9937628806673383\n",
      "\n",
      " Epoch: 286  Train Loss: 1.2302  Validation Loss: 61.7151  Validation Correlation: 0.9772\n",
      "\n",
      "epoch: 287, train_loss: 0.654 Training correlation: 0.9976001779749368\n",
      "\n",
      " Epoch: 287  Train Loss: 1.3570  Validation Loss: 58.4390  Validation Correlation: 0.9774\n",
      "\n",
      "epoch: 288, train_loss: 1.274 Training correlation: 0.9961495780648364\n",
      "\n",
      " Epoch: 288  Train Loss: 1.2715  Validation Loss: 67.6458  Validation Correlation: 0.9762\n",
      "\n",
      "epoch: 289, train_loss: 1.657 Training correlation: 0.9968144195641885\n",
      "\n",
      " Epoch: 289  Train Loss: 1.6666  Validation Loss: 59.7404  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 290, train_loss: 1.023 Training correlation: 0.9961090943337564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch: 290  Train Loss: 1.5484  Validation Loss: 39.0308  Validation Correlation: 0.9764\n",
      "\n",
      "epoch: 291, train_loss: 8.532 Training correlation: 0.9970547368079182\n",
      "\n",
      " Epoch: 291  Train Loss: 2.3366  Validation Loss: 54.0471  Validation Correlation: 0.9758\n",
      "\n",
      "epoch: 292, train_loss: 1.163 Training correlation: 0.9975219911280977\n",
      "\n",
      " Epoch: 292  Train Loss: 1.0975  Validation Loss: 58.3088  Validation Correlation: 0.9755\n",
      "\n",
      "epoch: 293, train_loss: 0.961 Training correlation: 0.9965964877916395\n",
      "\n",
      " Epoch: 293  Train Loss: 0.9356  Validation Loss: 57.0044  Validation Correlation: 0.9770\n",
      "\n",
      "epoch: 294, train_loss: 1.138 Training correlation: 0.9977837991311054\n",
      "\n",
      " Epoch: 294  Train Loss: 1.3837  Validation Loss: 61.3145  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 295, train_loss: 0.623 Training correlation: 0.9980211210083415\n",
      "\n",
      " Epoch: 295  Train Loss: 1.0160  Validation Loss: 59.2844  Validation Correlation: 0.9765\n",
      "\n",
      "epoch: 296, train_loss: 0.877 Training correlation: 0.996506436290577\n",
      "\n",
      " Epoch: 296  Train Loss: 1.1709  Validation Loss: 59.4396  Validation Correlation: 0.9757\n",
      "\n",
      "epoch: 297, train_loss: 1.017 Training correlation: 0.9959887129680179\n",
      "\n",
      " Epoch: 297  Train Loss: 1.0930  Validation Loss: 65.2278  Validation Correlation: 0.9765\n",
      "\n",
      "epoch: 298, train_loss: 0.832 Training correlation: 0.9971287484642176\n",
      "\n",
      " Epoch: 298  Train Loss: 1.3288  Validation Loss: 54.9060  Validation Correlation: 0.9759\n",
      "\n",
      "epoch: 299, train_loss: 1.491 Training correlation: 0.9945996308210164\n",
      "\n",
      " Epoch: 299  Train Loss: 1.4296  Validation Loss: 47.5550  Validation Correlation: 0.9754\n",
      "\n",
      "epoch: 300, train_loss: 3.355 Training correlation: 0.9965265993144936\n",
      "\n",
      " Epoch: 300  Train Loss: 1.2248  Validation Loss: 59.5309  Validation Correlation: 0.9756\n",
      "\n",
      "epoch: 301, train_loss: 0.517 Training correlation: 0.9981823778907198\n",
      "\n",
      " Epoch: 301  Train Loss: 1.1376  Validation Loss: 54.1372  Validation Correlation: 0.9770\n",
      "\n",
      "epoch: 302, train_loss: 1.741 Training correlation: 0.9944337781550854\n",
      "\n",
      " Epoch: 302  Train Loss: 1.2542  Validation Loss: 58.2631  Validation Correlation: 0.9752\n",
      "\n",
      "epoch: 303, train_loss: 0.902 Training correlation: 0.9966064059466646\n",
      "\n",
      " Epoch: 303  Train Loss: 1.3315  Validation Loss: 56.3652  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 304, train_loss: 0.904 Training correlation: 0.9967698454786809\n",
      "\n",
      " Epoch: 304  Train Loss: 1.3942  Validation Loss: 63.9579  Validation Correlation: 0.9767\n",
      "\n",
      "epoch: 305, train_loss: 1.476 Training correlation: 0.9968993558695906\n",
      "\n",
      " Epoch: 305  Train Loss: 1.3989  Validation Loss: 52.5402  Validation Correlation: 0.9752\n",
      "\n",
      "epoch: 306, train_loss: 1.320 Training correlation: 0.9966932934803595\n",
      "\n",
      " Epoch: 306  Train Loss: 1.3490  Validation Loss: 66.5385  Validation Correlation: 0.9744\n",
      "\n",
      "epoch: 307, train_loss: 1.036 Training correlation: 0.9978170884228992\n",
      "\n",
      " Epoch: 307  Train Loss: 1.1061  Validation Loss: 49.2853  Validation Correlation: 0.9759\n",
      "\n",
      "epoch: 308, train_loss: 2.778 Training correlation: 0.9972759878292197\n",
      "\n",
      " Epoch: 308  Train Loss: 1.1393  Validation Loss: 52.2002  Validation Correlation: 0.9747\n",
      "\n",
      "epoch: 309, train_loss: 1.409 Training correlation: 0.997613140240774\n",
      "\n",
      " Epoch: 309  Train Loss: 0.9642  Validation Loss: 57.4913  Validation Correlation: 0.9729\n",
      "\n",
      "epoch: 310, train_loss: 0.905 Training correlation: 0.9956651241181993\n",
      "\n",
      " Epoch: 310  Train Loss: 1.1996  Validation Loss: 54.4815  Validation Correlation: 0.9750\n",
      "\n",
      "epoch: 311, train_loss: 0.908 Training correlation: 0.9962907847112733\n",
      "\n",
      " Epoch: 311  Train Loss: 1.0146  Validation Loss: 68.9515  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 312, train_loss: 1.553 Training correlation: 0.9971568400793347\n",
      "\n",
      " Epoch: 312  Train Loss: 1.3558  Validation Loss: 50.3549  Validation Correlation: 0.9755\n",
      "\n",
      "epoch: 313, train_loss: 2.440 Training correlation: 0.9962545448613583\n",
      "\n",
      " Epoch: 313  Train Loss: 1.2908  Validation Loss: 62.3680  Validation Correlation: 0.9749\n",
      "\n",
      "epoch: 314, train_loss: 1.338 Training correlation: 0.9945144655042715\n",
      "\n",
      " Epoch: 314  Train Loss: 1.8447  Validation Loss: 55.1986  Validation Correlation: 0.9769\n",
      "\n",
      "epoch: 315, train_loss: 1.519 Training correlation: 0.9947955815846313\n",
      "\n",
      " Epoch: 315  Train Loss: 1.2825  Validation Loss: 52.2494  Validation Correlation: 0.9754\n",
      "\n",
      "epoch: 316, train_loss: 1.764 Training correlation: 0.9964339411185988\n",
      "\n",
      " Epoch: 316  Train Loss: 1.0451  Validation Loss: 53.7869  Validation Correlation: 0.9749\n",
      "\n",
      "epoch: 317, train_loss: 1.155 Training correlation: 0.9966655474086412\n",
      "\n",
      " Epoch: 317  Train Loss: 0.9645  Validation Loss: 59.6894  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 318, train_loss: 0.786 Training correlation: 0.9968141964153795\n",
      "\n",
      " Epoch: 318  Train Loss: 0.9906  Validation Loss: 66.5688  Validation Correlation: 0.9762\n",
      "\n",
      "epoch: 319, train_loss: 1.448 Training correlation: 0.9976541066618745\n",
      "\n",
      " Epoch: 319  Train Loss: 1.0726  Validation Loss: 58.2979  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 320, train_loss: 0.904 Training correlation: 0.996783563156372\n",
      "\n",
      " Epoch: 320  Train Loss: 1.1934  Validation Loss: 53.5326  Validation Correlation: 0.9763\n",
      "\n",
      "epoch: 321, train_loss: 1.172 Training correlation: 0.9961692625836663\n",
      "\n",
      " Epoch: 321  Train Loss: 1.0565  Validation Loss: 63.5731  Validation Correlation: 0.9779\n",
      "\n",
      "epoch: 322, train_loss: 1.161 Training correlation: 0.9975937312857195\n",
      "\n",
      " Epoch: 322  Train Loss: 1.3801  Validation Loss: 81.3697  Validation Correlation: 0.9731\n",
      "\n",
      "epoch: 323, train_loss: 6.625 Training correlation: 0.9944803718508156\n",
      "\n",
      " Epoch: 323  Train Loss: 2.0739  Validation Loss: 73.6708  Validation Correlation: 0.9764\n",
      "\n",
      "epoch: 324, train_loss: 3.221 Training correlation: 0.9977094773520457\n",
      "\n",
      " Epoch: 324  Train Loss: 1.4206  Validation Loss: 54.7410  Validation Correlation: 0.9772\n",
      "\n",
      "epoch: 325, train_loss: 0.918 Training correlation: 0.9977893288726634\n",
      "\n",
      " Epoch: 325  Train Loss: 1.2000  Validation Loss: 68.7012  Validation Correlation: 0.9766\n",
      "\n",
      "epoch: 326, train_loss: 1.914 Training correlation: 0.9969838183838354\n",
      "\n",
      " Epoch: 326  Train Loss: 1.3530  Validation Loss: 66.1087  Validation Correlation: 0.9770\n",
      "\n",
      "epoch: 327, train_loss: 1.199 Training correlation: 0.9972979324116774\n",
      "\n",
      " Epoch: 327  Train Loss: 1.2056  Validation Loss: 59.8216  Validation Correlation: 0.9751\n",
      "\n",
      "epoch: 328, train_loss: 0.761 Training correlation: 0.9981092689426468\n",
      "\n",
      " Epoch: 328  Train Loss: 1.2038  Validation Loss: 59.8895  Validation Correlation: 0.9768\n",
      "\n",
      "epoch: 329, train_loss: 1.460 Training correlation: 0.9949567362824969\n",
      "\n",
      " Epoch: 329  Train Loss: 1.0001  Validation Loss: 62.4395  Validation Correlation: 0.9769\n",
      "\n",
      "epoch: 330, train_loss: 1.341 Training correlation: 0.9976201027956433\n",
      "\n",
      " Epoch: 330  Train Loss: 0.9365  Validation Loss: 50.9314  Validation Correlation: 0.9756\n",
      "\n",
      "epoch: 331, train_loss: 1.159 Training correlation: 0.9975112989825621\n",
      "\n",
      " Epoch: 331  Train Loss: 0.9794  Validation Loss: 63.0139  Validation Correlation: 0.9769\n",
      "\n",
      "epoch: 332, train_loss: 1.261 Training correlation: 0.996308925879753\n",
      "\n",
      " Epoch: 332  Train Loss: 1.0142  Validation Loss: 49.0120  Validation Correlation: 0.9774\n",
      "\n",
      "epoch: 333, train_loss: 2.215 Training correlation: 0.9942125699837511\n",
      "\n",
      " Epoch: 333  Train Loss: 1.1173  Validation Loss: 52.6014  Validation Correlation: 0.9791\n",
      "\n",
      "epoch: 334, train_loss: 0.985 Training correlation: 0.9972073522094823\n",
      "\n",
      " Epoch: 334  Train Loss: 0.9941  Validation Loss: 66.9124  Validation Correlation: 0.9766\n",
      "\n",
      "epoch: 335, train_loss: 1.652 Training correlation: 0.99722328199971\n",
      "\n",
      " Epoch: 335  Train Loss: 1.1479  Validation Loss: 56.3791  Validation Correlation: 0.9762\n",
      "\n",
      "epoch: 336, train_loss: 0.627 Training correlation: 0.9979001860976912\n",
      "\n",
      " Epoch: 336  Train Loss: 0.9789  Validation Loss: 52.9582  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 337, train_loss: 0.852 Training correlation: 0.997813094828493\n",
      "\n",
      " Epoch: 337  Train Loss: 0.8963  Validation Loss: 58.1542  Validation Correlation: 0.9757\n",
      "\n",
      "epoch: 338, train_loss: 0.844 Training correlation: 0.9965036550130423\n",
      "\n",
      " Epoch: 338  Train Loss: 1.5127  Validation Loss: 65.2182  Validation Correlation: 0.9754\n",
      "\n",
      "epoch: 339, train_loss: 2.207 Training correlation: 0.9965802645636498\n",
      "\n",
      " Epoch: 339  Train Loss: 1.0992  Validation Loss: 68.6841  Validation Correlation: 0.9755\n",
      "\n",
      "epoch: 340, train_loss: 2.046 Training correlation: 0.9966984631304264\n",
      "\n",
      " Epoch: 340  Train Loss: 1.1184  Validation Loss: 50.1605  Validation Correlation: 0.9778\n",
      "\n",
      "epoch: 341, train_loss: 1.081 Training correlation: 0.9982743092622247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch: 341  Train Loss: 1.4385  Validation Loss: 64.2981  Validation Correlation: 0.9752\n",
      "\n",
      "epoch: 342, train_loss: 1.597 Training correlation: 0.9969310802456242\n",
      "\n",
      " Epoch: 342  Train Loss: 1.1498  Validation Loss: 63.0758  Validation Correlation: 0.9754\n",
      "\n",
      "epoch: 343, train_loss: 1.117 Training correlation: 0.997629719641816\n",
      "\n",
      " Epoch: 343  Train Loss: 1.1881  Validation Loss: 63.6043  Validation Correlation: 0.9753\n",
      "\n",
      "epoch: 344, train_loss: 1.211 Training correlation: 0.9972118247880835\n",
      "\n",
      " Epoch: 344  Train Loss: 1.3074  Validation Loss: 52.4918  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 345, train_loss: 0.958 Training correlation: 0.9953049598221742\n",
      "\n",
      " Epoch: 345  Train Loss: 1.1269  Validation Loss: 59.0220  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 346, train_loss: 0.781 Training correlation: 0.9980090745757937\n",
      "\n",
      " Epoch: 346  Train Loss: 1.0339  Validation Loss: 51.3146  Validation Correlation: 0.9767\n",
      "\n",
      "epoch: 347, train_loss: 1.450 Training correlation: 0.9970034981998969\n",
      "\n",
      " Epoch: 347  Train Loss: 1.1868  Validation Loss: 50.6876  Validation Correlation: 0.9769\n",
      "\n",
      "epoch: 348, train_loss: 0.932 Training correlation: 0.9979534976579825\n",
      "\n",
      " Epoch: 348  Train Loss: 1.2804  Validation Loss: 42.5430  Validation Correlation: 0.9764\n",
      "\n",
      "epoch: 349, train_loss: 2.707 Training correlation: 0.9982437174782667\n",
      "\n",
      " Epoch: 349  Train Loss: 2.2406  Validation Loss: 53.6232  Validation Correlation: 0.9732\n",
      "\n",
      "epoch: 350, train_loss: 0.807 Training correlation: 0.9967613172563144\n",
      "\n",
      " Epoch: 350  Train Loss: 1.1525  Validation Loss: 43.3378  Validation Correlation: 0.9731\n",
      "\n",
      "epoch: 351, train_loss: 3.477 Training correlation: 0.996922455749189\n",
      "\n",
      " Epoch: 351  Train Loss: 1.1722  Validation Loss: 51.0581  Validation Correlation: 0.9752\n",
      "\n",
      "epoch: 352, train_loss: 0.688 Training correlation: 0.9973689379654571\n",
      "\n",
      " Epoch: 352  Train Loss: 1.0062  Validation Loss: 58.8068  Validation Correlation: 0.9762\n",
      "\n",
      "epoch: 353, train_loss: 0.868 Training correlation: 0.9970491671620485\n",
      "\n",
      " Epoch: 353  Train Loss: 1.0726  Validation Loss: 51.2611  Validation Correlation: 0.9741\n",
      "\n",
      "epoch: 354, train_loss: 1.390 Training correlation: 0.9956650049394069\n",
      "\n",
      " Epoch: 354  Train Loss: 1.0046  Validation Loss: 61.1255  Validation Correlation: 0.9768\n",
      "\n",
      "epoch: 355, train_loss: 0.747 Training correlation: 0.9980320555390485\n",
      "\n",
      " Epoch: 355  Train Loss: 0.9725  Validation Loss: 54.2197  Validation Correlation: 0.9758\n",
      "\n",
      "epoch: 356, train_loss: 0.792 Training correlation: 0.9963714744057809\n",
      "\n",
      " Epoch: 356  Train Loss: 0.9016  Validation Loss: 45.4839  Validation Correlation: 0.9756\n",
      "\n",
      "epoch: 357, train_loss: 1.890 Training correlation: 0.9974438830753268\n",
      "\n",
      " Epoch: 357  Train Loss: 1.0703  Validation Loss: 57.6782  Validation Correlation: 0.9764\n",
      "\n",
      "epoch: 358, train_loss: 0.495 Training correlation: 0.9984995502059272\n",
      "\n",
      " Epoch: 358  Train Loss: 1.0503  Validation Loss: 55.0616  Validation Correlation: 0.9748\n",
      "\n",
      "epoch: 359, train_loss: 0.791 Training correlation: 0.9970786404373205\n",
      "\n",
      " Epoch: 359  Train Loss: 0.9569  Validation Loss: 52.9603  Validation Correlation: 0.9742\n",
      "\n",
      "epoch: 360, train_loss: 0.972 Training correlation: 0.9967507913714168\n",
      "\n",
      " Epoch: 360  Train Loss: 0.8897  Validation Loss: 47.0600  Validation Correlation: 0.9751\n",
      "\n",
      "epoch: 361, train_loss: 1.780 Training correlation: 0.9969720535401656\n",
      "\n",
      " Epoch: 361  Train Loss: 1.0416  Validation Loss: 58.8468  Validation Correlation: 0.9738\n",
      "\n",
      "epoch: 362, train_loss: 1.060 Training correlation: 0.9971890377960375\n",
      "\n",
      " Epoch: 362  Train Loss: 1.0402  Validation Loss: 54.6696  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 363, train_loss: 0.883 Training correlation: 0.9971052022462354\n",
      "\n",
      " Epoch: 363  Train Loss: 1.2380  Validation Loss: 63.3808  Validation Correlation: 0.9770\n",
      "\n",
      "epoch: 364, train_loss: 1.602 Training correlation: 0.9960786218423552\n",
      "\n",
      " Epoch: 364  Train Loss: 1.0927  Validation Loss: 66.8772  Validation Correlation: 0.9760\n",
      "\n",
      "epoch: 365, train_loss: 2.848 Training correlation: 0.996115761177378\n",
      "\n",
      " Epoch: 365  Train Loss: 1.5643  Validation Loss: 58.1390  Validation Correlation: 0.9768\n",
      "\n",
      "epoch: 366, train_loss: 0.956 Training correlation: 0.9966418486509918\n",
      "\n",
      " Epoch: 366  Train Loss: 1.0159  Validation Loss: 72.9331  Validation Correlation: 0.9749\n",
      "\n",
      "epoch: 367, train_loss: 4.456 Training correlation: 0.9977689487800993\n",
      "\n",
      " Epoch: 367  Train Loss: 1.6507  Validation Loss: 62.3047  Validation Correlation: 0.9749\n",
      "\n",
      "epoch: 368, train_loss: 2.619 Training correlation: 0.9968903944224373\n",
      "\n",
      " Epoch: 368  Train Loss: 1.3486  Validation Loss: 47.1903  Validation Correlation: 0.9751\n",
      "\n",
      "epoch: 369, train_loss: 1.256 Training correlation: 0.9968997450857237\n",
      "\n",
      " Epoch: 369  Train Loss: 0.9661  Validation Loss: 55.5384  Validation Correlation: 0.9765\n",
      "\n",
      "epoch: 370, train_loss: 0.914 Training correlation: 0.9969566841311601\n",
      "\n",
      " Epoch: 370  Train Loss: 0.9576  Validation Loss: 55.4171  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 371, train_loss: 0.684 Training correlation: 0.9975316184748961\n",
      "\n",
      " Epoch: 371  Train Loss: 1.0132  Validation Loss: 49.0213  Validation Correlation: 0.9736\n",
      "\n",
      "epoch: 372, train_loss: 1.246 Training correlation: 0.9972582532534338\n",
      "\n",
      " Epoch: 372  Train Loss: 1.0345  Validation Loss: 53.2414  Validation Correlation: 0.9767\n",
      "\n",
      "epoch: 373, train_loss: 1.116 Training correlation: 0.9965073497273178\n",
      "\n",
      " Epoch: 373  Train Loss: 1.0131  Validation Loss: 54.5248  Validation Correlation: 0.9729\n",
      "\n",
      "epoch: 374, train_loss: 1.038 Training correlation: 0.9953964114080898\n",
      "\n",
      " Epoch: 374  Train Loss: 1.0359  Validation Loss: 64.6129  Validation Correlation: 0.9749\n",
      "\n",
      "epoch: 375, train_loss: 2.880 Training correlation: 0.9967753571345904\n",
      "\n",
      " Epoch: 375  Train Loss: 1.1360  Validation Loss: 48.2210  Validation Correlation: 0.9757\n",
      "\n",
      "epoch: 376, train_loss: 1.065 Training correlation: 0.996686706529993\n",
      "\n",
      " Epoch: 376  Train Loss: 1.3339  Validation Loss: 48.6761  Validation Correlation: 0.9747\n",
      "\n",
      "epoch: 377, train_loss: 1.099 Training correlation: 0.9966301823278586\n",
      "\n",
      " Epoch: 377  Train Loss: 1.0406  Validation Loss: 45.1367  Validation Correlation: 0.9743\n",
      "\n",
      "epoch: 378, train_loss: 2.351 Training correlation: 0.9934463238815054\n",
      "\n",
      " Epoch: 378  Train Loss: 1.3573  Validation Loss: 38.9209  Validation Correlation: 0.9738\n",
      "\n",
      "epoch: 379, train_loss: 2.911 Training correlation: 0.9959262703575582\n",
      "\n",
      " Epoch: 379  Train Loss: 1.4670  Validation Loss: 54.4687  Validation Correlation: 0.9755\n",
      "\n",
      "epoch: 380, train_loss: 0.998 Training correlation: 0.9965399392545397\n",
      "\n",
      " Epoch: 380  Train Loss: 0.9391  Validation Loss: 55.4358  Validation Correlation: 0.9765\n",
      "\n",
      "epoch: 381, train_loss: 1.024 Training correlation: 0.9982768031110386\n",
      "\n",
      " Epoch: 381  Train Loss: 1.3458  Validation Loss: 42.6849  Validation Correlation: 0.9738\n",
      "\n",
      "epoch: 382, train_loss: 2.764 Training correlation: 0.9969194926161921\n",
      "\n",
      " Epoch: 382  Train Loss: 1.2275  Validation Loss: 40.4133  Validation Correlation: 0.9758\n",
      "\n",
      "epoch: 383, train_loss: 2.351 Training correlation: 0.9976420065984276\n",
      "\n",
      " Epoch: 383  Train Loss: 1.0099  Validation Loss: 45.4070  Validation Correlation: 0.9765\n",
      "\n",
      "epoch: 384, train_loss: 1.894 Training correlation: 0.9974701724036863\n",
      "\n",
      " Epoch: 384  Train Loss: 1.0935  Validation Loss: 50.1945  Validation Correlation: 0.9744\n",
      "\n",
      "epoch: 385, train_loss: 0.887 Training correlation: 0.9967800876537809\n",
      "\n",
      " Epoch: 385  Train Loss: 1.0431  Validation Loss: 59.8131  Validation Correlation: 0.9748\n",
      "\n",
      "epoch: 386, train_loss: 1.919 Training correlation: 0.9968969568992212\n",
      "\n",
      " Epoch: 386  Train Loss: 0.9318  Validation Loss: 50.3297  Validation Correlation: 0.9759\n",
      "\n",
      "epoch: 387, train_loss: 1.239 Training correlation: 0.9955910766541107\n",
      "\n",
      " Epoch: 387  Train Loss: 0.9441  Validation Loss: 50.2203  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 388, train_loss: 0.675 Training correlation: 0.997302121352784\n",
      "\n",
      " Epoch: 388  Train Loss: 1.0329  Validation Loss: 53.2834  Validation Correlation: 0.9744\n",
      "\n",
      "epoch: 389, train_loss: 0.640 Training correlation: 0.9977567502138313\n",
      "\n",
      " Epoch: 389  Train Loss: 1.1819  Validation Loss: 51.3234  Validation Correlation: 0.9765\n",
      "\n",
      "epoch: 390, train_loss: 0.754 Training correlation: 0.9967824921241677\n",
      "\n",
      " Epoch: 390  Train Loss: 0.8898  Validation Loss: 48.3937  Validation Correlation: 0.9747\n",
      "\n",
      "epoch: 391, train_loss: 0.669 Training correlation: 0.9969360000955123\n",
      "\n",
      " Epoch: 391  Train Loss: 0.8524  Validation Loss: 47.7538  Validation Correlation: 0.9754\n",
      "\n",
      "epoch: 392, train_loss: 1.065 Training correlation: 0.9977445069222308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch: 392  Train Loss: 0.9328  Validation Loss: 41.1621  Validation Correlation: 0.9740\n",
      "\n",
      "epoch: 393, train_loss: 2.740 Training correlation: 0.9964899182095597\n",
      "\n",
      " Epoch: 393  Train Loss: 0.9658  Validation Loss: 46.9632  Validation Correlation: 0.9772\n",
      "\n",
      "epoch: 394, train_loss: 2.028 Training correlation: 0.993084687315861\n",
      "\n",
      " Epoch: 394  Train Loss: 0.8182  Validation Loss: 56.7854  Validation Correlation: 0.9761\n",
      "\n",
      "epoch: 395, train_loss: 0.738 Training correlation: 0.9983817823743876\n",
      "\n",
      " Epoch: 395  Train Loss: 0.8196  Validation Loss: 60.4812  Validation Correlation: 0.9759\n",
      "\n",
      "epoch: 396, train_loss: 1.524 Training correlation: 0.996756329097764\n",
      "\n",
      " Epoch: 396  Train Loss: 0.8032  Validation Loss: 50.3615  Validation Correlation: 0.9756\n",
      "\n",
      "epoch: 397, train_loss: 0.549 Training correlation: 0.9979466358763958\n",
      "\n",
      " Epoch: 397  Train Loss: 0.7766  Validation Loss: 50.5354  Validation Correlation: 0.9753\n",
      "\n",
      "epoch: 398, train_loss: 0.603 Training correlation: 0.9977398487687255\n",
      "\n",
      " Epoch: 398  Train Loss: 0.8510  Validation Loss: 59.7331  Validation Correlation: 0.9739\n",
      "\n",
      "epoch: 399, train_loss: 1.215 Training correlation: 0.9973931973610954\n",
      "\n",
      " Epoch: 399  Train Loss: 0.8350  Validation Loss: 52.7801  Validation Correlation: 0.9753\n",
      "\n",
      "epoch: 400, train_loss: 0.740 Training correlation: 0.9967804658088195\n",
      "\n",
      " Epoch: 400  Train Loss: 1.2309  Validation Loss: 40.2787  Validation Correlation: 0.9746\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default data, without normalization\n",
      "Pearson Correlation: \n",
      " [[1.         0.97462201]\n",
      " [0.97462201 1.        ]]\n",
      "RSE= 0.39743619135051184\n",
      "R-Square= 0.9498880718159616\n",
      "rmse= 0.3958867266155978\n",
      "mae= 4.933344421683582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1d1e2379cd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from torch import nn\n",
    "import torch.cuda\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Loading the data\n",
    "train_x = np.load('./data_matrix/ptms/train_x.npy')\n",
    "train_y = np.load('./data_matrix/ptms/train_y.npy')\n",
    "val_x = np.load('./data_matrix/ptms/val_x.npy')\n",
    "val_y = np.load('./data_matrix/ptms/val_y.npy')\n",
    "test_x = np.load('./data_matrix/ptms/test_x.npy')\n",
    "test_y = np.load('./data_matrix/ptms/test_y.npy')\n",
    "test_no_mod_x = np.load('./data_matrix/ptms/test_no_mod_x.npy')\n",
    "test_no_mod_y = np.load('./data_matrix/ptms/test_no_mod_y.npy')\n",
    "\n",
    "want_figure = True\n",
    "over_test_set = False\n",
    "training = True\n",
    "save_model = True\n",
    "path = './saved_models/14ptms/400_acetyl.pth'\n",
    "\n",
    "\n",
    "# Initialize the hyper parameters\n",
    "\n",
    "def get_config(lr=0.0003, epoch=400, batch=64, cnn_layers=5, fc_layers=1, drop=0.15, kernel=5, fc_output=1024):\n",
    "    config = {\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": epoch,\n",
    "        \"batch_size\": batch,\n",
    "        \"cnn_layers\": cnn_layers,\n",
    "        \"fc_layers\": fc_layers,\n",
    "        \"dropout\": drop,\n",
    "        \"kernel_size\": kernel,\n",
    "        \"fc_out\": fc_output\n",
    "    }\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "# Making the pytorch dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, sequences, retention):\n",
    "        self.sequences = sequences\n",
    "        self.retention = retention\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.retention)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.sequences[idx], self.retention[idx])\n",
    "\n",
    "\n",
    "# Initiate Dataloaders\n",
    "\n",
    "train_dataset = MyDataset(train_x, train_y)\n",
    "val_dataset = MyDataset(val_x, val_y)\n",
    "test_dataset = MyDataset(test_x, test_y)\n",
    "test_no_mod_dataset = MyDataset(test_no_mod_x, test_no_mod_y)\n",
    "\n",
    "config = get_config()\n",
    "dataloader_train = DataLoader(train_dataset, shuffle=True, batch_size=config[\"batch_size\"])\n",
    "dataloader_val = DataLoader(val_dataset, pin_memory=True)\n",
    "dataloader_test = DataLoader(test_dataset, pin_memory=True)\n",
    "dataloader_test_no_mod = DataLoader(test_no_mod_dataset, pin_memory=True)\n",
    "\n",
    "import wandb\n",
    "\n",
    "# wandb.init(config=config, project=\"Retention Prediction\", entity=\"alirezak2\", )\n",
    "wandb.init(config=config, project=\"Hyper Parameters\", entity=\"alirezak2\", )\n",
    "\n",
    "\n",
    "# Create Network architecture\n",
    "\n",
    "class MyNet_firstOne(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.l = nn.ModuleList()\n",
    "\n",
    "        self.l.append(nn.Conv1d(in_channels=9, out_channels=100, kernel_size=(3,), stride=(1,), padding=(1,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "        self.l.append(nn.Dropout(p=0.2, inplace=False))\n",
    "\n",
    "        self.l.append(nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(3,), stride=(1,), padding=(1,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "        self.l.append(nn.Dropout(p=0.2, inplace=False))\n",
    "\n",
    "        self.l.append(nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(3,), stride=(1,), padding=(1,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "        self.l.append(nn.Dropout(p=0.2, inplace=False))\n",
    "\n",
    "        self.l.append(nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(3,), stride=(1,), padding=(1,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "        self.l.append(nn.Dropout(p=0.2, inplace=False))\n",
    "\n",
    "        self.l.append(nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(3,), stride=(1,), padding=(1,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "        self.l.append(nn.Dropout(p=0.2, inplace=False))\n",
    "\n",
    "        self.l.append(nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(3,), stride=(1,), padding=(1,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "        self.l.append(nn.Dropout(p=0.2, inplace=False))\n",
    "\n",
    "        self.l.append(nn.Conv1d(in_channels=100, out_channels=32, kernel_size=(4,), stride=(1,), padding=(1,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "\n",
    "        self.l.append(nn.Flatten())\n",
    "\n",
    "        self.l.append(nn.Linear(in_features=32 * 51, out_features=128))\n",
    "        self.l.append(nn.ReLU())\n",
    "\n",
    "        self.l.append(nn.Linear(in_features=128, out_features=1))\n",
    "        self.l.append(nn.Linear(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.l:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.l = nn.ModuleList()\n",
    "\n",
    "        self.l.append(nn.Conv1d(in_channels=30, out_channels=100, kernel_size=(3,), stride=(1,), padding=(1,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "\n",
    "        self.l.append(nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(3,), stride=(1,), padding=(1,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "\n",
    "        self.l.append(nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(3,), stride=(1,), padding=(1,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "\n",
    "        self.l.append(nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(3,), stride=(1,), padding=(1,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "\n",
    "        self.l.append(nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(3,), stride=(1,), padding=(1,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "\n",
    "        self.l.append(nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(3,), stride=(1,), padding=(1,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "\n",
    "        self.l.append(nn.Conv1d(in_channels=100, out_channels=32, kernel_size=(4,), stride=(1,), padding=(1,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "\n",
    "        self.l.append(nn.Flatten())\n",
    "\n",
    "        self.l.append(nn.Linear(in_features=32 * 51, out_features=128))\n",
    "        self.l.append(nn.ReLU())\n",
    "\n",
    "        self.l.append(nn.Linear(in_features=128, out_features=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.l:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "padding = int((wandb.config['kernel_size'] - 1) / 2)\n",
    "\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.l = nn.ModuleList()\n",
    "\n",
    "        self.l.append(\n",
    "            nn.Conv1d(in_channels=30, out_channels=100, kernel_size=(wandb.config['kernel_size'],), stride=(1,),\n",
    "                      padding=(padding,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "\n",
    "        for layer in range(wandb.config['cnn_layers']):\n",
    "            self.l.append(\n",
    "                nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(wandb.config['kernel_size'],), stride=(1,),\n",
    "                          padding=(padding,)))\n",
    "            self.l.append(nn.ReLU())\n",
    "            self.l.append(nn.Dropout(p=wandb.config['dropout'], inplace=False))\n",
    "\n",
    "            self.l.append(\n",
    "                nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(wandb.config['kernel_size'],), stride=(1,),\n",
    "                          padding=(padding,)))\n",
    "            self.l.append(nn.ReLU())\n",
    "            self.l.append(nn.Dropout(p=wandb.config['dropout'], inplace=False))\n",
    "\n",
    "            self.l.append(nn.MaxPool1d(kernel_size=(wandb.config['kernel_size'],), stride=(1,), padding=(padding,)))\n",
    "\n",
    "        self.l.append(\n",
    "            nn.Conv1d(in_channels=100, out_channels=32, kernel_size=(wandb.config['kernel_size'],), stride=(1,),\n",
    "                      padding=(padding,)))\n",
    "        self.l.append(nn.ReLU())\n",
    "        self.l.append(nn.Dropout(p=wandb.config['dropout'], inplace=False))\n",
    "        self.l.append(nn.MaxPool1d(kernel_size=(wandb.config['kernel_size'],), stride=(1,), padding=(padding,)))\n",
    "\n",
    "        self.l.append(nn.Flatten())\n",
    "\n",
    "        self.l.append(nn.Linear(in_features=32 * train_x.shape[2], out_features=wandb.config['fc_out']))\n",
    "        self.l.append(nn.ReLU())\n",
    "\n",
    "        for layer in range(wandb.config['fc_layers']):\n",
    "            self.l.append(nn.Linear(in_features=wandb.config['fc_out'], out_features=wandb.config['fc_out']))\n",
    "            self.l.append(nn.ReLU())\n",
    "\n",
    "        self.l.append(nn.Linear(in_features=wandb.config['fc_out'], out_features=int(wandb.config['fc_out'] / 2)))\n",
    "        self.l.append(\n",
    "            nn.Linear(in_features=int(wandb.config['fc_out'] / 2), out_features=int(wandb.config['fc_out'] / 4)))\n",
    "        self.l.append(nn.Linear(in_features=int(wandb.config['fc_out'] / 4), out_features=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.l:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# First look at Network\n",
    "\n",
    "print(MyNet().parameters)\n",
    "total_param = sum(p.numel() for p in MyNet().parameters())\n",
    "print(\"Total Parameters =  \", total_param)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = MyNet()\n",
    "model.to(device)\n",
    "\n",
    "for batch in dataloader_train:\n",
    "    X, y = batch[0].to(device), batch[1].to(device)\n",
    "    X = X.float()\n",
    "    print(X.dtype)\n",
    "    print('X is cuda:', X.is_cuda)\n",
    "    print('X.shape:', X.shape)\n",
    "    print('y.shape:', y.shape)\n",
    "    outputs = model(X)\n",
    "    print('outputs.shape:', outputs.shape)\n",
    "    break\n",
    "\n",
    "\n",
    "def train(model, loader, loss):\n",
    "    current_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # reading the data\n",
    "    for idx, batch in enumerate(loader):\n",
    "        X, y = batch\n",
    "        # move data to gpu if possible\n",
    "        if device.type == 'cuda':\n",
    "            X, y = X.to(device), y.to(device)\n",
    "        # training steps\n",
    "        optimizer.zero_grad()\n",
    "        X = X.float()\n",
    "        y = y.float()\n",
    "        output = model(X)\n",
    "        loss_fn = loss(output, y.reshape(-1, 1))\n",
    "        loss_fn.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss += loss_fn.item() * X.size(0)\n",
    "        correlation = np.corrcoef(output.cpu().detach().numpy().flat, y.cpu()).min()\n",
    "\n",
    "        if idx % 100 == 0:  # print every 2000 mini-batches\n",
    "            print('epoch: %d, train_loss: %.3f' % (epoch, loss_fn.item()), \"Training correlation:\", correlation)\n",
    "\n",
    "            # showing some examples of prediction\n",
    "            # print(\"5 Random samples: \")\n",
    "            # for i in random.sample(range(0,len(output)),5):\n",
    "            #    print('predict: %.3f, real: %.3f'%(output[i].item(),y[i].item()))\n",
    "            # print('')\n",
    "\n",
    "    epoch_loss = current_loss / len(loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def validation(model, loader, loss):\n",
    "    current_loss = 0\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    ys = []\n",
    "    # reading the data\n",
    "    for idx, batch in enumerate(loader):\n",
    "        X, y = batch\n",
    "        # move data to gpu if possible\n",
    "        if device.type == 'cuda':\n",
    "            X, y = X.to(device), y.to(device)\n",
    "        # validating steps\n",
    "\n",
    "        X = X.float()\n",
    "        y = y.float()\n",
    "        output = model(X)\n",
    "        ys.append(y.item())\n",
    "        outputs.append(output.item())\n",
    "        loss_fn = loss(output, y.reshape(-1, 1))\n",
    "\n",
    "        current_loss += loss_fn.item() * X.size(0)\n",
    "        # print('loss: %.3f' %(loss_fn.item()))\n",
    "    # print('number of outputs: ',len(output))\n",
    "\n",
    "    correlation = np.corrcoef(outputs, ys).min()\n",
    "    epoch_loss = current_loss / len(loader.dataset)\n",
    "    return epoch_loss, correlation, outputs, ys\n",
    "\n",
    "\n",
    "model = MyNet()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Hyper parameters\n",
    "\n",
    "epochs = wandb.config[\"epochs\"]\n",
    "learning_rate = wandb.config[\"learning_rate\"]\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "wandb.config[\"Total_Parameters\"] = total_param\n",
    "wandb.watch(model)\n",
    "\n",
    "# Training the model\n",
    "if training:\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss_train = train(model, dataloader_train, loss_function)\n",
    "        loss_valid, corr, output, y = validation(model, dataloader_val, loss_function)\n",
    "\n",
    "        loss_test, corr_test, output_test, y_test = validation(model, dataloader_test, loss_function)\n",
    "        loss_test_no_mod, corr_test_no_mod, output_test_no_mod, y_test_no_mod = validation(model, dataloader_test_no_mod,\n",
    "                                                                                          loss_function)\n",
    "        print('\\n Epoch: {}  Train Loss: {:.4f}  Validation Loss: {:.4f}  Validation Correlation: {:.4f}\\n'\n",
    "              .format(epoch, loss_train, loss_valid, corr))\n",
    "        if epoch == 2:\n",
    "            if not corr > 0.3 or corr == \"NaN\":\n",
    "                break\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch,\n",
    "            \"Train Loss Averaged\": loss_train,\n",
    "            \"Correlation\": corr,\n",
    "            \"Valid Loss Averaged\": loss_valid,\n",
    "            \"Test set Correlation\": corr_test,\n",
    "            \"No mod Test set Correlation\": corr_test_no_mod,\n",
    "            \"Total Parameters\": total_param\n",
    "            # \"Valid Acc\": acc_valid\n",
    "        })\n",
    "\n",
    "if not training:\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "if save_model:\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "if over_test_set:\n",
    "    loss_test, corr_test, output_test, y_test = validation(model, dataloader_test, loss_function)\n",
    "    print('\\n Test set Loss: {:.4f}  Test set Correlation: {:.4f}\\n'\n",
    "          .format(loss_test, corr_test))\n",
    "    print('\\n Test set no mod Loss: {:.4f}  Test set no mod Correlation: {:.4f}\\n'\n",
    "          .format(loss_test_no_mod, corr_test_no_mod))\n",
    "    output = output_test\n",
    "    y = y_test\n",
    "\n",
    "if want_figure:\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    mae_test = mean_absolute_error(output_test,y_test)\n",
    "    mae_no_mod = mean_absolute_error(output_test_no_mod,y_test_no_mod)\n",
    "\n",
    "    mod = 'Acetyl'\n",
    "\n",
    "    ax1.scatter(y_test_no_mod,output_test_no_mod,c='r',label=mod+\n",
    "                ' not_encoded (MAE: {:.3f}, R: {:.3f})'.format(mae_no_mod, corr_test_no_mod),s=3)\n",
    "    ax1.scatter(y_test,output_test,c='b',label=mod+\n",
    "                ' encoded (MAE: {:.3f}, R: {:.3f})'.format(mae_test, corr_test),s=3)\n",
    "\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xlabel('observed retention time')\n",
    "    plt.ylabel('predicted retention time')\n",
    "    plt.axis('scaled')\n",
    "    ax1.plot([0, 60], [0, 60], ls=\"--\", c=\".5\")\n",
    "    plt.xlim(0,60)\n",
    "    plt.ylim(0,60)\n",
    "    plt.show()\n",
    "\n",
    "# Metrics\n",
    "output_arrayed = np.array(output).reshape(-1, 1)\n",
    "y_arrayed = np.array(y).reshape(-1, 1)\n",
    "linear_regressor = LinearRegression()\n",
    "linear_regressor.fit(output_arrayed, y_arrayed)\n",
    "\n",
    "num_data = len(y)\n",
    "\n",
    "pear = np.corrcoef(output, y)\n",
    "mse = mean_squared_error(output, y)\n",
    "rmse = math.sqrt(mse / num_data)\n",
    "rse = math.sqrt(mse / (num_data - 2))\n",
    "rsquare = linear_regressor.score(output_arrayed, y_arrayed)\n",
    "mae = mean_absolute_error(output, y)\n",
    "\n",
    "print('default data, without normalization')\n",
    "print('Pearson Correlation: \\n', pear)\n",
    "print('RSE=', rse)\n",
    "print('R-Square=', rsquare)\n",
    "print('rmse=', rmse)\n",
    "print('mae=', mae)\n",
    "plt.scatter(output, y, s=0.2, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "          0         1         2         3         4         5         6   \\\n0   0.000000  1.220703  1.061523  0.765137  0.467773  1.061523  0.294434   \n1   0.000000 -0.323730 -0.968262 -0.586914 -1.414062 -0.685547 -1.493164   \n2   0.000000 -0.584473 -0.995605 -0.450684 -1.325195 -0.914062 -1.573242   \n3   0.000000 -0.734863 -0.733398 -0.759766 -0.733398 -0.734863 -0.734863   \n4   0.000000  1.360352 -0.472412  0.060364 -0.899902  0.931152  0.231079   \n5   0.000000  2.281250  0.000000  1.232422  0.000000  1.355469  0.000000   \n6   0.000000 -1.291992  0.000000 -2.000000  0.000000 -2.179688  0.000000   \n7   0.000000 -1.580078  0.000000 -1.775391  0.000000 -2.488281  0.000000   \n8   0.000000 -1.468750  0.000000 -1.493164  0.000000 -1.469727  0.000000   \n9   0.000000  0.887695  0.000000 -0.839355  0.000000  1.162109  0.000000   \n10  0.280029  0.280029  0.280029  0.280029  0.280029  0.280029  0.280029   \n11  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n12  0.000000  0.071411  0.142822  0.214233  0.285645  0.357178  0.428467   \n13  1.071289  1.000000  0.928711  0.856934  0.785645  0.714355  0.643066   \n14  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  1.000000   \n15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n16  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n17  0.000000  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000   \n18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n19  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n20  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n21  0.000000  1.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n22  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n23  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n24  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n25  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n26  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n27  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n28  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n29  0.000000  0.000000  0.000000  0.000000  1.000000  0.000000  0.000000   \n30  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000  0.000000   \n31  0.000000  0.000000  0.000000  0.000000  0.000000  1.000000  0.000000   \n32  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n33  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n34  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n\n          7         8         9   ...        42        43        44        45  \\\n0   0.765137  1.061523  0.294434  ...  0.000000  0.000000  0.000000  0.000000   \n1  -0.586914 -0.968262 -1.493164  ...  0.000000  0.000000  0.000000  0.000000   \n2  -0.450684 -0.995605 -1.573242  ...  0.000000  0.000000  0.000000  0.000000   \n3  -0.759766 -0.733398 -0.734863  ...  0.000000  0.000000  0.000000  0.000000   \n4   0.060364 -0.472412  0.231079  ...  0.000000  0.000000  0.000000  0.000000   \n5   1.826172  0.000000  1.059570  ...  0.000000  0.000000  0.000000  0.000000   \n6  -1.554688  0.000000 -2.013672  ...  0.000000  0.000000  0.000000  0.000000   \n7  -1.446289  0.000000 -1.996094  ...  0.000000  0.000000  0.000000  0.000000   \n8  -1.493164  0.000000 -1.469727  ...  0.000000  0.000000  0.000000  0.000000   \n9  -0.412109  0.000000 -0.368164  ...  0.000000  0.000000  0.000000  0.000000   \n10  0.280029  0.280029  0.280029  ...  0.280029  0.280029  0.280029  0.280029   \n11  1.000000  1.000000  1.000000  ...  0.000000  0.000000  0.000000  0.000000   \n12  0.500000  0.571289  0.643066  ...  0.000000  0.000000  0.000000  0.000000   \n13  0.571289  0.500000  0.428467  ...  0.000000  0.000000  0.000000  0.000000   \n14  0.000000  0.000000  1.000000  ...  0.000000  0.000000  0.000000  0.000000   \n15  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n16  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n17  1.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n18  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n19  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n20  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n21  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n22  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n23  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n24  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n25  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n26  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n27  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n28  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n29  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n30  0.000000  1.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n31  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n32  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n33  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n34  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n\n          46        47        48        49        50        51  \n0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n10  0.280029  0.280029  0.280029  0.280029  0.280029  0.280029  \n11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n14  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n16  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n19  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n20  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n21  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n22  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n23  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n24  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n25  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n26  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n27  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n28  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n29  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n30  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n31  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n32  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n33  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n34  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n\n[35 rows x 52 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>42</th>\n      <th>43</th>\n      <th>44</th>\n      <th>45</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n      <th>50</th>\n      <th>51</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>1.220703</td>\n      <td>1.061523</td>\n      <td>0.765137</td>\n      <td>0.467773</td>\n      <td>1.061523</td>\n      <td>0.294434</td>\n      <td>0.765137</td>\n      <td>1.061523</td>\n      <td>0.294434</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>-0.323730</td>\n      <td>-0.968262</td>\n      <td>-0.586914</td>\n      <td>-1.414062</td>\n      <td>-0.685547</td>\n      <td>-1.493164</td>\n      <td>-0.586914</td>\n      <td>-0.968262</td>\n      <td>-1.493164</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>-0.584473</td>\n      <td>-0.995605</td>\n      <td>-0.450684</td>\n      <td>-1.325195</td>\n      <td>-0.914062</td>\n      <td>-1.573242</td>\n      <td>-0.450684</td>\n      <td>-0.995605</td>\n      <td>-1.573242</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>-0.734863</td>\n      <td>-0.733398</td>\n      <td>-0.759766</td>\n      <td>-0.733398</td>\n      <td>-0.734863</td>\n      <td>-0.734863</td>\n      <td>-0.759766</td>\n      <td>-0.733398</td>\n      <td>-0.734863</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>1.360352</td>\n      <td>-0.472412</td>\n      <td>0.060364</td>\n      <td>-0.899902</td>\n      <td>0.931152</td>\n      <td>0.231079</td>\n      <td>0.060364</td>\n      <td>-0.472412</td>\n      <td>0.231079</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000000</td>\n      <td>2.281250</td>\n      <td>0.000000</td>\n      <td>1.232422</td>\n      <td>0.000000</td>\n      <td>1.355469</td>\n      <td>0.000000</td>\n      <td>1.826172</td>\n      <td>0.000000</td>\n      <td>1.059570</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.000000</td>\n      <td>-1.291992</td>\n      <td>0.000000</td>\n      <td>-2.000000</td>\n      <td>0.000000</td>\n      <td>-2.179688</td>\n      <td>0.000000</td>\n      <td>-1.554688</td>\n      <td>0.000000</td>\n      <td>-2.013672</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.000000</td>\n      <td>-1.580078</td>\n      <td>0.000000</td>\n      <td>-1.775391</td>\n      <td>0.000000</td>\n      <td>-2.488281</td>\n      <td>0.000000</td>\n      <td>-1.446289</td>\n      <td>0.000000</td>\n      <td>-1.996094</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.000000</td>\n      <td>-1.468750</td>\n      <td>0.000000</td>\n      <td>-1.493164</td>\n      <td>0.000000</td>\n      <td>-1.469727</td>\n      <td>0.000000</td>\n      <td>-1.493164</td>\n      <td>0.000000</td>\n      <td>-1.469727</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.000000</td>\n      <td>0.887695</td>\n      <td>0.000000</td>\n      <td>-0.839355</td>\n      <td>0.000000</td>\n      <td>1.162109</td>\n      <td>0.000000</td>\n      <td>-0.412109</td>\n      <td>0.000000</td>\n      <td>-0.368164</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>...</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.000000</td>\n      <td>0.071411</td>\n      <td>0.142822</td>\n      <td>0.214233</td>\n      <td>0.285645</td>\n      <td>0.357178</td>\n      <td>0.428467</td>\n      <td>0.500000</td>\n      <td>0.571289</td>\n      <td>0.643066</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1.071289</td>\n      <td>1.000000</td>\n      <td>0.928711</td>\n      <td>0.856934</td>\n      <td>0.785645</td>\n      <td>0.714355</td>\n      <td>0.643066</td>\n      <td>0.571289</td>\n      <td>0.500000</td>\n      <td>0.428467</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>35 rows × 52 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d13= np.load('data_matrix/14ptm/diamino13_custom_phospho9_normaltr/Acetyl/test_x.npy')\n",
    "d5=np.load('data_matrix/14ptm/diamino_custom_phospho9_normaltr/Acetyl/test_x.npy')\n",
    "pd.DataFrame(d5[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "          0         1         2         3         4         5         6   \\\n0   0.000000  1.220703  1.061523  0.765137  0.467773  1.061523  0.294434   \n1   0.000000 -0.323730 -0.968262 -0.586914 -1.414062 -0.685547 -1.493164   \n2   0.000000 -0.584473 -0.995605 -0.450684 -1.325195 -0.914062 -1.573242   \n3   0.000000 -0.734863 -0.733398 -0.759766 -0.733398 -0.734863 -0.734863   \n4   0.000000  1.360352 -0.472412  0.060364 -0.899902  0.931152  0.231079   \n5   0.000000 -0.862793 -0.070251  0.598145 -0.070251 -0.862793 -0.862793   \n6   0.000000 -0.569824 -0.569824 -0.569824 -0.569824 -0.569824 -0.569824   \n7   0.000000 -1.627930 -0.468506 -0.482422 -0.416504 -1.627930 -1.627930   \n8   0.000000 -0.629395  0.275879  1.125977  0.275879 -0.629395 -0.629395   \n9   0.000000 -0.134888 -0.134888 -0.134888 -0.134888 -0.134888 -0.134888   \n10  0.000000 -0.452148  0.375732  0.375732  0.375732 -0.452148 -0.452148   \n11  0.000000 -0.032318 -0.032318  0.930176 -0.032318 -0.032318 -0.032318   \n12  0.000000 -0.711914 -0.962891 -0.379883 -1.254883 -1.003906 -1.587891   \n13  0.000000  2.281250  0.000000  1.232422  0.000000  1.355469  0.000000   \n14  0.000000 -1.291992  0.000000 -2.000000  0.000000 -2.179688  0.000000   \n15  0.000000 -1.580078  0.000000 -1.775391  0.000000 -2.488281  0.000000   \n16  0.000000 -1.468750  0.000000 -1.493164  0.000000 -1.469727  0.000000   \n17  0.000000  0.887695  0.000000 -0.839355  0.000000  1.162109  0.000000   \n18  0.000000 -0.933105  0.000000  0.527832  0.000000 -1.725586  0.000000   \n19  0.000000 -1.139648  0.000000 -1.139648  0.000000 -1.139648  0.000000   \n20  0.000000 -2.095703  0.000000 -0.898926  0.000000 -3.255859  0.000000   \n21  0.000000 -0.353516  0.000000  1.402344  0.000000 -1.258789  0.000000   \n22  0.000000 -0.269775  0.000000 -0.269775  0.000000 -0.269775  0.000000   \n23  0.000000 -0.076416  0.000000  0.751465  0.000000 -0.904297  0.000000   \n24  0.000000 -0.064636  0.000000  0.897949  0.000000 -0.064636  0.000000   \n25  0.000000 -1.674805  0.000000 -1.634766  0.000000 -2.591797  0.000000   \n26  0.280029  0.280029  0.280029  0.280029  0.280029  0.280029  0.280029   \n27  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n28  0.000000  0.071411  0.142822  0.214233  0.285645  0.357178  0.428467   \n29  1.071289  1.000000  0.928711  0.856934  0.785645  0.714355  0.643066   \n30  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  1.000000   \n31  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n32  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n33  0.000000  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000   \n34  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n35  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n36  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n37  0.000000  1.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n38  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n39  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n40  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n41  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n42  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n43  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n44  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n45  0.000000  0.000000  0.000000  0.000000  1.000000  0.000000  0.000000   \n46  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000  0.000000   \n47  0.000000  0.000000  0.000000  0.000000  0.000000  1.000000  0.000000   \n48  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n49  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n50  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n\n          7         8         9   ...        42        43        44        45  \\\n0   0.765137  1.061523  0.294434  ...  0.000000  0.000000  0.000000  0.000000   \n1  -0.586914 -0.968262 -1.493164  ...  0.000000  0.000000  0.000000  0.000000   \n2  -0.450684 -0.995605 -1.573242  ...  0.000000  0.000000  0.000000  0.000000   \n3  -0.759766 -0.733398 -0.734863  ...  0.000000  0.000000  0.000000  0.000000   \n4   0.060364 -0.472412  0.231079  ...  0.000000  0.000000  0.000000  0.000000   \n5   0.598145 -0.070251 -0.862793  ...  0.000000  0.000000  0.000000  0.000000   \n6  -0.569824 -0.569824 -0.569824  ...  0.000000  0.000000  0.000000  0.000000   \n7  -0.482422 -0.468506 -1.627930  ...  0.000000  0.000000  0.000000  0.000000   \n8   1.125977  0.275879 -0.629395  ...  0.000000  0.000000  0.000000  0.000000   \n9  -0.134888 -0.134888 -0.134888  ...  0.000000  0.000000  0.000000  0.000000   \n10  0.375732  0.375732 -0.452148  ...  0.000000  0.000000  0.000000  0.000000   \n11  0.930176 -0.032318 -0.032318  ...  0.000000  0.000000  0.000000  0.000000   \n12 -0.379883 -0.962891 -1.587891  ...  0.000000  0.000000  0.000000  0.000000   \n13  1.826172  0.000000  1.059570  ...  0.000000  0.000000  0.000000  0.000000   \n14 -1.554688  0.000000 -2.013672  ...  0.000000  0.000000  0.000000  0.000000   \n15 -1.446289  0.000000 -1.996094  ...  0.000000  0.000000  0.000000  0.000000   \n16 -1.493164  0.000000 -1.469727  ...  0.000000  0.000000  0.000000  0.000000   \n17 -0.412109  0.000000 -0.368164  ...  0.000000  0.000000  0.000000  0.000000   \n18  0.527832  0.000000 -0.037598  ...  0.000000  0.000000  0.000000  0.000000   \n19 -1.139648  0.000000  0.062988  ...  0.000000  0.000000  0.000000  0.000000   \n20 -0.951172  0.000000 -2.644531  ...  0.000000  0.000000  0.000000  0.000000   \n21  1.402344  0.000000 -0.408936  ...  0.000000  0.000000  0.000000  0.000000   \n22 -0.269775  0.000000 -0.269775  ...  0.000000  0.000000  0.000000  0.000000   \n23  0.751465  0.000000  0.025391  ...  0.000000  0.000000  0.000000  0.000000   \n24  0.897949  0.000000  0.897949  ...  0.000000  0.000000  0.000000  0.000000   \n25 -1.342773  0.000000 -1.988281  ...  0.000000  0.000000  0.000000  0.000000   \n26  0.280029  0.280029  0.280029  ...  0.280029  0.280029  0.280029  0.280029   \n27  1.000000  1.000000  1.000000  ...  0.000000  0.000000  0.000000  0.000000   \n28  0.500000  0.571289  0.643066  ...  0.000000  0.000000  0.000000  0.000000   \n29  0.571289  0.500000  0.428467  ...  0.000000  0.000000  0.000000  0.000000   \n30  0.000000  0.000000  1.000000  ...  0.000000  0.000000  0.000000  0.000000   \n31  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n32  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n33  1.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n34  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n35  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n36  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n37  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n38  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n39  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n40  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n41  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n42  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n43  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n44  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n45  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n46  0.000000  1.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n47  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n48  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n49  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n50  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n\n          46        47        48        49        50        51  \n0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n14  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n16  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n19  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n20  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n21  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n22  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n23  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n24  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n25  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n26  0.280029  0.280029  0.280029  0.280029  0.280029  0.280029  \n27  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n28  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n29  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n30  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n31  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n32  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n33  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n34  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n35  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n36  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n37  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n38  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n39  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n40  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n41  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n42  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n43  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n44  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n45  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n46  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n47  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n48  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n49  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n50  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n\n[51 rows x 52 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>42</th>\n      <th>43</th>\n      <th>44</th>\n      <th>45</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n      <th>50</th>\n      <th>51</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>1.220703</td>\n      <td>1.061523</td>\n      <td>0.765137</td>\n      <td>0.467773</td>\n      <td>1.061523</td>\n      <td>0.294434</td>\n      <td>0.765137</td>\n      <td>1.061523</td>\n      <td>0.294434</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>-0.323730</td>\n      <td>-0.968262</td>\n      <td>-0.586914</td>\n      <td>-1.414062</td>\n      <td>-0.685547</td>\n      <td>-1.493164</td>\n      <td>-0.586914</td>\n      <td>-0.968262</td>\n      <td>-1.493164</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>-0.584473</td>\n      <td>-0.995605</td>\n      <td>-0.450684</td>\n      <td>-1.325195</td>\n      <td>-0.914062</td>\n      <td>-1.573242</td>\n      <td>-0.450684</td>\n      <td>-0.995605</td>\n      <td>-1.573242</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>-0.734863</td>\n      <td>-0.733398</td>\n      <td>-0.759766</td>\n      <td>-0.733398</td>\n      <td>-0.734863</td>\n      <td>-0.734863</td>\n      <td>-0.759766</td>\n      <td>-0.733398</td>\n      <td>-0.734863</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>1.360352</td>\n      <td>-0.472412</td>\n      <td>0.060364</td>\n      <td>-0.899902</td>\n      <td>0.931152</td>\n      <td>0.231079</td>\n      <td>0.060364</td>\n      <td>-0.472412</td>\n      <td>0.231079</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000000</td>\n      <td>-0.862793</td>\n      <td>-0.070251</td>\n      <td>0.598145</td>\n      <td>-0.070251</td>\n      <td>-0.862793</td>\n      <td>-0.862793</td>\n      <td>0.598145</td>\n      <td>-0.070251</td>\n      <td>-0.862793</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.000000</td>\n      <td>-0.569824</td>\n      <td>-0.569824</td>\n      <td>-0.569824</td>\n      <td>-0.569824</td>\n      <td>-0.569824</td>\n      <td>-0.569824</td>\n      <td>-0.569824</td>\n      <td>-0.569824</td>\n      <td>-0.569824</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.000000</td>\n      <td>-1.627930</td>\n      <td>-0.468506</td>\n      <td>-0.482422</td>\n      <td>-0.416504</td>\n      <td>-1.627930</td>\n      <td>-1.627930</td>\n      <td>-0.482422</td>\n      <td>-0.468506</td>\n      <td>-1.627930</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.000000</td>\n      <td>-0.629395</td>\n      <td>0.275879</td>\n      <td>1.125977</td>\n      <td>0.275879</td>\n      <td>-0.629395</td>\n      <td>-0.629395</td>\n      <td>1.125977</td>\n      <td>0.275879</td>\n      <td>-0.629395</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.000000</td>\n      <td>-0.134888</td>\n      <td>-0.134888</td>\n      <td>-0.134888</td>\n      <td>-0.134888</td>\n      <td>-0.134888</td>\n      <td>-0.134888</td>\n      <td>-0.134888</td>\n      <td>-0.134888</td>\n      <td>-0.134888</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.000000</td>\n      <td>-0.452148</td>\n      <td>0.375732</td>\n      <td>0.375732</td>\n      <td>0.375732</td>\n      <td>-0.452148</td>\n      <td>-0.452148</td>\n      <td>0.375732</td>\n      <td>0.375732</td>\n      <td>-0.452148</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.000000</td>\n      <td>-0.032318</td>\n      <td>-0.032318</td>\n      <td>0.930176</td>\n      <td>-0.032318</td>\n      <td>-0.032318</td>\n      <td>-0.032318</td>\n      <td>0.930176</td>\n      <td>-0.032318</td>\n      <td>-0.032318</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.000000</td>\n      <td>-0.711914</td>\n      <td>-0.962891</td>\n      <td>-0.379883</td>\n      <td>-1.254883</td>\n      <td>-1.003906</td>\n      <td>-1.587891</td>\n      <td>-0.379883</td>\n      <td>-0.962891</td>\n      <td>-1.587891</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.000000</td>\n      <td>2.281250</td>\n      <td>0.000000</td>\n      <td>1.232422</td>\n      <td>0.000000</td>\n      <td>1.355469</td>\n      <td>0.000000</td>\n      <td>1.826172</td>\n      <td>0.000000</td>\n      <td>1.059570</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.000000</td>\n      <td>-1.291992</td>\n      <td>0.000000</td>\n      <td>-2.000000</td>\n      <td>0.000000</td>\n      <td>-2.179688</td>\n      <td>0.000000</td>\n      <td>-1.554688</td>\n      <td>0.000000</td>\n      <td>-2.013672</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.000000</td>\n      <td>-1.580078</td>\n      <td>0.000000</td>\n      <td>-1.775391</td>\n      <td>0.000000</td>\n      <td>-2.488281</td>\n      <td>0.000000</td>\n      <td>-1.446289</td>\n      <td>0.000000</td>\n      <td>-1.996094</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.000000</td>\n      <td>-1.468750</td>\n      <td>0.000000</td>\n      <td>-1.493164</td>\n      <td>0.000000</td>\n      <td>-1.469727</td>\n      <td>0.000000</td>\n      <td>-1.493164</td>\n      <td>0.000000</td>\n      <td>-1.469727</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.000000</td>\n      <td>0.887695</td>\n      <td>0.000000</td>\n      <td>-0.839355</td>\n      <td>0.000000</td>\n      <td>1.162109</td>\n      <td>0.000000</td>\n      <td>-0.412109</td>\n      <td>0.000000</td>\n      <td>-0.368164</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.000000</td>\n      <td>-0.933105</td>\n      <td>0.000000</td>\n      <td>0.527832</td>\n      <td>0.000000</td>\n      <td>-1.725586</td>\n      <td>0.000000</td>\n      <td>0.527832</td>\n      <td>0.000000</td>\n      <td>-0.037598</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.000000</td>\n      <td>-1.139648</td>\n      <td>0.000000</td>\n      <td>-1.139648</td>\n      <td>0.000000</td>\n      <td>-1.139648</td>\n      <td>0.000000</td>\n      <td>-1.139648</td>\n      <td>0.000000</td>\n      <td>0.062988</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.000000</td>\n      <td>-2.095703</td>\n      <td>0.000000</td>\n      <td>-0.898926</td>\n      <td>0.000000</td>\n      <td>-3.255859</td>\n      <td>0.000000</td>\n      <td>-0.951172</td>\n      <td>0.000000</td>\n      <td>-2.644531</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.000000</td>\n      <td>-0.353516</td>\n      <td>0.000000</td>\n      <td>1.402344</td>\n      <td>0.000000</td>\n      <td>-1.258789</td>\n      <td>0.000000</td>\n      <td>1.402344</td>\n      <td>0.000000</td>\n      <td>-0.408936</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.000000</td>\n      <td>-0.269775</td>\n      <td>0.000000</td>\n      <td>-0.269775</td>\n      <td>0.000000</td>\n      <td>-0.269775</td>\n      <td>0.000000</td>\n      <td>-0.269775</td>\n      <td>0.000000</td>\n      <td>-0.269775</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.000000</td>\n      <td>-0.076416</td>\n      <td>0.000000</td>\n      <td>0.751465</td>\n      <td>0.000000</td>\n      <td>-0.904297</td>\n      <td>0.000000</td>\n      <td>0.751465</td>\n      <td>0.000000</td>\n      <td>0.025391</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.000000</td>\n      <td>-0.064636</td>\n      <td>0.000000</td>\n      <td>0.897949</td>\n      <td>0.000000</td>\n      <td>-0.064636</td>\n      <td>0.000000</td>\n      <td>0.897949</td>\n      <td>0.000000</td>\n      <td>0.897949</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.000000</td>\n      <td>-1.674805</td>\n      <td>0.000000</td>\n      <td>-1.634766</td>\n      <td>0.000000</td>\n      <td>-2.591797</td>\n      <td>0.000000</td>\n      <td>-1.342773</td>\n      <td>0.000000</td>\n      <td>-1.988281</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>...</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n      <td>0.280029</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.000000</td>\n      <td>0.071411</td>\n      <td>0.142822</td>\n      <td>0.214233</td>\n      <td>0.285645</td>\n      <td>0.357178</td>\n      <td>0.428467</td>\n      <td>0.500000</td>\n      <td>0.571289</td>\n      <td>0.643066</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>1.071289</td>\n      <td>1.000000</td>\n      <td>0.928711</td>\n      <td>0.856934</td>\n      <td>0.785645</td>\n      <td>0.714355</td>\n      <td>0.643066</td>\n      <td>0.571289</td>\n      <td>0.500000</td>\n      <td>0.428467</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>51 rows × 52 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(d13[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
